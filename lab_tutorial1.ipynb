{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab_tutorial1-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namoshi/dl_intro/blob/master/lab_tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ght-oPu9-r4R"
      },
      "source": [
        "1. Convolutional Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jKhf9HBgqfpB"
      },
      "source": [
        "This is an example of classification of mnist using Convolutional Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CmgnFhZGAyOz"
      },
      "source": [
        "Import of required library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UDpMZHJEA8ZA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "85b304af-37eb-46d1-b10c-422f9ca8a8b7"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print('torch version is {}'.format(torch.__version__))\n",
        "if torch.cuda.is_available():\n",
        "    print('cuda is available')\n",
        "else:\n",
        "    print('cuda is not avaibalbe')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch version is 1.2.0\n",
            "cuda is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8G0RhRpBHn6"
      },
      "source": [
        "Fixed seed value for random numbers.\n",
        "\n",
        "The initial value of the weight is determined by random numbers. By fixing the seed value of the random number, the initial value of the weight is fixed to prevent different results depending on the initial value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ap5k0pKmCQLH",
        "colab": {}
      },
      "source": [
        "torch.cuda.manual_seed_all(100100)\n",
        "torch.manual_seed(100100)\n",
        "np.random.seed(100100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SFBh-snpCoG6"
      },
      "source": [
        "Reading dataset.\n",
        "\n",
        "Image data is stored in x_train and x_test, and labels are stored in y_train and y_test.\n",
        "\n",
        "This part should be rewritten as appropriate depending on the data set you use.\n",
        "\n",
        "The input image to the model should have the shape of $(N, C, H, W)$.\n",
        "\n",
        "$N:$ number of data\n",
        "\n",
        "$C:$ number of channnel\n",
        "\n",
        "$H:$ height\n",
        "\n",
        "$W:$ width"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BcxOk-KICqxf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "0a757d7f-a84c-4772-962f-b73f061a39d8"
      },
      "source": [
        "mnist_train = dsets.MNIST(\".\", download=True, train=True)\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "mnist_test = dsets.MNIST(\".\", download=True, train=False)\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(mnist_train)):\n",
        "\tx_train.append(np.array(mnist_train[i][0]))\n",
        "\ty_train.append(mnist_train[i][1])\n",
        "\n",
        "for i in range(len(mnist_test)):\n",
        "\tx_test.append(np.array(mnist_test[i][0]))\n",
        "\ty_test.append(mnist_test[i][1])\n",
        "\n",
        "#Change shape to (N, C, H, W) by reshape.\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_train = x_train.reshape(len(mnist_train), 1, 28, 28)\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "x_test = x_test.reshape(len(mnist_test), 1, 28, 28)\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "\n",
        "datasize = len(y_train)\n",
        "datasize_test = len(y_test)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3643059.03it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 58175.81it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 876028.97it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21602.03it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k8rIo1KHtmGC"
      },
      "source": [
        "Define the model structure.\n",
        "\n",
        "The computation within the convolution layers is regarded as a filtering process of the input image as\n",
        "\\begin{align}\n",
        "f_{p,q}^{(c)}=h(\\sum^{convy-1}_{r=0}\\sum^{convx-1}_{s=0}w^{(c)}_{r,s}f^{(c-1)}_{p+r, q+s}+b^{(c)}) \\; ,\n",
        "\\end{align}\n",
        "\n",
        "where $w^{(c)}_{r,s}$ is the weight of the neuron indexed as $(r,s)$ in the $c$-th convolution layer and $b^{(c)}$ is the bias of the $c$-th convolution layer. \n",
        "\n",
        "The size of the convolution filter is given as $convx \\times convy$. The activation function of each neuron is denoted as $h$. \n",
        "\n",
        "Usually, pooling layers are added after the convolution layers. The pooling layer performs downsampling for reducing computational costs and enhancing against micro position changes. \n",
        "\n",
        "Fully-connected layers like multi layer perceptron is connected to the convolution layers which is used to construct the classifier. \n",
        "\n",
        "In this example, a model of the following structure is defined.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "convolution layer1:(filtersize:3*3, channel:32)\n",
        "\n",
        "convolution layer2:(filtersize:3*3, channel:32)\n",
        "\n",
        "full connected:(128)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RcZi9egWvd5C",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,32,3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv2 = nn.Conv2d(32,32,3, padding=1)\n",
        "        self.fc1 = nn.Linear(32*7*7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = Network()\n",
        "if torch.cuda.is_available():\n",
        "  net.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jHZ-VU_Nv83w"
      },
      "source": [
        "Definition of loss function.\n",
        "\n",
        "The cross entropy loss is defined as the following equation.\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "E = \\sum_n^N {\\boldsymbol t}_n^Tlog(S({\\boldsymbol y}_n)) \\; \n",
        "\\end{align}\n",
        "\n",
        "$S(\\cdot)$ is an activation function of the output layer.\n",
        "\n",
        "${\\boldsymbol t_n}$ is a label and ${\\boldsymbol y_n}$ is the output of the network.\n",
        "\n",
        "In multiclass classification, the softmax function is generally used for the activation function of the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mP60jrGTzdft",
        "colab": {}
      },
      "source": [
        "softmax_cross_entropy = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aG1793oHf9Ss"
      },
      "source": [
        "Definition of optimizer. \n",
        "\n",
        "In the example, the stochastic gradient descent method with momentum (SGD with momentum) is used.\n",
        "\n",
        "$\\mu$ is a learning rate and $\\eta$ is a momentum parameter.\n",
        "\n",
        "\\begin{align}\n",
        "{\\boldsymbol w}^{(t+1)} &\\leftarrow {\\boldsymbol w}^{(t)} + \\Delta  {\\boldsymbol w}^{(t)} \\\\\n",
        " \\Delta  {\\boldsymbol w}^{(t)} &= \\mu \\Delta {\\boldsymbol w}^{(t - 1)} - (1-\\mu) \\eta {\\boldsymbol g}^{(t)} \\\\\n",
        " {\\boldsymbol g}^{(t)} &= \\nabla E({\\boldsymbol w}^{(t)})\n",
        " \\end{align}\n",
        " \n",
        "In addition, regularization terms such as weight decay may be added as necessary to prevent over-learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ErvMc0GZlcx",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v1jsgYv8d9CX"
      },
      "source": [
        "Input data into the model for each mini-batch and perform parameter update.\n",
        "\n",
        "(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mYoFAxXld9SP",
        "colab": {}
      },
      "source": [
        "train_loss=[]\n",
        "train_acc=[]\n",
        "test_loss=[]\n",
        "test_acc=[]\n",
        "\n",
        "#define batch-size and epoch.\n",
        "epoch=100\n",
        "batchsize=100\n",
        "\n",
        "\n",
        "for epoch in range(1, epoch+1):\n",
        "\tprint('epoch', epoch)\n",
        "\tperm = np.random.permutation(datasize)\n",
        "\tfor i in range(0, datasize, batchsize):\n",
        "\t\t#Create mini-batch.\n",
        "\t\tx_batch = x_train[perm[i:i+batchsize]]\n",
        "\t\ty_batch = y_train[perm[i:i+batchsize]]\n",
        "    \n",
        "\t\t#Convert a numpy array to a tensor to do gradient calculations.\n",
        "\t\tif torch.cuda.is_available():\n",
        "\t\t\tx_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "\t\t\ty_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "\t\telse:\n",
        "\t\t\tx_batch = torch.from_numpy(x_batch).float()\n",
        "\t\t\ty_batch = torch.from_numpy(y_batch).long()\n",
        "\n",
        "\t\t#Initialize the stored gradient.\n",
        "\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\t#Get the model output for the input mini-batch.\n",
        "\t\ty = net(x_batch)\n",
        "    \n",
        "\t\t#Calculate loss from the output of the model. At this time, the activation function of the output layer is also reflected in the calculation.\n",
        "\t\tloss = softmax_cross_entropy(y, y_batch)  \n",
        "    \n",
        "\t\t#From loss, calculate the gradient of each parameter.\n",
        "\t\tloss.backward()\n",
        "\n",
        "\t\t#Each parameter is updated from the calculated gradient.\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\tsum_score = 0\n",
        "\tsum_loss = 0\n",
        "  \n",
        "\t#Evaluate the model with train data.\n",
        "\tfor i in range(0, datasize, batchsize):\n",
        "\t\tx_batch = x_train[i:i+batchsize]\n",
        "\t\ty_batch = y_train[i:i+batchsize]\n",
        "\t\tif torch.cuda.is_available():\n",
        "\t\t\tx_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "\t\t\ty_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "\t\telse:\n",
        "\t\t\tx_batch = torch.from_numpy(x_batch).float()\n",
        "\t\t\ty_batch = torch.from_numpy(y_batch).long()\n",
        "\t\ty = net(x_batch)\n",
        "\t\tloss = softmax_cross_entropy(y, y_batch)\n",
        "\t\tsum_loss += float(loss.cpu().data.item()) * batchsize\n",
        "\t\t_, predict = y.max(1)\n",
        "\t\tsum_score += predict.eq(y_batch).sum().item()\n",
        "\tprint(\"\\ttrain  mean loss={}, accuracy={}\".format(sum_loss / datasize, sum_score / datasize))\n",
        "\ttrain_loss.append(sum_loss / datasize)\n",
        "\ttrain_acc.append(sum_score / datasize)\n",
        "\n",
        "\n",
        "\tsum_score = 0\n",
        "\tsum_loss = 0\n",
        "\n",
        "  \n",
        "\t#Evaluate the model with test data.\n",
        "\tfor i in range(0, datasize_test, batchsize):\n",
        "\t\tx_batch = x_test[i:i+batchsize]\n",
        "\t\ty_batch = y_test[i:i+batchsize]\n",
        "\t\tif torch.cuda.is_available():\n",
        "\t\t\tx_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "\t\t\ty_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "\t\telse:\n",
        "\t\t\tx_batch = torch.from_numpy(x_batch).float()\n",
        "\t\t\ty_batch = torch.from_numpy(y_batch).long()\n",
        "\t\ty = net(x_batch)\n",
        "\t\tloss = softmax_cross_entropy(y, y_batch)\n",
        "\t\tsum_loss += float(loss.cpu().data.item()) * batchsize\n",
        "\t\t_, predict = y.max(1)\n",
        "\t\tsum_score += predict.eq(y_batch).sum().item()\n",
        "\tprint(\"\\ttest  mean loss={}, accuracy={}\".format(sum_loss / datasize_test, sum_score / datasize_test))\n",
        "\ttest_loss.append(sum_loss / datasize_test)\n",
        "\ttest_acc.append(sum_score / datasize_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RQ7ExXwkgaeR"
      },
      "source": [
        "Draw a graph of training curve (loss)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AwkiA2h_gaod",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "\n",
        "plt.ylim(0,1.0)\n",
        "plt.plot(range(epoch), train_loss)\n",
        "plt.plot(range(epoch), test_loss, c='#00ff00')\n",
        "plt.legend(['train loss', 'test loss'])\n",
        "plt.title('loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gopav6c2htLt"
      },
      "source": [
        "Draw the graph of training curve (accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KGmlusz6htZN",
        "colab": {}
      },
      "source": [
        "plt.plot(range(epoch), train_acc)\n",
        "plt.plot(range(epoch), test_acc, c='#00ff00')\n",
        "plt.ylim(0,1.0)\n",
        "plt.legend(['train acc', 'test acc'])\n",
        "plt.title('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Q1aiv18bLJ",
        "colab_type": "text"
      },
      "source": [
        "Next, look at the change in the weight of the middle layer during learning.\n",
        "\n",
        "First, visualize the connection weights from the input of fully connected layers to the middle layer.\n",
        "\n",
        "Let's take a look at the unit connection weight, which uses the upper left value of the feature map as input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UlRNBOxgE4ZF",
        "colab": {}
      },
      "source": [
        "net = Network()\n",
        "net.cuda()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NUVgSxVKIPj4",
        "colab": {}
      },
      "source": [
        "train_loss=[]\n",
        "train_acc=[]\n",
        "test_loss=[]\n",
        "test_acc=[]\n",
        "weight_num = 10\n",
        "\n",
        "\n",
        "#define batch-size and epoch.\n",
        "epoch=100\n",
        "batchsize=100\n",
        "\n",
        "#Defines an array that holds the weight for each epoch.\n",
        "train_weight = np.zeros((epoch, weight_num))\n",
        "\n",
        "#Specify the weight to be visualized.\n",
        "#Specify hidden layer unit j.\n",
        "rum_num = np.array([0,15,30,45,60,75,90,105,120,127])\n",
        "\n",
        "#Specify input layer unit i.\n",
        "perm_num = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "\n",
        "#The weight w_ij connecting the input layer unit i and the hidden layer unit j is acquired.\n",
        "\n",
        "\n",
        "for epoch in range(1, epoch+1):\n",
        "  print('epoch', epoch)\n",
        "  perm = np.random.permutation(datasize)\n",
        "  for i in range(0, datasize, batchsize):\n",
        "    #Create mini-batch.\n",
        "    x_batch = x_train[perm[i:i+batchsize]]\n",
        "    y_batch = y_train[perm[i:i+batchsize]]\n",
        "    \n",
        "    #Convert a numpy array to a tensor to do gradient calculations.\n",
        "    if torch.cuda.is_available():\n",
        "      x_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "      y_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "    else:\n",
        "      x_batch = torch.from_numpy(x_batch).float()\n",
        "      y_batch = torch.from_numpy(y_batch).long()\n",
        "    \n",
        "    #Initialize the stored gradient.\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #Get the model output for the input mini-batch.\n",
        "    y = net(x_batch)\n",
        "    \n",
        "    #Calculate loss from the output of the model. At this time, the activation function of the output layer is also reflected in the calculation.\n",
        "    loss = softmax_cross_entropy(y, y_batch)  \n",
        "    \n",
        "    #From loss, calculate the gradient of each parameter.\n",
        "    loss.backward()               \n",
        "    \n",
        "    #Each parameter is updated from the calculated gradient.\n",
        "    optimizer.step()\n",
        "    \n",
        "  #Extract weight values from the model.\n",
        "  weight = net.fc1.weight.cpu().detach().numpy()\n",
        "  \n",
        "  for i in range(weight_num):\n",
        "    weight0 = weight[rum_num[i]][perm_num[i]]\n",
        "    train_weight[epoch-1][i] = weight0\n",
        "\n",
        "\n",
        "  sum_score = 0\n",
        "  sum_loss = 0\n",
        "  \n",
        "  #Evaluate the model with train data.\n",
        "  for i in range(0, datasize, batchsize):\n",
        "    x_batch = x_train[i:i+batchsize]\n",
        "    y_batch = y_train[i:i+batchsize]\n",
        "    if torch.cuda.is_available():\n",
        "      x_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "      y_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "    else:\n",
        "      x_batch = torch.from_numpy(x_batch).float()\n",
        "      y_batch = torch.from_numpy(y_batch).long()\n",
        "    y = net(x_batch)\n",
        "    loss = softmax_cross_entropy(y, y_batch)\n",
        "    sum_loss += float(loss.cpu().data.item()) * batchsize\n",
        "    _, predict = y.max(1)\n",
        "    sum_score += predict.eq(y_batch).sum().item()\n",
        "  print(\"\\ttrain  mean loss={}, accuracy={}\".format(sum_loss / datasize, sum_score / datasize))\n",
        "  train_loss.append(sum_loss / datasize)\n",
        "  train_acc.append(sum_score / datasize)\n",
        "\n",
        "\n",
        "  sum_score = 0\n",
        "  sum_loss = 0\n",
        "\n",
        "  \n",
        "  #Evaluate the model with test data.\n",
        "  for i in range(0, datasize_test, batchsize):\n",
        "    x_batch = x_test[i:i+batchsize]\n",
        "    y_batch = y_test[i:i+batchsize]\n",
        "    if torch.cuda.is_available():\n",
        "      x_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "      y_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "    else:\n",
        "      x_batch = torch.from_numpy(x_batch).float()\n",
        "      y_batch = torch.from_numpy(y_batch).long()\n",
        "    y = net(x_batch)\n",
        "    loss = softmax_cross_entropy(y, y_batch)\n",
        "    sum_loss += float(loss.cpu().data.item()) * batchsize\n",
        "    _, predict = y.max(1)\n",
        "    sum_score += predict.eq(y_batch).sum().item()\n",
        "  print(\"\\ttest  mean loss={}, accuracy={}\".format(sum_loss / datasize_test, sum_score / datasize_test))\n",
        "  test_loss.append(sum_loss / datasize_test)\n",
        "  test_acc.append(sum_score / datasize_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kEpCXDI2NshD",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "#A list for specifying the color of the graph. Changes are required according to the number of weights to be displayed.\n",
        "c = ['#ff0000', '#ffff00', '#00ff00', '#00ffff', '#0000ff','#ff00ff', '#990000', '#999900', '#009900', '#009999']\n",
        "\n",
        "#Plot the obtained weights on a graph.\n",
        "train_weight = train_weight.transpose(1,0)\n",
        "for i in range(weight_num):\n",
        "  plt.plot(range(epoch), train_weight[i], c=c[i])\n",
        "plt.legend(['w0,0', 'w15,0', 'w30,0', 'w45,0', 'w60,0', 'w75,0', 'w90,0', 'w105,0', 'w120,0', 'w127,0'])\n",
        "plt.title('weight')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQoK5fVM32KW",
        "colab_type": "text"
      },
      "source": [
        "The unit connection weights are all close to 0.\n",
        "\n",
        "That is, the value of index 0 unit in the input layer is rarely used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "518_6UkpKsWD",
        "colab_type": "text"
      },
      "source": [
        "Next, visualize the filter weight of the first layer of convolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6n84POtr8Fky",
        "colab": {}
      },
      "source": [
        "net = Network()\n",
        "net.cuda()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RI2vZScnaKo-",
        "colab": {}
      },
      "source": [
        "train_loss=[]\n",
        "train_acc=[]\n",
        "test_loss=[]\n",
        "test_acc=[]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#define batch-size and epoch.\n",
        "epoch=100\n",
        "batchsize=100\n",
        "\n",
        "#Specify the weight to be visualized.\n",
        "#Specify the input channel.\n",
        "inputc_num = np.array([0,0,0])\n",
        "\n",
        "#Specify the output channel.\n",
        "outputc_num = np.array([0,16,31])\n",
        "\n",
        "#Defines an array that holds the weight for each epoch.\n",
        "weight_num = int(len(inputc_num))\n",
        "k_size = int(net.conv1.weight.cpu().detach().numpy().shape[2])\n",
        "train_weight = np.zeros((weight_num, epoch, k_size*k_size))\n",
        "\n",
        "for epoch in range(1, epoch+1):\n",
        "  print('epoch', epoch)\n",
        "  perm = np.random.permutation(datasize)\n",
        "  for i in range(0, datasize, batchsize):\n",
        "    #Create mini-batch.\n",
        "    x_batch = x_train[perm[i:i+batchsize]]\n",
        "    y_batch = y_train[perm[i:i+batchsize]]\n",
        "    \n",
        "    #Convert a numpy array to a tensor to do gradient calculations.\n",
        "    if torch.cuda.is_available():\n",
        "      x_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "      y_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "    else:\n",
        "        x_batch = torch.from_numpy(x_batch).float()\n",
        "        y_batch = torch.from_numpy(y_batch).long()\n",
        "    \n",
        "    #Initialize the stored gradient.\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #Get the model output for the input mini-batch.\n",
        "    y = net(x_batch)\n",
        "    \n",
        "    #Calculate loss from the output of the model. At this time, the activation function of the output layer is also reflected in the calculation.\n",
        "    loss = softmax_cross_entropy(y, y_batch)  \n",
        "    \n",
        "    #From loss, calculate the gradient of each parameter.\n",
        "    loss.backward()               \n",
        "    \n",
        "    #Each parameter is updated from the calculated gradient.\n",
        "    optimizer.step()\n",
        "  #Extract weight values from the model.\n",
        "  weight = net.conv1.weight.cpu().detach().numpy()\n",
        "  \n",
        "  for i in range(weight_num):\n",
        "    weight0 = weight[outputc_num[i]][inputc_num[i]]\n",
        "    weight0 = weight0.flatten()\n",
        "    train_weight[i][epoch-1] = weight0\n",
        "\n",
        "\n",
        "  sum_score = 0\n",
        "  sum_loss = 0\n",
        "  \n",
        "  #Evaluate the model with train data.\n",
        "  for i in range(0, datasize, batchsize):\n",
        "    x_batch = x_train[i:i+batchsize]\n",
        "    y_batch = y_train[i:i+batchsize]\n",
        "    if torch.cuda.is_available():\n",
        "      x_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "      y_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "    else:\n",
        "        x_batch = torch.from_numpy(x_batch).float()\n",
        "        y_batch = torch.from_numpy(y_batch).long()\n",
        "    y = net(x_batch)\n",
        "    loss = softmax_cross_entropy(y, y_batch)\n",
        "    sum_loss += float(loss.cpu().data.item()) * batchsize\n",
        "    _, predict = y.max(1)\n",
        "    sum_score += predict.eq(y_batch).sum().item()\n",
        "  print(\"\\ttrain  mean loss={}, accuracy={}\".format(sum_loss / datasize, sum_score / datasize))\n",
        "  train_loss.append(sum_loss / datasize)\n",
        "  train_acc.append(sum_score / datasize)\n",
        "\n",
        "\n",
        "  sum_score = 0\n",
        "  sum_loss = 0\n",
        "\n",
        "  \n",
        "  #Evaluate the model with test data.\n",
        "  for i in range(0, datasize_test, batchsize):\n",
        "    x_batch = x_test[i:i+batchsize]\n",
        "    y_batch = y_test[i:i+batchsize]\n",
        "    if torch.cuda.is_available():\n",
        "      x_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "      y_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "    else:\n",
        "        x_batch = torch.from_numpy(x_batch).float()\n",
        "        y_batch = torch.from_numpy(y_batch).long()\n",
        "    y = net(x_batch)\n",
        "    loss = softmax_cross_entropy(y, y_batch)\n",
        "    sum_loss += float(loss.cpu().data.item()) * batchsize\n",
        "    _, predict = y.max(1)\n",
        "    sum_score += predict.eq(y_batch).sum().item()\n",
        "  print(\"\\ttest  mean loss={}, accuracy={}\".format(sum_loss / datasize_test, sum_score / datasize_test))\n",
        "  test_loss.append(sum_loss / datasize_test)\n",
        "  test_acc.append(sum_score / datasize_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jt2NXm-cbyh5",
        "colab": {}
      },
      "source": [
        "#A list for specifying the color of the graph. Changes are required according to the number of weights to be displayed.\n",
        "c = ['#ff0000', '#ffff00', '#00ff00', '#00ffff', '#0000ff','#ff00ff', '#990000', '#999900', '#009900', '#009999']\n",
        "\n",
        "for i in range(weight_num):\n",
        "  plt.figure(figsize=(6,6))\n",
        "  p = train_weight[i]\n",
        "  p = p.transpose(1,0)\n",
        "  for j in range(k_size*k_size):\n",
        "    plt.plot(range(epoch), p[j], c=c[j])\n",
        "  plt.title('filter%d'%i)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CqbJSz8N2i2u",
        "colab": {}
      },
      "source": [
        "net = Network()\n",
        "net.cuda()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tww682NC2m4i",
        "colab": {}
      },
      "source": [
        "train_loss=[]\n",
        "train_acc=[]\n",
        "test_loss=[]\n",
        "test_acc=[]\n",
        "\n",
        "\n",
        "\n",
        "#define batch-size and epoch.\n",
        "epoch=100\n",
        "batchsize=100\n",
        "\n",
        "#Specify the weight to be visualized.\n",
        "#Specify the input channel.\n",
        "inputc_num = np.array([0,16,25])\n",
        "\n",
        "#Specify the output channel.\n",
        "outputc_num = np.array([0,16,25])\n",
        "\n",
        "weight_num = int(len(inputc_num))\n",
        "k_size = int(net.conv2.weight.cpu().detach().numpy().shape[2])\n",
        "\n",
        "train_weight = np.zeros((weight_num, epoch, k_size*k_size))\n",
        "\n",
        "for epoch in range(1, epoch+1):\n",
        "  print('epoch', epoch)\n",
        "  perm = np.random.permutation(datasize)\n",
        "  for i in range(0, datasize, batchsize):\n",
        "    #Create mini-batch.\n",
        "    x_batch = x_train[perm[i:i+batchsize]]\n",
        "    y_batch = y_train[perm[i:i+batchsize]]\n",
        "    \n",
        "    #Convert a numpy array to a tensor to do gradient calculations.\n",
        "    if torch.cuda.is_available():\n",
        "      x_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "      y_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "    else:\n",
        "        x_batch = torch.from_numpy(x_batch).float()\n",
        "        y_batch = torch.from_numpy(y_batch).long()\n",
        "    \n",
        "    #Initialize the stored gradient.\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #Get the model output for the input mini-batch.\n",
        "    y = net(x_batch)\n",
        "    \n",
        "    #Calculate loss from the output of the model. At this time, the activation function of the output layer is also reflected in the calculation.\n",
        "    loss = softmax_cross_entropy(y, y_batch)  \n",
        "    \n",
        "    #From loss, calculate the gradient of each parameter.\n",
        "    loss.backward()               \n",
        "    \n",
        "    #Each parameter is updated from the calculated gradient.\n",
        "    optimizer.step()\n",
        "  #Extract weight values from the model.\n",
        "  weight = net.conv2.weight.cpu().detach().numpy()\n",
        "  \n",
        "  for i in range(weight_num):\n",
        "    weight0 = weight[outputc_num[i]][inputc_num[i]]\n",
        "    weight0 = weight0.flatten()\n",
        "    train_weight[i][epoch-1] = weight0\n",
        "\n",
        "\n",
        "  sum_score = 0\n",
        "  sum_loss = 0\n",
        "  \n",
        "  #Evaluate the model with train data.\n",
        "  for i in range(0, datasize, batchsize):\n",
        "    x_batch = x_train[i:i+batchsize]\n",
        "    y_batch = y_train[i:i+batchsize]\n",
        "    if torch.cuda.is_available():\n",
        "      x_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "      y_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "    else:\n",
        "        x_batch = torch.from_numpy(x_batch).float()\n",
        "        y_batch = torch.from_numpy(y_batch).long()\n",
        "    y = net(x_batch)\n",
        "    loss = softmax_cross_entropy(y, y_batch)\n",
        "    sum_loss += float(loss.cpu().data.item()) * batchsize\n",
        "    _, predict = y.max(1)\n",
        "    sum_score += predict.eq(y_batch).sum().item()\n",
        "  print(\"\\ttrain  mean loss={}, accuracy={}\".format(sum_loss / datasize, sum_score / datasize))\n",
        "  train_loss.append(sum_loss / datasize)\n",
        "  train_acc.append(sum_score / datasize)\n",
        "\n",
        "\n",
        "  sum_score = 0\n",
        "  sum_loss = 0\n",
        "\n",
        "  \n",
        "  #Evaluate the model with test data.\n",
        "  for i in range(0, datasize_test, batchsize):\n",
        "    x_batch = x_test[i:i+batchsize]\n",
        "    y_batch = y_test[i:i+batchsize]\n",
        "    if torch.cuda.is_available():\n",
        "      x_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "      y_batch = torch.from_numpy(y_batch).long().cuda()\n",
        "    else:\n",
        "        x_batch = torch.from_numpy(x_batch).float()\n",
        "        y_batch = torch.from_numpy(y_batch).long()\n",
        "    y = net(x_batch)\n",
        "    loss = softmax_cross_entropy(y, y_batch)\n",
        "    sum_loss += float(loss.cpu().data.item()) * batchsize\n",
        "    _, predict = y.max(1)\n",
        "    sum_score += predict.eq(y_batch).sum().item()\n",
        "  print(\"\\ttest  mean loss={}, accuracy={}\".format(sum_loss / datasize_test, sum_score / datasize_test))\n",
        "  test_loss.append(sum_loss / datasize_test)\n",
        "  test_acc.append(sum_score / datasize_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MUMuDwcX34X7",
        "colab": {}
      },
      "source": [
        "#A list for specifying the color of the graph. Changes are required according to the number of weights to be displayed.\n",
        "c = ['#ff0000', '#ffff00', '#00ff00', '#00ffff', '#0000ff','#ff00ff', '#990000', '#999900', '#009900', '#009999']\n",
        "\n",
        "#Plot the obtained weights on a graph.\n",
        "for i in range(weight_num):\n",
        "  plt.figure(figsize=(6,6))\n",
        "  p = train_weight[i]\n",
        "  p = p.transpose(1,0)\n",
        "  for j in range(k_size*k_size):\n",
        "    plt.plot(range(epoch), p[j], c=c[j])\n",
        "  plt.title('filter%d'%i)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9hE9DA62Qzx",
        "colab_type": "text"
      },
      "source": [
        "It may be the effect of weight decay that some filter weight values are approaching zero.\n",
        "\n",
        "You can also see the change in weight when not using weight decay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-PTux3H2_BR",
        "colab_type": "text"
      },
      "source": [
        "Next, let's take out the feature map of the middle layer.\n",
        "\n",
        "Extract the feature map obtained from the first layer of convolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZW-_JErluFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = np.zeros((datasize, 32, 28, 28))\n",
        "\n",
        "for i in range(0, datasize, batchsize):\n",
        "  x_batch = x_train[i:i+batchsize]\n",
        "  x_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "  fmap = net.conv1(x_batch)\n",
        "  result[i:i+batchsize] = fmap.cpu().detach().numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bl8SJEal5nf",
        "colab_type": "text"
      },
      "source": [
        "The extracted feature map is displayed in as image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBXtpJmfl6Vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify the index of the image you want to display.\n",
        "index = 250\n",
        "\n",
        "for i in range(32):\n",
        "  pic = result[index]\n",
        "  pic = pic[i]\n",
        "  pic= pic.reshape((28,28))\n",
        "  plt.imshow(pic)\n",
        "  plt.gray()\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w40EUGv1NtjX",
        "colab_type": "text"
      },
      "source": [
        "Next, extract the feature map obtained from the second layer of convolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WTvw-btpG0dm",
        "colab": {}
      },
      "source": [
        "result = np.zeros((datasize, 32, 28, 28))\n",
        "\n",
        "for i in range(0, datasize, batchsize):\n",
        "  x_batch = x_train[i:i+batchsize]\n",
        "  x_batch = torch.from_numpy(x_batch).float().cuda()\n",
        "  fmap = net.conv1(x_batch)\n",
        "  fmap = net.conv2(fmap)\n",
        "  result[i:i+batchsize] = fmap.cpu().detach().numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9oqMd0JcFAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify the index of the image you want to display.\n",
        "index = 250\n",
        "\n",
        "\n",
        "for i in range(32):\n",
        "  pic = result[index]\n",
        "  pic = pic[i]\n",
        "  pic= pic.reshape((28,28))\n",
        "  plt.imshow(pic)\n",
        "  plt.gray()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBPNAtJBdjc3",
        "colab_type": "text"
      },
      "source": [
        "Each feature map shows that edge extraction has been performed."
      ]
    }
  ]
}