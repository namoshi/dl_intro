{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Encoder for Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the data =  (150, 4)\n",
      "N= 150 mdim= 4\n",
      "size of t (150,)\n",
      "[[-9.00681170e-01  1.01900435e+00 -1.34022653e+00 -1.31544430e+00]\n",
      " [-1.14301691e+00 -1.31979479e-01 -1.34022653e+00 -1.31544430e+00]\n",
      " [-1.38535265e+00  3.28414053e-01 -1.39706395e+00 -1.31544430e+00]\n",
      " [-1.50652052e+00  9.82172869e-02 -1.28338910e+00 -1.31544430e+00]\n",
      " [-1.02184904e+00  1.24920112e+00 -1.34022653e+00 -1.31544430e+00]\n",
      " [-5.37177559e-01  1.93979142e+00 -1.16971425e+00 -1.05217993e+00]\n",
      " [-1.50652052e+00  7.88807586e-01 -1.34022653e+00 -1.18381211e+00]\n",
      " [-1.02184904e+00  7.88807586e-01 -1.28338910e+00 -1.31544430e+00]\n",
      " [-1.74885626e+00 -3.62176246e-01 -1.34022653e+00 -1.31544430e+00]\n",
      " [-1.14301691e+00  9.82172869e-02 -1.28338910e+00 -1.44707648e+00]\n",
      " [-5.37177559e-01  1.47939788e+00 -1.28338910e+00 -1.31544430e+00]\n",
      " [-1.26418478e+00  7.88807586e-01 -1.22655167e+00 -1.31544430e+00]\n",
      " [-1.26418478e+00 -1.31979479e-01 -1.34022653e+00 -1.44707648e+00]\n",
      " [-1.87002413e+00 -1.31979479e-01 -1.51073881e+00 -1.44707648e+00]\n",
      " [-5.25060772e-02  2.16998818e+00 -1.45390138e+00 -1.31544430e+00]\n",
      " [-1.73673948e-01  3.09077525e+00 -1.28338910e+00 -1.05217993e+00]\n",
      " [-5.37177559e-01  1.93979142e+00 -1.39706395e+00 -1.05217993e+00]\n",
      " [-9.00681170e-01  1.01900435e+00 -1.34022653e+00 -1.18381211e+00]\n",
      " [-1.73673948e-01  1.70959465e+00 -1.16971425e+00 -1.18381211e+00]\n",
      " [-9.00681170e-01  1.70959465e+00 -1.28338910e+00 -1.18381211e+00]\n",
      " [-5.37177559e-01  7.88807586e-01 -1.16971425e+00 -1.31544430e+00]\n",
      " [-9.00681170e-01  1.47939788e+00 -1.28338910e+00 -1.05217993e+00]\n",
      " [-1.50652052e+00  1.24920112e+00 -1.56757623e+00 -1.31544430e+00]\n",
      " [-9.00681170e-01  5.58610819e-01 -1.16971425e+00 -9.20547742e-01]\n",
      " [-1.26418478e+00  7.88807586e-01 -1.05603939e+00 -1.31544430e+00]\n",
      " [-1.02184904e+00 -1.31979479e-01 -1.22655167e+00 -1.31544430e+00]\n",
      " [-1.02184904e+00  7.88807586e-01 -1.22655167e+00 -1.05217993e+00]\n",
      " [-7.79513300e-01  1.01900435e+00 -1.28338910e+00 -1.31544430e+00]\n",
      " [-7.79513300e-01  7.88807586e-01 -1.34022653e+00 -1.31544430e+00]\n",
      " [-1.38535265e+00  3.28414053e-01 -1.22655167e+00 -1.31544430e+00]\n",
      " [-1.26418478e+00  9.82172869e-02 -1.22655167e+00 -1.31544430e+00]\n",
      " [-5.37177559e-01  7.88807586e-01 -1.28338910e+00 -1.05217993e+00]\n",
      " [-7.79513300e-01  2.40018495e+00 -1.28338910e+00 -1.44707648e+00]\n",
      " [-4.16009689e-01  2.63038172e+00 -1.34022653e+00 -1.31544430e+00]\n",
      " [-1.14301691e+00  9.82172869e-02 -1.28338910e+00 -1.31544430e+00]\n",
      " [-1.02184904e+00  3.28414053e-01 -1.45390138e+00 -1.31544430e+00]\n",
      " [-4.16009689e-01  1.01900435e+00 -1.39706395e+00 -1.31544430e+00]\n",
      " [-1.14301691e+00  1.24920112e+00 -1.34022653e+00 -1.44707648e+00]\n",
      " [-1.74885626e+00 -1.31979479e-01 -1.39706395e+00 -1.31544430e+00]\n",
      " [-9.00681170e-01  7.88807586e-01 -1.28338910e+00 -1.31544430e+00]\n",
      " [-1.02184904e+00  1.01900435e+00 -1.39706395e+00 -1.18381211e+00]\n",
      " [-1.62768839e+00 -1.74335684e+00 -1.39706395e+00 -1.18381211e+00]\n",
      " [-1.74885626e+00  3.28414053e-01 -1.39706395e+00 -1.31544430e+00]\n",
      " [-1.02184904e+00  1.01900435e+00 -1.22655167e+00 -7.88915558e-01]\n",
      " [-9.00681170e-01  1.70959465e+00 -1.05603939e+00 -1.05217993e+00]\n",
      " [-1.26418478e+00 -1.31979479e-01 -1.34022653e+00 -1.18381211e+00]\n",
      " [-9.00681170e-01  1.70959465e+00 -1.22655167e+00 -1.31544430e+00]\n",
      " [-1.50652052e+00  3.28414053e-01 -1.34022653e+00 -1.31544430e+00]\n",
      " [-6.58345429e-01  1.47939788e+00 -1.28338910e+00 -1.31544430e+00]\n",
      " [-1.02184904e+00  5.58610819e-01 -1.34022653e+00 -1.31544430e+00]\n",
      " [ 1.40150837e+00  3.28414053e-01  5.35408562e-01  2.64141916e-01]\n",
      " [ 6.74501145e-01  3.28414053e-01  4.21733708e-01  3.95774101e-01]\n",
      " [ 1.28034050e+00  9.82172869e-02  6.49083415e-01  3.95774101e-01]\n",
      " [-4.16009689e-01 -1.74335684e+00  1.37546573e-01  1.32509732e-01]\n",
      " [ 7.95669016e-01 -5.92373012e-01  4.78571135e-01  3.95774101e-01]\n",
      " [-1.73673948e-01 -5.92373012e-01  4.21733708e-01  1.32509732e-01]\n",
      " [ 5.53333275e-01  5.58610819e-01  5.35408562e-01  5.27406285e-01]\n",
      " [-1.14301691e+00 -1.51316008e+00 -2.60315415e-01 -2.62386821e-01]\n",
      " [ 9.16836886e-01 -3.62176246e-01  4.78571135e-01  1.32509732e-01]\n",
      " [-7.79513300e-01 -8.22569778e-01  8.07091462e-02  2.64141916e-01]\n",
      " [-1.02184904e+00 -2.43394714e+00 -1.46640561e-01 -2.62386821e-01]\n",
      " [ 6.86617933e-02 -1.31979479e-01  2.51221427e-01  3.95774101e-01]\n",
      " [ 1.89829664e-01 -1.97355361e+00  1.37546573e-01 -2.62386821e-01]\n",
      " [ 3.10997534e-01 -3.62176246e-01  5.35408562e-01  2.64141916e-01]\n",
      " [-2.94841818e-01 -3.62176246e-01 -8.98031345e-02  1.32509732e-01]\n",
      " [ 1.03800476e+00  9.82172869e-02  3.64896281e-01  2.64141916e-01]\n",
      " [-2.94841818e-01 -1.31979479e-01  4.21733708e-01  3.95774101e-01]\n",
      " [-5.25060772e-02 -8.22569778e-01  1.94384000e-01 -2.62386821e-01]\n",
      " [ 4.32165405e-01 -1.97355361e+00  4.21733708e-01  3.95774101e-01]\n",
      " [-2.94841818e-01 -1.28296331e+00  8.07091462e-02 -1.30754636e-01]\n",
      " [ 6.86617933e-02  3.28414053e-01  5.92245988e-01  7.90670654e-01]\n",
      " [ 3.10997534e-01 -5.92373012e-01  1.37546573e-01  1.32509732e-01]\n",
      " [ 5.53333275e-01 -1.28296331e+00  6.49083415e-01  3.95774101e-01]\n",
      " [ 3.10997534e-01 -5.92373012e-01  5.35408562e-01  8.77547895e-04]\n",
      " [ 6.74501145e-01 -3.62176246e-01  3.08058854e-01  1.32509732e-01]\n",
      " [ 9.16836886e-01 -1.31979479e-01  3.64896281e-01  2.64141916e-01]\n",
      " [ 1.15917263e+00 -5.92373012e-01  5.92245988e-01  2.64141916e-01]\n",
      " [ 1.03800476e+00 -1.31979479e-01  7.05920842e-01  6.59038469e-01]\n",
      " [ 1.89829664e-01 -3.62176246e-01  4.21733708e-01  3.95774101e-01]\n",
      " [-1.73673948e-01 -1.05276654e+00 -1.46640561e-01 -2.62386821e-01]\n",
      " [-4.16009689e-01 -1.51316008e+00  2.38717193e-02 -1.30754636e-01]\n",
      " [-4.16009689e-01 -1.51316008e+00 -3.29657076e-02 -2.62386821e-01]\n",
      " [-5.25060772e-02 -8.22569778e-01  8.07091462e-02  8.77547895e-04]\n",
      " [ 1.89829664e-01 -8.22569778e-01  7.62758269e-01  5.27406285e-01]\n",
      " [-5.37177559e-01 -1.31979479e-01  4.21733708e-01  3.95774101e-01]\n",
      " [ 1.89829664e-01  7.88807586e-01  4.21733708e-01  5.27406285e-01]\n",
      " [ 1.03800476e+00  9.82172869e-02  5.35408562e-01  3.95774101e-01]\n",
      " [ 5.53333275e-01 -1.74335684e+00  3.64896281e-01  1.32509732e-01]\n",
      " [-2.94841818e-01 -1.31979479e-01  1.94384000e-01  1.32509732e-01]\n",
      " [-4.16009689e-01 -1.28296331e+00  1.37546573e-01  1.32509732e-01]\n",
      " [-4.16009689e-01 -1.05276654e+00  3.64896281e-01  8.77547895e-04]\n",
      " [ 3.10997534e-01 -1.31979479e-01  4.78571135e-01  2.64141916e-01]\n",
      " [-5.25060772e-02 -1.05276654e+00  1.37546573e-01  8.77547895e-04]\n",
      " [-1.02184904e+00 -1.74335684e+00 -2.60315415e-01 -2.62386821e-01]\n",
      " [-2.94841818e-01 -8.22569778e-01  2.51221427e-01  1.32509732e-01]\n",
      " [-1.73673948e-01 -1.31979479e-01  2.51221427e-01  8.77547895e-04]\n",
      " [-1.73673948e-01 -3.62176246e-01  2.51221427e-01  1.32509732e-01]\n",
      " [ 4.32165405e-01 -3.62176246e-01  3.08058854e-01  1.32509732e-01]\n",
      " [-9.00681170e-01 -1.28296331e+00 -4.30827696e-01 -1.30754636e-01]\n",
      " [-1.73673948e-01 -5.92373012e-01  1.94384000e-01  1.32509732e-01]\n",
      " [ 5.53333275e-01  5.58610819e-01  1.27429511e+00  1.71209594e+00]\n",
      " [-5.25060772e-02 -8.22569778e-01  7.62758269e-01  9.22302838e-01]\n",
      " [ 1.52267624e+00 -1.31979479e-01  1.21745768e+00  1.18556721e+00]\n",
      " [ 5.53333275e-01 -3.62176246e-01  1.04694540e+00  7.90670654e-01]\n",
      " [ 7.95669016e-01 -1.31979479e-01  1.16062026e+00  1.31719939e+00]\n",
      " [ 2.12851559e+00 -1.31979479e-01  1.61531967e+00  1.18556721e+00]\n",
      " [-1.14301691e+00 -1.28296331e+00  4.21733708e-01  6.59038469e-01]\n",
      " [ 1.76501198e+00 -3.62176246e-01  1.44480739e+00  7.90670654e-01]\n",
      " [ 1.03800476e+00 -1.28296331e+00  1.16062026e+00  7.90670654e-01]\n",
      " [ 1.64384411e+00  1.24920112e+00  1.33113254e+00  1.71209594e+00]\n",
      " [ 7.95669016e-01  3.28414053e-01  7.62758269e-01  1.05393502e+00]\n",
      " [ 6.74501145e-01 -8.22569778e-01  8.76433123e-01  9.22302838e-01]\n",
      " [ 1.15917263e+00 -1.31979479e-01  9.90107977e-01  1.18556721e+00]\n",
      " [-1.73673948e-01 -1.28296331e+00  7.05920842e-01  1.05393502e+00]\n",
      " [-5.25060772e-02 -5.92373012e-01  7.62758269e-01  1.58046376e+00]\n",
      " [ 6.74501145e-01  3.28414053e-01  8.76433123e-01  1.44883158e+00]\n",
      " [ 7.95669016e-01 -1.31979479e-01  9.90107977e-01  7.90670654e-01]\n",
      " [ 2.24968346e+00  1.70959465e+00  1.67215710e+00  1.31719939e+00]\n",
      " [ 2.24968346e+00 -1.05276654e+00  1.78583195e+00  1.44883158e+00]\n",
      " [ 1.89829664e-01 -1.97355361e+00  7.05920842e-01  3.95774101e-01]\n",
      " [ 1.28034050e+00  3.28414053e-01  1.10378283e+00  1.44883158e+00]\n",
      " [-2.94841818e-01 -5.92373012e-01  6.49083415e-01  1.05393502e+00]\n",
      " [ 2.24968346e+00 -5.92373012e-01  1.67215710e+00  1.05393502e+00]\n",
      " [ 5.53333275e-01 -8.22569778e-01  6.49083415e-01  7.90670654e-01]\n",
      " [ 1.03800476e+00  5.58610819e-01  1.10378283e+00  1.18556721e+00]\n",
      " [ 1.64384411e+00  3.28414053e-01  1.27429511e+00  7.90670654e-01]\n",
      " [ 4.32165405e-01 -5.92373012e-01  5.92245988e-01  7.90670654e-01]\n",
      " [ 3.10997534e-01 -1.31979479e-01  6.49083415e-01  7.90670654e-01]\n",
      " [ 6.74501145e-01 -5.92373012e-01  1.04694540e+00  1.18556721e+00]\n",
      " [ 1.64384411e+00 -1.31979479e-01  1.16062026e+00  5.27406285e-01]\n",
      " [ 1.88617985e+00 -5.92373012e-01  1.33113254e+00  9.22302838e-01]\n",
      " [ 2.49201920e+00  1.70959465e+00  1.50164482e+00  1.05393502e+00]\n",
      " [ 6.74501145e-01 -5.92373012e-01  1.04694540e+00  1.31719939e+00]\n",
      " [ 5.53333275e-01 -5.92373012e-01  7.62758269e-01  3.95774101e-01]\n",
      " [ 3.10997534e-01 -1.05276654e+00  1.04694540e+00  2.64141916e-01]\n",
      " [ 2.24968346e+00 -1.31979479e-01  1.33113254e+00  1.44883158e+00]\n",
      " [ 5.53333275e-01  7.88807586e-01  1.04694540e+00  1.58046376e+00]\n",
      " [ 6.74501145e-01  9.82172869e-02  9.90107977e-01  7.90670654e-01]\n",
      " [ 1.89829664e-01 -1.31979479e-01  5.92245988e-01  7.90670654e-01]\n",
      " [ 1.28034050e+00  9.82172869e-02  9.33270550e-01  1.18556721e+00]\n",
      " [ 1.03800476e+00  9.82172869e-02  1.04694540e+00  1.58046376e+00]\n",
      " [ 1.28034050e+00  9.82172869e-02  7.62758269e-01  1.44883158e+00]\n",
      " [-5.25060772e-02 -8.22569778e-01  7.62758269e-01  9.22302838e-01]\n",
      " [ 1.15917263e+00  3.28414053e-01  1.21745768e+00  1.44883158e+00]\n",
      " [ 1.03800476e+00  5.58610819e-01  1.10378283e+00  1.71209594e+00]\n",
      " [ 1.03800476e+00 -1.31979479e-01  8.19595696e-01  1.44883158e+00]\n",
      " [ 5.53333275e-01 -1.28296331e+00  7.05920842e-01  9.22302838e-01]\n",
      " [ 7.95669016e-01 -1.31979479e-01  8.19595696e-01  1.05393502e+00]\n",
      " [ 4.32165405e-01  7.88807586e-01  9.33270550e-01  1.44883158e+00]\n",
      " [ 6.86617933e-02 -1.31979479e-01  7.62758269e-01  7.90670654e-01]]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "t = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "print('Size of the data = ', X.shape)\n",
    "n = X.shape[0]\n",
    "mdim = X.shape[1]\n",
    "print('N=', n, 'mdim=', mdim)\n",
    "print('size of t', t.shape)\n",
    "\n",
    "# normarize X\n",
    "xmean = np.mean(X, axis=0)\n",
    "xsd = np.std(X, axis=0)\n",
    "X = (X - xmean)/xsd\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 4]) torch.Size([150])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "t = torch.tensor(t, dtype=torch.int64)\n",
    "\n",
    "print(X.shape, t.shape)  # 出力: (150, 4) (150,)\n",
    "\n",
    "print(type(X))\n",
    "\n",
    "# データセットの分割割合\n",
    "n_train = int(len(X) * 0.8)\n",
    "n_test = len(X) - n_train\n",
    "\n",
    "train, test = torch.utils.data.random_split(torch.utils.data.TensorDataset(X, t), [n_train, n_test])\n",
    "\n",
    "# DataLoaderの定義\n",
    "batch_size = 10\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[-0.1737, -0.5924,  0.4217,  0.1325],\n",
      "        [ 1.6438,  1.2492,  1.3311,  1.7121],\n",
      "        [-1.0218,  0.7888, -1.2834, -1.3154],\n",
      "        [ 1.1592, -0.5924,  0.5922,  0.2641],\n",
      "        [-0.1737, -0.5924,  0.1944,  0.1325],\n",
      "        [-0.9007, -1.2830, -0.4308, -0.1308],\n",
      "        [-0.4160, -1.7434,  0.1375,  0.1325],\n",
      "        [-1.2642, -0.1320, -1.3402, -1.4471],\n",
      "        [-0.2948, -0.8226,  0.2512,  0.1325],\n",
      "        [ 1.2803,  0.0982,  0.7628,  1.4488]])\n",
      "tensor([1, 2, 0, 1, 1, 1, 1, 0, 1, 2])\n",
      "1\n",
      "tensor([[ 0.4322, -1.9736,  0.4217,  0.3958],\n",
      "        [-1.3854,  0.3284, -1.2266, -1.3154],\n",
      "        [ 1.2803,  0.0982,  0.9333,  1.1856],\n",
      "        [-1.0218, -1.7434, -0.2603, -0.2624],\n",
      "        [ 0.7957, -0.1320,  1.1606,  1.3172],\n",
      "        [-0.2948, -0.3622, -0.0898,  0.1325],\n",
      "        [-0.5372,  0.7888, -1.2834, -1.0522],\n",
      "        [-1.5065,  1.2492, -1.5676, -1.3154],\n",
      "        [-1.0218,  0.3284, -1.4539, -1.3154],\n",
      "        [-0.4160, -1.5132,  0.0239, -0.1308]])\n",
      "tensor([1, 0, 2, 1, 2, 1, 0, 0, 0, 1])\n",
      "2\n",
      "tensor([[ 0.1898, -1.9736,  0.1375, -0.2624],\n",
      "        [-0.4160, -1.2830,  0.1375,  0.1325],\n",
      "        [ 0.5533, -0.8226,  0.6491,  0.7907],\n",
      "        [ 0.5533,  0.5586,  0.5354,  0.5274],\n",
      "        [-0.4160, -1.5132, -0.0330, -0.2624],\n",
      "        [ 2.1285, -0.1320,  1.6153,  1.1856],\n",
      "        [ 2.2497, -0.5924,  1.6722,  1.0539],\n",
      "        [ 0.1898, -0.3622,  0.4217,  0.3958],\n",
      "        [-1.7489,  0.3284, -1.3971, -1.3154],\n",
      "        [-0.0525, -0.8226,  0.7628,  0.9223]])\n",
      "tensor([1, 1, 2, 1, 1, 2, 2, 1, 0, 2])\n",
      "3\n",
      "tensor([[ 6.7450e-01,  3.2841e-01,  8.7643e-01,  1.4488e+00],\n",
      "        [-1.0218e+00,  5.5861e-01, -1.3402e+00, -1.3154e+00],\n",
      "        [ 5.5333e-01, -1.7434e+00,  3.6490e-01,  1.3251e-01],\n",
      "        [-1.1430e+00, -1.5132e+00, -2.6032e-01, -2.6239e-01],\n",
      "        [-5.2506e-02, -8.2257e-01,  8.0709e-02,  8.7755e-04],\n",
      "        [ 7.9567e-01, -1.3198e-01,  8.1960e-01,  1.0539e+00],\n",
      "        [ 6.7450e-01,  9.8217e-02,  9.9011e-01,  7.9067e-01],\n",
      "        [-1.5065e+00,  7.8881e-01, -1.3402e+00, -1.1838e+00],\n",
      "        [-2.9484e-01, -1.3198e-01,  1.9438e-01,  1.3251e-01],\n",
      "        [-5.3718e-01, -1.3198e-01,  4.2173e-01,  3.9577e-01]])\n",
      "tensor([2, 0, 1, 1, 1, 2, 2, 0, 1, 1])\n",
      "4\n",
      "tensor([[-1.1430,  0.0982, -1.2834, -1.4471],\n",
      "        [-1.0218,  1.0190, -1.2266, -0.7889],\n",
      "        [ 0.6745, -0.8226,  0.8764,  0.9223],\n",
      "        [ 0.6745, -0.5924,  1.0469,  1.1856],\n",
      "        [-1.0218,  1.2492, -1.3402, -1.3154],\n",
      "        [-1.2642,  0.0982, -1.2266, -1.3154],\n",
      "        [ 2.2497,  1.7096,  1.6722,  1.3172],\n",
      "        [-0.9007,  1.4794, -1.2834, -1.0522],\n",
      "        [-1.0218,  1.0190, -1.3971, -1.1838],\n",
      "        [ 0.5533, -0.5924,  0.7628,  0.3958]])\n",
      "tensor([0, 0, 2, 2, 0, 0, 2, 0, 0, 2])\n",
      "5\n",
      "tensor([[-0.9007,  1.7096, -1.2266, -1.3154],\n",
      "        [-1.2642,  0.7888, -1.0560, -1.3154],\n",
      "        [-0.2948, -0.5924,  0.6491,  1.0539],\n",
      "        [-1.1430, -1.2830,  0.4217,  0.6590],\n",
      "        [-1.2642,  0.7888, -1.2266, -1.3154],\n",
      "        [ 0.6745, -0.5924,  1.0469,  1.3172],\n",
      "        [ 0.9168, -0.1320,  0.3649,  0.2641],\n",
      "        [ 0.5533, -1.2830,  0.7059,  0.9223],\n",
      "        [ 1.0380, -1.2830,  1.1606,  0.7907],\n",
      "        [ 1.5227, -0.1320,  1.2175,  1.1856]])\n",
      "tensor([0, 0, 2, 2, 0, 2, 1, 2, 2, 2])\n",
      "6\n",
      "tensor([[ 1.0380,  0.5586,  1.1038,  1.1856],\n",
      "        [ 0.0687, -0.1320,  0.7628,  0.7907],\n",
      "        [-0.1737, -1.2830,  0.7059,  1.0539],\n",
      "        [-0.9007,  1.7096, -1.2834, -1.1838],\n",
      "        [-0.5372,  1.9398, -1.1697, -1.0522],\n",
      "        [-1.1430,  0.0982, -1.2834, -1.3154],\n",
      "        [-0.4160,  2.6304, -1.3402, -1.3154],\n",
      "        [ 0.3110, -0.1320,  0.4786,  0.2641],\n",
      "        [-1.5065,  0.0982, -1.2834, -1.3154],\n",
      "        [ 0.3110, -0.3622,  0.5354,  0.2641]])\n",
      "tensor([2, 2, 2, 0, 0, 0, 0, 1, 0, 1])\n",
      "7\n",
      "tensor([[ 6.7450e-01, -3.6218e-01,  3.0806e-01,  1.3251e-01],\n",
      "        [-6.5835e-01,  1.4794e+00, -1.2834e+00, -1.3154e+00],\n",
      "        [-9.0068e-01,  1.0190e+00, -1.3402e+00, -1.1838e+00],\n",
      "        [-9.0068e-01,  7.8881e-01, -1.2834e+00, -1.3154e+00],\n",
      "        [-7.7951e-01, -8.2257e-01,  8.0709e-02,  2.6414e-01],\n",
      "        [ 1.6438e+00, -1.3198e-01,  1.1606e+00,  5.2741e-01],\n",
      "        [ 1.0380e+00, -1.3198e-01,  7.0592e-01,  6.5904e-01],\n",
      "        [-1.3854e+00,  3.2841e-01, -1.3971e+00, -1.3154e+00],\n",
      "        [-1.7367e-01, -1.3198e-01,  2.5122e-01,  8.7755e-04],\n",
      "        [-1.6277e+00, -1.7434e+00, -1.3971e+00, -1.1838e+00]])\n",
      "tensor([1, 0, 0, 0, 1, 2, 1, 0, 1, 0])\n",
      "8\n",
      "tensor([[-7.7951e-01,  1.0190e+00, -1.2834e+00, -1.3154e+00],\n",
      "        [-5.3718e-01,  1.9398e+00, -1.3971e+00, -1.0522e+00],\n",
      "        [ 5.5333e-01, -3.6218e-01,  1.0469e+00,  7.9067e-01],\n",
      "        [ 4.3217e-01,  7.8881e-01,  9.3327e-01,  1.4488e+00],\n",
      "        [ 1.8983e-01,  7.8881e-01,  4.2173e-01,  5.2741e-01],\n",
      "        [ 2.2497e+00, -1.3198e-01,  1.3311e+00,  1.4488e+00],\n",
      "        [-4.1601e-01, -1.0528e+00,  3.6490e-01,  8.7755e-04],\n",
      "        [ 7.9567e-01,  3.2841e-01,  7.6276e-01,  1.0539e+00],\n",
      "        [ 6.8662e-02, -1.3198e-01,  2.5122e-01,  3.9577e-01],\n",
      "        [-9.0068e-01,  1.0190e+00, -1.3402e+00, -1.3154e+00]])\n",
      "tensor([0, 0, 2, 2, 1, 2, 1, 2, 1, 0])\n",
      "9\n",
      "tensor([[-0.9007,  0.5586, -1.1697, -0.9205],\n",
      "        [ 2.2497, -1.0528,  1.7858,  1.4488],\n",
      "        [ 1.8862, -0.5924,  1.3311,  0.9223],\n",
      "        [ 1.4015,  0.3284,  0.5354,  0.2641],\n",
      "        [ 2.4920,  1.7096,  1.5016,  1.0539],\n",
      "        [ 0.5533, -1.2830,  0.6491,  0.3958],\n",
      "        [ 0.0687,  0.3284,  0.5922,  0.7907],\n",
      "        [ 0.1898, -0.1320,  0.5922,  0.7907],\n",
      "        [ 0.4322, -0.5924,  0.5922,  0.7907],\n",
      "        [ 0.5533,  0.5586,  1.2743,  1.7121]])\n",
      "tensor([0, 2, 2, 1, 2, 1, 1, 2, 2, 2])\n",
      "10\n",
      "tensor([[ 0.9168, -0.3622,  0.4786,  0.1325],\n",
      "        [ 1.1592,  0.3284,  1.2175,  1.4488],\n",
      "        [ 1.6438,  0.3284,  1.2743,  0.7907],\n",
      "        [-1.7489, -0.3622, -1.3402, -1.3154],\n",
      "        [-0.0525, -0.8226,  0.1944, -0.2624],\n",
      "        [ 1.0380,  0.5586,  1.1038,  1.7121],\n",
      "        [-0.7795,  2.4002, -1.2834, -1.4471],\n",
      "        [-1.0218, -2.4339, -0.1466, -0.2624],\n",
      "        [-0.2948, -1.2830,  0.0807, -0.1308],\n",
      "        [ 0.3110, -0.5924,  0.1375,  0.1325]])\n",
      "tensor([1, 2, 2, 0, 1, 2, 0, 1, 1, 1])\n",
      "11\n",
      "tensor([[ 1.7650, -0.3622,  1.4448,  0.7907],\n",
      "        [ 0.3110, -0.1320,  0.6491,  0.7907],\n",
      "        [-1.8700, -0.1320, -1.5107, -1.4471],\n",
      "        [-0.7795,  0.7888, -1.3402, -1.3154],\n",
      "        [-0.0525,  2.1700, -1.4539, -1.3154],\n",
      "        [-0.4160,  1.0190, -1.3971, -1.3154],\n",
      "        [ 0.7957, -0.5924,  0.4786,  0.3958],\n",
      "        [ 0.3110, -1.0528,  1.0469,  0.2641],\n",
      "        [-1.5065,  0.3284, -1.3402, -1.3154],\n",
      "        [-0.5372,  1.4794, -1.2834, -1.3154]])\n",
      "tensor([2, 2, 0, 0, 0, 0, 1, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i, (xdata, tdata) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(xdata)\n",
    "    print(tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Layer Perceptron Network\n",
    "n_hidden = 2\n",
    "class AENet (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AENet, self).__init__()\n",
    "        self.fc1 = nn.Linear(mdim, n_hidden)\n",
    "#        self.bn1 = nn.BatchNorm1d(num_features=n_hidden, affine=True)\n",
    "#        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(n_hidden, mdim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "#        y = self.bn1(y)\n",
    "#        y = self.dropout1(y)\n",
    "#        y = y + 0.1*torch.randn(n_hidden).cuda()\n",
    "        z = self.fc2(y)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# select device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = AENet().to(device)\n",
    "print(device)\n",
    "\n",
    "# optimizing\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0000001)\n",
    "#optimizer = optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start ...\n",
      "Epoch [1/500] train Loss: 0.00437227 \n",
      "Epoch [2/500] train Loss: 0.00437133 \n",
      "Epoch [3/500] train Loss: 0.00437054 \n",
      "Epoch [4/500] train Loss: 0.00436959 \n",
      "Epoch [5/500] train Loss: 0.00436874 \n",
      "Epoch [6/500] train Loss: 0.00436797 \n",
      "Epoch [7/500] train Loss: 0.00436708 \n",
      "Epoch [8/500] train Loss: 0.00436640 \n",
      "Epoch [9/500] train Loss: 0.00436556 \n",
      "Epoch [10/500] train Loss: 0.00436462 \n",
      "Epoch [11/500] train Loss: 0.00436382 \n",
      "Epoch [12/500] train Loss: 0.00436309 \n",
      "Epoch [13/500] train Loss: 0.00436231 \n",
      "Epoch [14/500] train Loss: 0.00436149 \n",
      "Epoch [15/500] train Loss: 0.00436069 \n",
      "Epoch [16/500] train Loss: 0.00436003 \n",
      "Epoch [17/500] train Loss: 0.00435925 \n",
      "Epoch [18/500] train Loss: 0.00435856 \n",
      "Epoch [19/500] train Loss: 0.00435773 \n",
      "Epoch [20/500] train Loss: 0.00435702 \n",
      "Epoch [21/500] train Loss: 0.00435631 \n",
      "Epoch [22/500] train Loss: 0.00435556 \n",
      "Epoch [23/500] train Loss: 0.00435482 \n",
      "Epoch [24/500] train Loss: 0.00435415 \n",
      "Epoch [25/500] train Loss: 0.00435341 \n",
      "Epoch [26/500] train Loss: 0.00435274 \n",
      "Epoch [27/500] train Loss: 0.00435199 \n",
      "Epoch [28/500] train Loss: 0.00435135 \n",
      "Epoch [29/500] train Loss: 0.00435065 \n",
      "Epoch [30/500] train Loss: 0.00435011 \n",
      "Epoch [31/500] train Loss: 0.00434927 \n",
      "Epoch [32/500] train Loss: 0.00434866 \n",
      "Epoch [33/500] train Loss: 0.00434795 \n",
      "Epoch [34/500] train Loss: 0.00434721 \n",
      "Epoch [35/500] train Loss: 0.00434664 \n",
      "Epoch [36/500] train Loss: 0.00434594 \n",
      "Epoch [37/500] train Loss: 0.00434526 \n",
      "Epoch [38/500] train Loss: 0.00434464 \n",
      "Epoch [39/500] train Loss: 0.00434399 \n",
      "Epoch [40/500] train Loss: 0.00434350 \n",
      "Epoch [41/500] train Loss: 0.00434273 \n",
      "Epoch [42/500] train Loss: 0.00434217 \n",
      "Epoch [43/500] train Loss: 0.00434149 \n",
      "Epoch [44/500] train Loss: 0.00434077 \n",
      "Epoch [45/500] train Loss: 0.00434017 \n",
      "Epoch [46/500] train Loss: 0.00433959 \n",
      "Epoch [47/500] train Loss: 0.00433897 \n",
      "Epoch [48/500] train Loss: 0.00433834 \n",
      "Epoch [49/500] train Loss: 0.00433777 \n",
      "Epoch [50/500] train Loss: 0.00433712 \n",
      "Epoch [51/500] train Loss: 0.00433667 \n",
      "Epoch [52/500] train Loss: 0.00433601 \n",
      "Epoch [53/500] train Loss: 0.00433543 \n",
      "Epoch [54/500] train Loss: 0.00433480 \n",
      "Epoch [55/500] train Loss: 0.00433421 \n",
      "Epoch [56/500] train Loss: 0.00433364 \n",
      "Epoch [57/500] train Loss: 0.00433317 \n",
      "Epoch [58/500] train Loss: 0.00433253 \n",
      "Epoch [59/500] train Loss: 0.00433195 \n",
      "Epoch [60/500] train Loss: 0.00433136 \n",
      "Epoch [61/500] train Loss: 0.00433079 \n",
      "Epoch [62/500] train Loss: 0.00433023 \n",
      "Epoch [63/500] train Loss: 0.00432966 \n",
      "Epoch [64/500] train Loss: 0.00432918 \n",
      "Epoch [65/500] train Loss: 0.00432857 \n",
      "Epoch [66/500] train Loss: 0.00432798 \n",
      "Epoch [67/500] train Loss: 0.00432749 \n",
      "Epoch [68/500] train Loss: 0.00432689 \n",
      "Epoch [69/500] train Loss: 0.00432636 \n",
      "Epoch [70/500] train Loss: 0.00432581 \n",
      "Epoch [71/500] train Loss: 0.00432528 \n",
      "Epoch [72/500] train Loss: 0.00432478 \n",
      "Epoch [73/500] train Loss: 0.00432429 \n",
      "Epoch [74/500] train Loss: 0.00432374 \n",
      "Epoch [75/500] train Loss: 0.00432321 \n",
      "Epoch [76/500] train Loss: 0.00432267 \n",
      "Epoch [77/500] train Loss: 0.00432218 \n",
      "Epoch [78/500] train Loss: 0.00432165 \n",
      "Epoch [79/500] train Loss: 0.00432122 \n",
      "Epoch [80/500] train Loss: 0.00432069 \n",
      "Epoch [81/500] train Loss: 0.00432010 \n",
      "Epoch [82/500] train Loss: 0.00431973 \n",
      "Epoch [83/500] train Loss: 0.00431916 \n",
      "Epoch [84/500] train Loss: 0.00431865 \n",
      "Epoch [85/500] train Loss: 0.00431810 \n",
      "Epoch [86/500] train Loss: 0.00431765 \n",
      "Epoch [87/500] train Loss: 0.00431714 \n",
      "Epoch [88/500] train Loss: 0.00431670 \n",
      "Epoch [89/500] train Loss: 0.00431614 \n",
      "Epoch [90/500] train Loss: 0.00431567 \n",
      "Epoch [91/500] train Loss: 0.00431523 \n",
      "Epoch [92/500] train Loss: 0.00431486 \n",
      "Epoch [93/500] train Loss: 0.00431423 \n",
      "Epoch [94/500] train Loss: 0.00431375 \n",
      "Epoch [95/500] train Loss: 0.00431334 \n",
      "Epoch [96/500] train Loss: 0.00431281 \n",
      "Epoch [97/500] train Loss: 0.00431234 \n",
      "Epoch [98/500] train Loss: 0.00431187 \n",
      "Epoch [99/500] train Loss: 0.00431139 \n",
      "Epoch [100/500] train Loss: 0.00431091 \n",
      "Epoch [101/500] train Loss: 0.00431048 \n",
      "Epoch [102/500] train Loss: 0.00431000 \n",
      "Epoch [103/500] train Loss: 0.00430966 \n",
      "Epoch [104/500] train Loss: 0.00430910 \n",
      "Epoch [105/500] train Loss: 0.00430862 \n",
      "Epoch [106/500] train Loss: 0.00430816 \n",
      "Epoch [107/500] train Loss: 0.00430774 \n",
      "Epoch [108/500] train Loss: 0.00430732 \n",
      "Epoch [109/500] train Loss: 0.00430689 \n",
      "Epoch [110/500] train Loss: 0.00430641 \n",
      "Epoch [111/500] train Loss: 0.00430599 \n",
      "Epoch [112/500] train Loss: 0.00430558 \n",
      "Epoch [113/500] train Loss: 0.00430513 \n",
      "Epoch [114/500] train Loss: 0.00430471 \n",
      "Epoch [115/500] train Loss: 0.00430426 \n",
      "Epoch [116/500] train Loss: 0.00430379 \n",
      "Epoch [117/500] train Loss: 0.00430338 \n",
      "Epoch [118/500] train Loss: 0.00430305 \n",
      "Epoch [119/500] train Loss: 0.00430256 \n",
      "Epoch [120/500] train Loss: 0.00430217 \n",
      "Epoch [121/500] train Loss: 0.00430168 \n",
      "Epoch [122/500] train Loss: 0.00430132 \n",
      "Epoch [123/500] train Loss: 0.00430086 \n",
      "Epoch [124/500] train Loss: 0.00430045 \n",
      "Epoch [125/500] train Loss: 0.00430002 \n",
      "Epoch [126/500] train Loss: 0.00429961 \n",
      "Epoch [127/500] train Loss: 0.00429916 \n",
      "Epoch [128/500] train Loss: 0.00429874 \n",
      "Epoch [129/500] train Loss: 0.00429839 \n",
      "Epoch [130/500] train Loss: 0.00429798 \n",
      "Epoch [131/500] train Loss: 0.00429759 \n",
      "Epoch [132/500] train Loss: 0.00429716 \n",
      "Epoch [133/500] train Loss: 0.00429677 \n",
      "Epoch [134/500] train Loss: 0.00429635 \n",
      "Epoch [135/500] train Loss: 0.00429594 \n",
      "Epoch [136/500] train Loss: 0.00429559 \n",
      "Epoch [137/500] train Loss: 0.00429519 \n",
      "Epoch [138/500] train Loss: 0.00429476 \n",
      "Epoch [139/500] train Loss: 0.00429442 \n",
      "Epoch [140/500] train Loss: 0.00429402 \n",
      "Epoch [141/500] train Loss: 0.00429365 \n",
      "Epoch [142/500] train Loss: 0.00429320 \n",
      "Epoch [143/500] train Loss: 0.00429283 \n",
      "Epoch [144/500] train Loss: 0.00429243 \n",
      "Epoch [145/500] train Loss: 0.00429207 \n",
      "Epoch [146/500] train Loss: 0.00429169 \n",
      "Epoch [147/500] train Loss: 0.00429134 \n",
      "Epoch [148/500] train Loss: 0.00429100 \n",
      "Epoch [149/500] train Loss: 0.00429058 \n",
      "Epoch [150/500] train Loss: 0.00429020 \n",
      "Epoch [151/500] train Loss: 0.00428980 \n",
      "Epoch [152/500] train Loss: 0.00428959 \n",
      "Epoch [153/500] train Loss: 0.00428905 \n",
      "Epoch [154/500] train Loss: 0.00428869 \n",
      "Epoch [155/500] train Loss: 0.00428834 \n",
      "Epoch [156/500] train Loss: 0.00428798 \n",
      "Epoch [157/500] train Loss: 0.00428763 \n",
      "Epoch [158/500] train Loss: 0.00428723 \n",
      "Epoch [159/500] train Loss: 0.00428691 \n",
      "Epoch [160/500] train Loss: 0.00428652 \n",
      "Epoch [161/500] train Loss: 0.00428619 \n",
      "Epoch [162/500] train Loss: 0.00428583 \n",
      "Epoch [163/500] train Loss: 0.00428551 \n",
      "Epoch [164/500] train Loss: 0.00428509 \n",
      "Epoch [165/500] train Loss: 0.00428477 \n",
      "Epoch [166/500] train Loss: 0.00428439 \n",
      "Epoch [167/500] train Loss: 0.00428405 \n",
      "Epoch [168/500] train Loss: 0.00428369 \n",
      "Epoch [169/500] train Loss: 0.00428334 \n",
      "Epoch [170/500] train Loss: 0.00428301 \n",
      "Epoch [171/500] train Loss: 0.00428264 \n",
      "Epoch [172/500] train Loss: 0.00428231 \n",
      "Epoch [173/500] train Loss: 0.00428195 \n",
      "Epoch [174/500] train Loss: 0.00428165 \n",
      "Epoch [175/500] train Loss: 0.00428133 \n",
      "Epoch [176/500] train Loss: 0.00428096 \n",
      "Epoch [177/500] train Loss: 0.00428061 \n",
      "Epoch [178/500] train Loss: 0.00428025 \n",
      "Epoch [179/500] train Loss: 0.00427999 \n",
      "Epoch [180/500] train Loss: 0.00427959 \n",
      "Epoch [181/500] train Loss: 0.00427927 \n",
      "Epoch [182/500] train Loss: 0.00427894 \n",
      "Epoch [183/500] train Loss: 0.00427858 \n",
      "Epoch [184/500] train Loss: 0.00427827 \n",
      "Epoch [185/500] train Loss: 0.00427794 \n",
      "Epoch [186/500] train Loss: 0.00427764 \n",
      "Epoch [187/500] train Loss: 0.00427736 \n",
      "Epoch [188/500] train Loss: 0.00427698 \n",
      "Epoch [189/500] train Loss: 0.00427667 \n",
      "Epoch [190/500] train Loss: 0.00427631 \n",
      "Epoch [191/500] train Loss: 0.00427598 \n",
      "Epoch [192/500] train Loss: 0.00427576 \n",
      "Epoch [193/500] train Loss: 0.00427537 \n",
      "Epoch [194/500] train Loss: 0.00427503 \n",
      "Epoch [195/500] train Loss: 0.00427474 \n",
      "Epoch [196/500] train Loss: 0.00427445 \n",
      "Epoch [197/500] train Loss: 0.00427409 \n",
      "Epoch [198/500] train Loss: 0.00427376 \n",
      "Epoch [199/500] train Loss: 0.00427345 \n",
      "Epoch [200/500] train Loss: 0.00427315 \n",
      "Epoch [201/500] train Loss: 0.00427290 \n",
      "Epoch [202/500] train Loss: 0.00427253 \n",
      "Epoch [203/500] train Loss: 0.00427222 \n",
      "Epoch [204/500] train Loss: 0.00427199 \n",
      "Epoch [205/500] train Loss: 0.00427158 \n",
      "Epoch [206/500] train Loss: 0.00427129 \n",
      "Epoch [207/500] train Loss: 0.00427098 \n",
      "Epoch [208/500] train Loss: 0.00427071 \n",
      "Epoch [209/500] train Loss: 0.00427039 \n",
      "Epoch [210/500] train Loss: 0.00427010 \n",
      "Epoch [211/500] train Loss: 0.00426982 \n",
      "Epoch [212/500] train Loss: 0.00426950 \n",
      "Epoch [213/500] train Loss: 0.00426922 \n",
      "Epoch [214/500] train Loss: 0.00426890 \n",
      "Epoch [215/500] train Loss: 0.00426861 \n",
      "Epoch [216/500] train Loss: 0.00426830 \n",
      "Epoch [217/500] train Loss: 0.00426806 \n",
      "Epoch [218/500] train Loss: 0.00426769 \n",
      "Epoch [219/500] train Loss: 0.00426740 \n",
      "Epoch [220/500] train Loss: 0.00426713 \n",
      "Epoch [221/500] train Loss: 0.00426690 \n",
      "Epoch [222/500] train Loss: 0.00426654 \n",
      "Epoch [223/500] train Loss: 0.00426628 \n",
      "Epoch [224/500] train Loss: 0.00426599 \n",
      "Epoch [225/500] train Loss: 0.00426570 \n",
      "Epoch [226/500] train Loss: 0.00426547 \n",
      "Epoch [227/500] train Loss: 0.00426511 \n",
      "Epoch [228/500] train Loss: 0.00426483 \n",
      "Epoch [229/500] train Loss: 0.00426456 \n",
      "Epoch [230/500] train Loss: 0.00426429 \n",
      "Epoch [231/500] train Loss: 0.00426400 \n",
      "Epoch [232/500] train Loss: 0.00426379 \n",
      "Epoch [233/500] train Loss: 0.00426347 \n",
      "Epoch [234/500] train Loss: 0.00426316 \n",
      "Epoch [235/500] train Loss: 0.00426291 \n",
      "Epoch [236/500] train Loss: 0.00426260 \n",
      "Epoch [237/500] train Loss: 0.00426237 \n",
      "Epoch [238/500] train Loss: 0.00426204 \n",
      "Epoch [239/500] train Loss: 0.00426181 \n",
      "Epoch [240/500] train Loss: 0.00426151 \n",
      "Epoch [241/500] train Loss: 0.00426123 \n",
      "Epoch [242/500] train Loss: 0.00426098 \n",
      "Epoch [243/500] train Loss: 0.00426069 \n",
      "Epoch [244/500] train Loss: 0.00426048 \n",
      "Epoch [245/500] train Loss: 0.00426019 \n",
      "Epoch [246/500] train Loss: 0.00425988 \n",
      "Epoch [247/500] train Loss: 0.00425960 \n",
      "Epoch [248/500] train Loss: 0.00425935 \n",
      "Epoch [249/500] train Loss: 0.00425909 \n",
      "Epoch [250/500] train Loss: 0.00425889 \n",
      "Epoch [251/500] train Loss: 0.00425858 \n",
      "Epoch [252/500] train Loss: 0.00425838 \n",
      "Epoch [253/500] train Loss: 0.00425806 \n",
      "Epoch [254/500] train Loss: 0.00425776 \n",
      "Epoch [255/500] train Loss: 0.00425753 \n",
      "Epoch [256/500] train Loss: 0.00425727 \n",
      "Epoch [257/500] train Loss: 0.00425704 \n",
      "Epoch [258/500] train Loss: 0.00425682 \n",
      "Epoch [259/500] train Loss: 0.00425658 \n",
      "Epoch [260/500] train Loss: 0.00425623 \n",
      "Epoch [261/500] train Loss: 0.00425600 \n",
      "Epoch [262/500] train Loss: 0.00425573 \n",
      "Epoch [263/500] train Loss: 0.00425551 \n",
      "Epoch [264/500] train Loss: 0.00425521 \n",
      "Epoch [265/500] train Loss: 0.00425496 \n",
      "Epoch [266/500] train Loss: 0.00425471 \n",
      "Epoch [267/500] train Loss: 0.00425454 \n",
      "Epoch [268/500] train Loss: 0.00425423 \n",
      "Epoch [269/500] train Loss: 0.00425400 \n",
      "Epoch [270/500] train Loss: 0.00425373 \n",
      "Epoch [271/500] train Loss: 0.00425345 \n",
      "Epoch [272/500] train Loss: 0.00425324 \n",
      "Epoch [273/500] train Loss: 0.00425306 \n",
      "Epoch [274/500] train Loss: 0.00425277 \n",
      "Epoch [275/500] train Loss: 0.00425252 \n",
      "Epoch [276/500] train Loss: 0.00425227 \n",
      "Epoch [277/500] train Loss: 0.00425206 \n",
      "Epoch [278/500] train Loss: 0.00425177 \n",
      "Epoch [279/500] train Loss: 0.00425158 \n",
      "Epoch [280/500] train Loss: 0.00425133 \n",
      "Epoch [281/500] train Loss: 0.00425106 \n",
      "Epoch [282/500] train Loss: 0.00425084 \n",
      "Epoch [283/500] train Loss: 0.00425058 \n",
      "Epoch [284/500] train Loss: 0.00425036 \n",
      "Epoch [285/500] train Loss: 0.00425008 \n",
      "Epoch [286/500] train Loss: 0.00424996 \n",
      "Epoch [287/500] train Loss: 0.00424964 \n",
      "Epoch [288/500] train Loss: 0.00424942 \n",
      "Epoch [289/500] train Loss: 0.00424919 \n",
      "Epoch [290/500] train Loss: 0.00424892 \n",
      "Epoch [291/500] train Loss: 0.00424869 \n",
      "Epoch [292/500] train Loss: 0.00424843 \n",
      "Epoch [293/500] train Loss: 0.00424821 \n",
      "Epoch [294/500] train Loss: 0.00424799 \n",
      "Epoch [295/500] train Loss: 0.00424782 \n",
      "Epoch [296/500] train Loss: 0.00424753 \n",
      "Epoch [297/500] train Loss: 0.00424730 \n",
      "Epoch [298/500] train Loss: 0.00424711 \n",
      "Epoch [299/500] train Loss: 0.00424685 \n",
      "Epoch [300/500] train Loss: 0.00424663 \n",
      "Epoch [301/500] train Loss: 0.00424641 \n",
      "Epoch [302/500] train Loss: 0.00424620 \n",
      "Epoch [303/500] train Loss: 0.00424599 \n",
      "Epoch [304/500] train Loss: 0.00424575 \n",
      "Epoch [305/500] train Loss: 0.00424558 \n",
      "Epoch [306/500] train Loss: 0.00424534 \n",
      "Epoch [307/500] train Loss: 0.00424511 \n",
      "Epoch [308/500] train Loss: 0.00424485 \n",
      "Epoch [309/500] train Loss: 0.00424469 \n",
      "Epoch [310/500] train Loss: 0.00424442 \n",
      "Epoch [311/500] train Loss: 0.00424424 \n",
      "Epoch [312/500] train Loss: 0.00424399 \n",
      "Epoch [313/500] train Loss: 0.00424376 \n",
      "Epoch [314/500] train Loss: 0.00424354 \n",
      "Epoch [315/500] train Loss: 0.00424333 \n",
      "Epoch [316/500] train Loss: 0.00424310 \n",
      "Epoch [317/500] train Loss: 0.00424289 \n",
      "Epoch [318/500] train Loss: 0.00424270 \n",
      "Epoch [319/500] train Loss: 0.00424250 \n",
      "Epoch [320/500] train Loss: 0.00424225 \n",
      "Epoch [321/500] train Loss: 0.00424206 \n",
      "Epoch [322/500] train Loss: 0.00424183 \n",
      "Epoch [323/500] train Loss: 0.00424163 \n",
      "Epoch [324/500] train Loss: 0.00424144 \n",
      "Epoch [325/500] train Loss: 0.00424120 \n",
      "Epoch [326/500] train Loss: 0.00424100 \n",
      "Epoch [327/500] train Loss: 0.00424090 \n",
      "Epoch [328/500] train Loss: 0.00424060 \n",
      "Epoch [329/500] train Loss: 0.00424038 \n",
      "Epoch [330/500] train Loss: 0.00424015 \n",
      "Epoch [331/500] train Loss: 0.00423996 \n",
      "Epoch [332/500] train Loss: 0.00423973 \n",
      "Epoch [333/500] train Loss: 0.00423953 \n",
      "Epoch [334/500] train Loss: 0.00423934 \n",
      "Epoch [335/500] train Loss: 0.00423917 \n",
      "Epoch [336/500] train Loss: 0.00423892 \n",
      "Epoch [337/500] train Loss: 0.00423876 \n",
      "Epoch [338/500] train Loss: 0.00423854 \n",
      "Epoch [339/500] train Loss: 0.00423837 \n",
      "Epoch [340/500] train Loss: 0.00423816 \n",
      "Epoch [341/500] train Loss: 0.00423798 \n",
      "Epoch [342/500] train Loss: 0.00423774 \n",
      "Epoch [343/500] train Loss: 0.00423754 \n",
      "Epoch [344/500] train Loss: 0.00423734 \n",
      "Epoch [345/500] train Loss: 0.00423715 \n",
      "Epoch [346/500] train Loss: 0.00423702 \n",
      "Epoch [347/500] train Loss: 0.00423673 \n",
      "Epoch [348/500] train Loss: 0.00423664 \n",
      "Epoch [349/500] train Loss: 0.00423642 \n",
      "Epoch [350/500] train Loss: 0.00423615 \n",
      "Epoch [351/500] train Loss: 0.00423597 \n",
      "Epoch [352/500] train Loss: 0.00423576 \n",
      "Epoch [353/500] train Loss: 0.00423557 \n",
      "Epoch [354/500] train Loss: 0.00423538 \n",
      "Epoch [355/500] train Loss: 0.00423521 \n",
      "Epoch [356/500] train Loss: 0.00423503 \n",
      "Epoch [357/500] train Loss: 0.00423481 \n",
      "Epoch [358/500] train Loss: 0.00423465 \n",
      "Epoch [359/500] train Loss: 0.00423445 \n",
      "Epoch [360/500] train Loss: 0.00423423 \n",
      "Epoch [361/500] train Loss: 0.00423406 \n",
      "Epoch [362/500] train Loss: 0.00423387 \n",
      "Epoch [363/500] train Loss: 0.00423375 \n",
      "Epoch [364/500] train Loss: 0.00423350 \n",
      "Epoch [365/500] train Loss: 0.00423335 \n",
      "Epoch [366/500] train Loss: 0.00423316 \n",
      "Epoch [367/500] train Loss: 0.00423296 \n",
      "Epoch [368/500] train Loss: 0.00423278 \n",
      "Epoch [369/500] train Loss: 0.00423257 \n",
      "Epoch [370/500] train Loss: 0.00423244 \n",
      "Epoch [371/500] train Loss: 0.00423226 \n",
      "Epoch [372/500] train Loss: 0.00423219 \n",
      "Epoch [373/500] train Loss: 0.00423183 \n",
      "Epoch [374/500] train Loss: 0.00423165 \n",
      "Epoch [375/500] train Loss: 0.00423145 \n",
      "Epoch [376/500] train Loss: 0.00423132 \n",
      "Epoch [377/500] train Loss: 0.00423108 \n",
      "Epoch [378/500] train Loss: 0.00423091 \n",
      "Epoch [379/500] train Loss: 0.00423073 \n",
      "Epoch [380/500] train Loss: 0.00423056 \n",
      "Epoch [381/500] train Loss: 0.00423037 \n",
      "Epoch [382/500] train Loss: 0.00423026 \n",
      "Epoch [383/500] train Loss: 0.00423003 \n",
      "Epoch [384/500] train Loss: 0.00422983 \n",
      "Epoch [385/500] train Loss: 0.00422968 \n",
      "Epoch [386/500] train Loss: 0.00422959 \n",
      "Epoch [387/500] train Loss: 0.00422930 \n",
      "Epoch [388/500] train Loss: 0.00422913 \n",
      "Epoch [389/500] train Loss: 0.00422897 \n",
      "Epoch [390/500] train Loss: 0.00422882 \n",
      "Epoch [391/500] train Loss: 0.00422861 \n",
      "Epoch [392/500] train Loss: 0.00422850 \n",
      "Epoch [393/500] train Loss: 0.00422828 \n",
      "Epoch [394/500] train Loss: 0.00422811 \n",
      "Epoch [395/500] train Loss: 0.00422797 \n",
      "Epoch [396/500] train Loss: 0.00422776 \n",
      "Epoch [397/500] train Loss: 0.00422758 \n",
      "Epoch [398/500] train Loss: 0.00422742 \n",
      "Epoch [399/500] train Loss: 0.00422728 \n",
      "Epoch [400/500] train Loss: 0.00422707 \n",
      "Epoch [401/500] train Loss: 0.00422693 \n",
      "Epoch [402/500] train Loss: 0.00422677 \n",
      "Epoch [403/500] train Loss: 0.00422657 \n",
      "Epoch [404/500] train Loss: 0.00422640 \n",
      "Epoch [405/500] train Loss: 0.00422623 \n",
      "Epoch [406/500] train Loss: 0.00422605 \n",
      "Epoch [407/500] train Loss: 0.00422594 \n",
      "Epoch [408/500] train Loss: 0.00422572 \n",
      "Epoch [409/500] train Loss: 0.00422556 \n",
      "Epoch [410/500] train Loss: 0.00422544 \n",
      "Epoch [411/500] train Loss: 0.00422525 \n",
      "Epoch [412/500] train Loss: 0.00422507 \n",
      "Epoch [413/500] train Loss: 0.00422497 \n",
      "Epoch [414/500] train Loss: 0.00422476 \n",
      "Epoch [415/500] train Loss: 0.00422457 \n",
      "Epoch [416/500] train Loss: 0.00422443 \n",
      "Epoch [417/500] train Loss: 0.00422427 \n",
      "Epoch [418/500] train Loss: 0.00422415 \n",
      "Epoch [419/500] train Loss: 0.00422397 \n",
      "Epoch [420/500] train Loss: 0.00422378 \n",
      "Epoch [421/500] train Loss: 0.00422362 \n",
      "Epoch [422/500] train Loss: 0.00422348 \n",
      "Epoch [423/500] train Loss: 0.00422331 \n",
      "Epoch [424/500] train Loss: 0.00422315 \n",
      "Epoch [425/500] train Loss: 0.00422300 \n",
      "Epoch [426/500] train Loss: 0.00422282 \n",
      "Epoch [427/500] train Loss: 0.00422269 \n",
      "Epoch [428/500] train Loss: 0.00422255 \n",
      "Epoch [429/500] train Loss: 0.00422242 \n",
      "Epoch [430/500] train Loss: 0.00422220 \n",
      "Epoch [431/500] train Loss: 0.00422206 \n",
      "Epoch [432/500] train Loss: 0.00422190 \n",
      "Epoch [433/500] train Loss: 0.00422173 \n",
      "Epoch [434/500] train Loss: 0.00422161 \n",
      "Epoch [435/500] train Loss: 0.00422142 \n",
      "Epoch [436/500] train Loss: 0.00422129 \n",
      "Epoch [437/500] train Loss: 0.00422111 \n",
      "Epoch [438/500] train Loss: 0.00422096 \n",
      "Epoch [439/500] train Loss: 0.00422083 \n",
      "Epoch [440/500] train Loss: 0.00422066 \n",
      "Epoch [441/500] train Loss: 0.00422058 \n",
      "Epoch [442/500] train Loss: 0.00422036 \n",
      "Epoch [443/500] train Loss: 0.00422021 \n",
      "Epoch [444/500] train Loss: 0.00422006 \n",
      "Epoch [445/500] train Loss: 0.00421992 \n",
      "Epoch [446/500] train Loss: 0.00421977 \n",
      "Epoch [447/500] train Loss: 0.00421963 \n",
      "Epoch [448/500] train Loss: 0.00421945 \n",
      "Epoch [449/500] train Loss: 0.00421930 \n",
      "Epoch [450/500] train Loss: 0.00421917 \n",
      "Epoch [451/500] train Loss: 0.00421902 \n",
      "Epoch [452/500] train Loss: 0.00421888 \n",
      "Epoch [453/500] train Loss: 0.00421875 \n",
      "Epoch [454/500] train Loss: 0.00421868 \n",
      "Epoch [455/500] train Loss: 0.00421849 \n",
      "Epoch [456/500] train Loss: 0.00421834 \n",
      "Epoch [457/500] train Loss: 0.00421819 \n",
      "Epoch [458/500] train Loss: 0.00421802 \n",
      "Epoch [459/500] train Loss: 0.00421787 \n",
      "Epoch [460/500] train Loss: 0.00421772 \n",
      "Epoch [461/500] train Loss: 0.00421757 \n",
      "Epoch [462/500] train Loss: 0.00421743 \n",
      "Epoch [463/500] train Loss: 0.00421740 \n",
      "Epoch [464/500] train Loss: 0.00421714 \n",
      "Epoch [465/500] train Loss: 0.00421699 \n",
      "Epoch [466/500] train Loss: 0.00421686 \n",
      "Epoch [467/500] train Loss: 0.00421671 \n",
      "Epoch [468/500] train Loss: 0.00421657 \n",
      "Epoch [469/500] train Loss: 0.00421643 \n",
      "Epoch [470/500] train Loss: 0.00421633 \n",
      "Epoch [471/500] train Loss: 0.00421618 \n",
      "Epoch [472/500] train Loss: 0.00421617 \n",
      "Epoch [473/500] train Loss: 0.00421592 \n",
      "Epoch [474/500] train Loss: 0.00421576 \n",
      "Epoch [475/500] train Loss: 0.00421560 \n",
      "Epoch [476/500] train Loss: 0.00421545 \n",
      "Epoch [477/500] train Loss: 0.00421532 \n",
      "Epoch [478/500] train Loss: 0.00421521 \n",
      "Epoch [479/500] train Loss: 0.00421505 \n",
      "Epoch [480/500] train Loss: 0.00421498 \n",
      "Epoch [481/500] train Loss: 0.00421480 \n",
      "Epoch [482/500] train Loss: 0.00421464 \n",
      "Epoch [483/500] train Loss: 0.00421453 \n",
      "Epoch [484/500] train Loss: 0.00421437 \n",
      "Epoch [485/500] train Loss: 0.00421425 \n",
      "Epoch [486/500] train Loss: 0.00421413 \n",
      "Epoch [487/500] train Loss: 0.00421401 \n",
      "Epoch [488/500] train Loss: 0.00421388 \n",
      "Epoch [489/500] train Loss: 0.00421371 \n",
      "Epoch [490/500] train Loss: 0.00421374 \n",
      "Epoch [491/500] train Loss: 0.00421350 \n",
      "Epoch [492/500] train Loss: 0.00421344 \n",
      "Epoch [493/500] train Loss: 0.00421317 \n",
      "Epoch [494/500] train Loss: 0.00421306 \n",
      "Epoch [495/500] train Loss: 0.00421300 \n",
      "Epoch [496/500] train Loss: 0.00421296 \n",
      "Epoch [497/500] train Loss: 0.00421265 \n",
      "Epoch [498/500] train Loss: 0.00421254 \n",
      "Epoch [499/500] train Loss: 0.00421239 \n",
      "Epoch [500/500] train Loss: 0.00421226 \n"
     ]
    }
   ],
   "source": [
    "###  training\n",
    "print ('training start ...')\n",
    "num_epochs = 500   \n",
    "\n",
    "#lrs = []\n",
    "\n",
    "# initialize list for plot graph after training\n",
    "train_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize each epoch\n",
    "    train_loss = 0\n",
    "    \n",
    "    # ======== train_mode (Backprop) ======\n",
    "    net.train()\n",
    "    for i, (xdata, tdata) in enumerate(train_loader):  # get mini batch samples\n",
    "#        print(i, xdata)\n",
    "        xdata = xdata.to(device)\n",
    "        optimizer.zero_grad()  # Reset the gradients\n",
    "        outputs = net(xdata)  # forward computation\n",
    "        loss = criterion(outputs, xdata)  # loss\n",
    "        loss.backward()  # backward computation        \n",
    "        optimizer.step()  # optimization\n",
    "\n",
    "    # ======== update learning rate ======\n",
    "#    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "#    print(\"Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n",
    "#    scheduler.step()\n",
    "\n",
    "    \n",
    "    # ======== eval_mode (training samples) ======\n",
    "    net.eval()\n",
    "    with torch.no_grad():  # no computation of gradients\n",
    "      for (xdata, tdata) in train_loader:        \n",
    "          xdata = xdata.to(device)\n",
    "          outputs = net(xdata)\n",
    "          loss = criterion(outputs, xdata)\n",
    "          train_loss += loss.item()\n",
    "#          acc = (outputs.max(1)[1] == labels).sum()\n",
    "#          train_acc += acc.item()\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "#    avg_train_acc = train_acc / len(train_loader.dataset)\n",
    "    \n",
    "    # print log\n",
    "    print ('Epoch [{}/{}] train Loss: {loss:.8f} ' \n",
    "                   .format(epoch+1, num_epochs, i+1, loss=avg_train_loss))\n",
    "\n",
    "    # append list for polt graph after training\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "\n",
    "# plt.plot(range(num_epochs), lrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
