{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAeDZzbCAjOH"
   },
   "source": [
    "<b>Data Augumentation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dh3c1ko9AaF_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13142,
     "status": "ok",
     "timestamp": 1598961991945,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "4r3YagKIArTc",
    "outputId": "15c1e1b0-fd93-4369-a09d-cf1f3ebaf11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Dataset : Without Augumentation\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils as utils\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset_train = datasets.STL10(\n",
    "    './data', \n",
    "    split=\"train\", \n",
    "    download=True, \n",
    "    transform=transform_train)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset_test = datasets.STL10(\n",
    "    './data', \n",
    "    split=\"test\", \n",
    "    download=True, \n",
    "    transform=transform_test)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "dataloader_train = utils.data.DataLoader(dataset_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4)\n",
    "dataloader_test = utils.data.DataLoader(dataset_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctDpZBf6VvbS"
   },
   "outputs": [],
   "source": [
    "# Network\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(3, 4, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(4, 8, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(8, 16, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "    )\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(16 * 12 * 12, 1000),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(1000, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 10),\n",
    "    )\n",
    "\n",
    "  def forward(self, x1):\n",
    "    x2 = self.conv(x1)\n",
    "    x3 = x2.view(x2.size()[0], -1)\n",
    "    x4 = self.fc(x3)\n",
    "    return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 512288,
     "status": "ok",
     "timestamp": 1598963348528,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "59KXKr-RYGcv",
    "outputId": "ba6604bc-2a55-4fe8-dc25-61c3818d0508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2304, out_features=1000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "EPOCH: 1\n",
      "  train loss: 2.302873582839966\n",
      "  train acc : 0.1\n",
      "  test  loss: 2.302932435274124\n",
      "  test  acc : 0.1\n",
      "EPOCH: 2\n",
      "  train loss: 2.3018790435791017\n",
      "  train acc : 0.1\n",
      "  test  loss: 2.3019659250974653\n",
      "  test  acc : 0.1\n",
      "EPOCH: 3\n",
      "  train loss: 2.3006522130966185\n",
      "  train acc : 0.1\n",
      "  test  loss: 2.3007696628570558\n",
      "  test  acc : 0.100125\n",
      "EPOCH: 4\n",
      "  train loss: 2.2990383100509644\n",
      "  train acc : 0.1018\n",
      "  test  loss: 2.299189102649689\n",
      "  test  acc : 0.102625\n",
      "EPOCH: 5\n",
      "  train loss: 2.2965167474746706\n",
      "  train acc : 0.124\n",
      "  test  loss: 2.2967206686735153\n",
      "  test  acc : 0.121875\n",
      "EPOCH: 6\n",
      "  train loss: 2.2916021013259886\n",
      "  train acc : 0.1808\n",
      "  test  loss: 2.291906601190567\n",
      "  test  acc : 0.179125\n",
      "EPOCH: 7\n",
      "  train loss: 2.2794241189956663\n",
      "  train acc : 0.1856\n",
      "  test  loss: 2.2799165517091753\n",
      "  test  acc : 0.174\n",
      "EPOCH: 8\n",
      "  train loss: 2.2414101648330687\n",
      "  train acc : 0.1496\n",
      "  test  loss: 2.2427171647548674\n",
      "  test  acc : 0.13925\n",
      "EPOCH: 9\n",
      "  train loss: 2.1138728952407835\n",
      "  train acc : 0.2364\n",
      "  test  loss: 2.120939442515373\n",
      "  test  acc : 0.224\n",
      "EPOCH: 10\n",
      "  train loss: 2.0067641949653625\n",
      "  train acc : 0.282\n",
      "  test  loss: 2.028583200275898\n",
      "  test  acc : 0.27275\n",
      "EPOCH: 11\n",
      "  train loss: 2.004104380607605\n",
      "  train acc : 0.2768\n",
      "  test  loss: 2.03149945884943\n",
      "  test  acc : 0.268875\n",
      "EPOCH: 12\n",
      "  train loss: 1.9070248913764953\n",
      "  train acc : 0.3218\n",
      "  test  loss: 1.9351710975170135\n",
      "  test  acc : 0.29575\n",
      "EPOCH: 13\n",
      "  train loss: 1.9207044553756714\n",
      "  train acc : 0.2868\n",
      "  test  loss: 1.96457589417696\n",
      "  test  acc : 0.2725\n",
      "EPOCH: 14\n",
      "  train loss: 1.8298637795448303\n",
      "  train acc : 0.337\n",
      "  test  loss: 1.8760433688759803\n",
      "  test  acc : 0.315375\n",
      "EPOCH: 15\n",
      "  train loss: 1.7958361029624939\n",
      "  train acc : 0.3534\n",
      "  test  loss: 1.8509706005454063\n",
      "  test  acc : 0.322125\n",
      "EPOCH: 16\n",
      "  train loss: 1.7954225420951844\n",
      "  train acc : 0.3454\n",
      "  test  loss: 1.8631497249007225\n",
      "  test  acc : 0.316125\n",
      "EPOCH: 17\n",
      "  train loss: 1.7423370862007141\n",
      "  train acc : 0.3644\n",
      "  test  loss: 1.830041490495205\n",
      "  test  acc : 0.32175\n",
      "EPOCH: 18\n",
      "  train loss: 1.679945363998413\n",
      "  train acc : 0.3848\n",
      "  test  loss: 1.7755770519375802\n",
      "  test  acc : 0.34825\n",
      "EPOCH: 19\n",
      "  train loss: 1.6803824186325074\n",
      "  train acc : 0.401\n",
      "  test  loss: 1.7885147690773011\n",
      "  test  acc : 0.347625\n",
      "EPOCH: 20\n",
      "  train loss: 1.6044994950294496\n",
      "  train acc : 0.4204\n",
      "  test  loss: 1.7283487528562547\n",
      "  test  acc : 0.361375\n",
      "EPOCH: 21\n",
      "  train loss: 1.6073039841651917\n",
      "  train acc : 0.4158\n",
      "  test  loss: 1.7473922580480576\n",
      "  test  acc : 0.347375\n",
      "EPOCH: 22\n",
      "  train loss: 1.5494710803031921\n",
      "  train acc : 0.4546\n",
      "  test  loss: 1.704343755543232\n",
      "  test  acc : 0.368625\n",
      "EPOCH: 23\n",
      "  train loss: 1.5962113666534423\n",
      "  train acc : 0.415\n",
      "  test  loss: 1.7647626772522926\n",
      "  test  acc : 0.354875\n",
      "EPOCH: 24\n",
      "  train loss: 1.491448950767517\n",
      "  train acc : 0.4564\n",
      "  test  loss: 1.691310243308544\n",
      "  test  acc : 0.37175\n",
      "EPOCH: 25\n",
      "  train loss: 1.4241138076782227\n",
      "  train acc : 0.5014\n",
      "  test  loss: 1.6721865996718406\n",
      "  test  acc : 0.38625\n",
      "EPOCH: 26\n",
      "  train loss: 1.434716718196869\n",
      "  train acc : 0.4868\n",
      "  test  loss: 1.6963865756988525\n",
      "  test  acc : 0.37975\n",
      "EPOCH: 27\n",
      "  train loss: 1.375783212184906\n",
      "  train acc : 0.515\n",
      "  test  loss: 1.6770976707339287\n",
      "  test  acc : 0.3915\n",
      "EPOCH: 28\n",
      "  train loss: 1.3281266593933105\n",
      "  train acc : 0.5306\n",
      "  test  loss: 1.6710042998194694\n",
      "  test  acc : 0.39325\n",
      "EPOCH: 29\n",
      "  train loss: 1.2790784215927125\n",
      "  train acc : 0.554\n",
      "  test  loss: 1.6554098293185233\n",
      "  test  acc : 0.388375\n",
      "EPOCH: 30\n",
      "  train loss: 1.2928894829750062\n",
      "  train acc : 0.523\n",
      "  test  loss: 1.7356128886342048\n",
      "  test  acc : 0.378375\n",
      "EPOCH: 31\n",
      "  train loss: 1.2668419981002808\n",
      "  train acc : 0.5294\n",
      "  test  loss: 1.7560419753193854\n",
      "  test  acc : 0.36975\n",
      "EPOCH: 32\n",
      "  train loss: 1.171146434545517\n",
      "  train acc : 0.5958\n",
      "  test  loss: 1.6972515150904655\n",
      "  test  acc : 0.393375\n",
      "EPOCH: 33\n",
      "  train loss: 1.070019416809082\n",
      "  train acc : 0.6206\n",
      "  test  loss: 1.7067339971661568\n",
      "  test  acc : 0.39975\n",
      "EPOCH: 34\n",
      "  train loss: 1.0322918593883514\n",
      "  train acc : 0.6338\n",
      "  test  loss: 1.7571010902523994\n",
      "  test  acc : 0.395375\n",
      "EPOCH: 35\n",
      "  train loss: 1.0645189201831817\n",
      "  train acc : 0.646\n",
      "  test  loss: 1.7509745582938194\n",
      "  test  acc : 0.382875\n",
      "EPOCH: 36\n",
      "  train loss: 0.905486376285553\n",
      "  train acc : 0.7002\n",
      "  test  loss: 1.7294901430606842\n",
      "  test  acc : 0.40325\n",
      "EPOCH: 37\n",
      "  train loss: 0.8759325397014618\n",
      "  train acc : 0.7108\n",
      "  test  loss: 1.819139724969864\n",
      "  test  acc : 0.393625\n",
      "EPOCH: 38\n",
      "  train loss: 0.9578588151931763\n",
      "  train acc : 0.669\n",
      "  test  loss: 1.956073123216629\n",
      "  test  acc : 0.380625\n",
      "EPOCH: 39\n",
      "  train loss: 0.6881902885437011\n",
      "  train acc : 0.789\n",
      "  test  loss: 1.8091503217816354\n",
      "  test  acc : 0.405875\n",
      "EPOCH: 40\n",
      "  train loss: 0.6998332095146179\n",
      "  train acc : 0.7776\n",
      "  test  loss: 1.8380860537290573\n",
      "  test  acc : 0.39975\n",
      "EPOCH: 41\n",
      "  train loss: 0.7132578170299531\n",
      "  train acc : 0.7368\n",
      "  test  loss: 2.1752956584095955\n",
      "  test  acc : 0.382125\n",
      "EPOCH: 42\n",
      "  train loss: 0.5034420937299728\n",
      "  train acc : 0.847\n",
      "  test  loss: 2.0808230340480804\n",
      "  test  acc : 0.384125\n",
      "EPOCH: 43\n",
      "  train loss: 0.6708379280567169\n",
      "  train acc : 0.7766\n",
      "  test  loss: 2.093031443655491\n",
      "  test  acc : 0.39475\n",
      "EPOCH: 44\n",
      "  train loss: 0.37346134692430494\n",
      "  train acc : 0.9076\n",
      "  test  loss: 2.100810731947422\n",
      "  test  acc : 0.3965\n",
      "EPOCH: 45\n",
      "  train loss: 0.5356598663330078\n",
      "  train acc : 0.7992\n",
      "  test  loss: 2.5415920197963713\n",
      "  test  acc : 0.373625\n",
      "EPOCH: 46\n",
      "  train loss: 0.29549560099840166\n",
      "  train acc : 0.9248\n",
      "  test  loss: 2.2223034292459487\n",
      "  test  acc : 0.397125\n",
      "EPOCH: 47\n",
      "  train loss: 0.2802977764606476\n",
      "  train acc : 0.9278\n",
      "  test  loss: 2.381011950969696\n",
      "  test  acc : 0.398\n",
      "EPOCH: 48\n",
      "  train loss: 0.5733031237125397\n",
      "  train acc : 0.7846\n",
      "  test  loss: 2.71784550845623\n",
      "  test  acc : 0.366875\n",
      "EPOCH: 49\n",
      "  train loss: 0.1846100591123104\n",
      "  train acc : 0.9474\n",
      "  test  loss: 2.7956010669469835\n",
      "  test  acc : 0.395625\n",
      "EPOCH: 50\n",
      "  train loss: 0.09538517661392688\n",
      "  train acc : 0.982\n",
      "  test  loss: 2.8036549568176268\n",
      "  test  acc : 0.40875\n",
      "EPOCH: 51\n",
      "  train loss: 1.1762366533279418\n",
      "  train acc : 0.5708\n",
      "  test  loss: 2.3840204283595083\n",
      "  test  acc : 0.29\n",
      "EPOCH: 52\n",
      "  train loss: 0.14690726712346078\n",
      "  train acc : 0.974\n",
      "  test  loss: 2.6606175556778906\n",
      "  test  acc : 0.3905\n",
      "EPOCH: 53\n",
      "  train loss: 0.08509961269795895\n",
      "  train acc : 0.9852\n",
      "  test  loss: 3.0519637152552606\n",
      "  test  acc : 0.404125\n",
      "EPOCH: 54\n",
      "  train loss: 0.039090515170246365\n",
      "  train acc : 0.9952\n",
      "  test  loss: 3.161902832984924\n",
      "  test  acc : 0.409\n",
      "EPOCH: 55\n",
      "  train loss: 0.027299348171800374\n",
      "  train acc : 0.9966\n",
      "  test  loss: 3.3686175376176832\n",
      "  test  acc : 0.4145\n",
      "EPOCH: 56\n",
      "  train loss: 0.015286605432629585\n",
      "  train acc : 0.9988\n",
      "  test  loss: 3.4864005178213118\n",
      "  test  acc : 0.414375\n",
      "EPOCH: 57\n",
      "  train loss: 0.011637286338955164\n",
      "  train acc : 0.999\n",
      "  test  loss: 3.6134398579597473\n",
      "  test  acc : 0.416\n",
      "EPOCH: 58\n",
      "  train loss: 0.017754915030673148\n",
      "  train acc : 0.9974\n",
      "  test  loss: 3.723554849624634\n",
      "  test  acc : 0.414\n",
      "EPOCH: 59\n",
      "  train loss: 0.00911859210114926\n",
      "  train acc : 0.999\n",
      "  test  loss: 3.799682465195656\n",
      "  test  acc : 0.41325\n",
      "EPOCH: 60\n",
      "  train loss: 0.007267704582773149\n",
      "  train acc : 0.9992\n",
      "  test  loss: 3.8579902738332748\n",
      "  test  acc : 0.41625\n",
      "EPOCH: 61\n",
      "  train loss: 0.006440467271022499\n",
      "  train acc : 0.9992\n",
      "  test  loss: 3.9399026840925218\n",
      "  test  acc : 0.4165\n",
      "EPOCH: 62\n",
      "  train loss: 0.005879086742643267\n",
      "  train acc : 0.9994\n",
      "  test  loss: 3.991654798388481\n",
      "  test  acc : 0.415625\n",
      "EPOCH: 63\n",
      "  train loss: 0.005362473339773715\n",
      "  train acc : 0.9994\n",
      "  test  loss: 4.051862233877182\n",
      "  test  acc : 0.415375\n",
      "EPOCH: 64\n",
      "  train loss: 0.0050038103153929115\n",
      "  train acc : 0.9994\n",
      "  test  loss: 4.097467687726021\n",
      "  test  acc : 0.413625\n",
      "EPOCH: 65\n",
      "  train loss: 0.004594317919109017\n",
      "  train acc : 0.9994\n",
      "  test  loss: 4.140557500720024\n",
      "  test  acc : 0.41325\n",
      "EPOCH: 66\n",
      "  train loss: 0.004269818705506623\n",
      "  train acc : 0.9994\n",
      "  test  loss: 4.186239340901375\n",
      "  test  acc : 0.412625\n",
      "EPOCH: 67\n",
      "  train loss: 0.003973512277007103\n",
      "  train acc : 0.9994\n",
      "  test  loss: 4.234238141775132\n",
      "  test  acc : 0.41425\n",
      "EPOCH: 68\n",
      "  train loss: 0.0037420837627723815\n",
      "  train acc : 0.9994\n",
      "  test  loss: 4.2697685152292255\n",
      "  test  acc : 0.413125\n",
      "EPOCH: 69\n",
      "  train loss: 0.0035375299374572934\n",
      "  train acc : 0.9996\n",
      "  test  loss: 4.303728318214416\n",
      "  test  acc : 0.412125\n",
      "EPOCH: 70\n",
      "  train loss: 0.003339884945889935\n",
      "  train acc : 0.9996\n",
      "  test  loss: 4.34431032538414\n",
      "  test  acc : 0.413625\n",
      "EPOCH: 71\n",
      "  train loss: 0.0031608245335519314\n",
      "  train acc : 0.9998\n",
      "  test  loss: 4.377397418022156\n",
      "  test  acc : 0.41375\n",
      "EPOCH: 72\n",
      "  train loss: 0.003010745638748631\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.40874752998352\n",
      "  test  acc : 0.4125\n",
      "EPOCH: 73\n",
      "  train loss: 0.002857936469372362\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.432236906886101\n",
      "  test  acc : 0.412375\n",
      "EPOCH: 74\n",
      "  train loss: 0.002718814864056185\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.465176737308502\n",
      "  test  acc : 0.412375\n",
      "EPOCH: 75\n",
      "  train loss: 0.0025920922087971122\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.484599599242211\n",
      "  test  acc : 0.41375\n",
      "EPOCH: 76\n",
      "  train loss: 0.0024742934003006666\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.516997224092483\n",
      "  test  acc : 0.413\n",
      "EPOCH: 77\n",
      "  train loss: 0.0023644994280766695\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.5399694591760635\n",
      "  test  acc : 0.412375\n",
      "EPOCH: 78\n",
      "  train loss: 0.002273307384457439\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.567383494973183\n",
      "  test  acc : 0.4135\n",
      "EPOCH: 79\n",
      "  train loss: 0.0021802433172706513\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.589962312579155\n",
      "  test  acc : 0.412625\n",
      "EPOCH: 80\n",
      "  train loss: 0.0020876616809982805\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.61113548874855\n",
      "  test  acc : 0.41175\n",
      "EPOCH: 81\n",
      "  train loss: 0.0020057978318072855\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.636479237675667\n",
      "  test  acc : 0.41175\n",
      "EPOCH: 82\n",
      "  train loss: 0.001932687551015988\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.649996614456176\n",
      "  test  acc : 0.413\n",
      "EPOCH: 83\n",
      "  train loss: 0.0018580538651440293\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.673604154586792\n",
      "  test  acc : 0.411625\n",
      "EPOCH: 84\n",
      "  train loss: 0.0017873169586528092\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.695732855796814\n",
      "  test  acc : 0.41175\n",
      "EPOCH: 85\n",
      "  train loss: 0.0017237867426592856\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.718527254462242\n",
      "  test  acc : 0.412875\n",
      "EPOCH: 86\n",
      "  train loss: 0.0016680806840304286\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.739530530571938\n",
      "  test  acc : 0.411\n",
      "EPOCH: 87\n",
      "  train loss: 0.0016074620292056351\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.752934318780899\n",
      "  test  acc : 0.412375\n",
      "EPOCH: 88\n",
      "  train loss: 0.0015568237740080804\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.777848479151726\n",
      "  test  acc : 0.411125\n",
      "EPOCH: 89\n",
      "  train loss: 0.0015039789187721907\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.795172342658043\n",
      "  test  acc : 0.41125\n",
      "EPOCH: 90\n",
      "  train loss: 0.0014549643523059784\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.806382220983505\n",
      "  test  acc : 0.411375\n",
      "EPOCH: 91\n",
      "  train loss: 0.0014094961492810399\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.8267903417348865\n",
      "  test  acc : 0.411375\n",
      "EPOCH: 92\n",
      "  train loss: 0.0013654197787400335\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.842943680286408\n",
      "  test  acc : 0.41\n",
      "EPOCH: 93\n",
      "  train loss: 0.001325559573015198\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.857422187924385\n",
      "  test  acc : 0.41025\n",
      "EPOCH: 94\n",
      "  train loss: 0.0012876129691721872\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.8728581130504605\n",
      "  test  acc : 0.41175\n",
      "EPOCH: 95\n",
      "  train loss: 0.0012498701876029372\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.891032955050468\n",
      "  test  acc : 0.410125\n",
      "EPOCH: 96\n",
      "  train loss: 0.0012129677663324401\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.906815668940544\n",
      "  test  acc : 0.4095\n",
      "EPOCH: 97\n",
      "  train loss: 0.0011826882307650521\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.920118188858032\n",
      "  test  acc : 0.41\n",
      "EPOCH: 98\n",
      "  train loss: 0.0011472752102417872\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.938670349121094\n",
      "  test  acc : 0.410375\n",
      "EPOCH: 99\n",
      "  train loss: 0.0011160123039735481\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.954535761475563\n",
      "  test  acc : 0.40975\n",
      "EPOCH: 100\n",
      "  train loss: 0.0010869954724330455\n",
      "  train acc : 1.0\n",
      "  test  loss: 4.96416050195694\n",
      "  test  acc : 0.409875\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "print(model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.00001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "nepoch = 100\n",
    "\n",
    "for i in range(nepoch):\n",
    "  print(f\"EPOCH: {i+1}\")\n",
    "\n",
    "  ### Train ###\n",
    "  model.train()\n",
    "  for x, t in dataloader_train:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    model.zero_grad()\n",
    "    y = model(x)\n",
    "    loss = criterion(y, t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  sum_loss = 0.0\n",
    "  sum_correct = 0\n",
    "  sum_iter = 0\n",
    "  for x, t in dataloader_train:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    y = model(x)\n",
    "    loss = criterion(y, t)\n",
    "    _, predicted = y.max(1)\n",
    "    sum_loss += loss.cpu().detach().numpy()\n",
    "    sum_correct += (predicted == t).sum().item()\n",
    "    sum_iter += 1\n",
    "  print(f\"  train loss: {sum_loss/sum_iter}\")\n",
    "  print(f\"  train acc : {sum_correct/(sum_iter*batch_size)}\")\n",
    "\n",
    "  ### Test ###\n",
    "  model.eval()\n",
    "  sum_loss = 0.0\n",
    "  sum_correct = 0\n",
    "  sum_iter = 0\n",
    "  for x, t in dataloader_test:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    y = model(x)\n",
    "    loss = criterion(y, t)\n",
    "    _, predicted = y.max(1)\n",
    "    sum_loss += loss.cpu().detach().numpy()\n",
    "    sum_correct += (predicted == t).sum().item()\n",
    "    sum_iter += 1\n",
    "  print(f\"  test  loss: {sum_loss/sum_iter}\")\n",
    "  print(f\"  test  acc : {sum_correct/(sum_iter*batch_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12720,
     "status": "ok",
     "timestamp": 1598963766519,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "pynvT4RXZ1Mi",
    "outputId": "e95a1faf-49e2-4fbb-b5a4-394e13b4f3fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Dataset : With Augumentation\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils as utils\n",
    "\n",
    "### Augumentation ###\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomAffine(degrees=5.0, translate=(0.05,0.05), scale=(0.95,1.05)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset_train = datasets.STL10(\n",
    "    './data', \n",
    "    split=\"train\", \n",
    "    download=True, \n",
    "    transform=transform_train)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset_test = datasets.STL10(\n",
    "    './data', \n",
    "    split=\"test\", \n",
    "    download=True, \n",
    "    transform=transform_test)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "dataloader_train = utils.data.DataLoader(dataset_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4)\n",
    "\n",
    "dataloader_test = utils.data.DataLoader(dataset_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 582717,
     "status": "ok",
     "timestamp": 1598964382974,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "ie1ObbNgqpFD",
    "outputId": "c038dfd0-8f23-4510-c340-b87519021a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2304, out_features=1000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "EPOCH: 1\n",
      "  train loss: 2.3041697216033934\n",
      "  train acc : 0.1\n",
      "  test  loss: 2.304154482483864\n",
      "  test  acc : 0.1\n",
      "EPOCH: 2\n",
      "  train loss: 2.303791809082031\n",
      "  train acc : 0.1\n",
      "  test  loss: 2.303795635700226\n",
      "  test  acc : 0.1\n",
      "EPOCH: 3\n",
      "  train loss: 2.3034748220443726\n",
      "  train acc : 0.1\n",
      "  test  loss: 2.3034702599048615\n",
      "  test  acc : 0.1\n",
      "EPOCH: 4\n",
      "  train loss: 2.303158497810364\n",
      "  train acc : 0.1\n",
      "  test  loss: 2.303154781460762\n",
      "  test  acc : 0.1\n",
      "EPOCH: 5\n",
      "  train loss: 2.3028781366348268\n",
      "  train acc : 0.1\n",
      "  test  loss: 2.3028564512729646\n",
      "  test  acc : 0.1\n",
      "EPOCH: 6\n",
      "  train loss: 2.302546362876892\n",
      "  train acc : 0.1\n",
      "  test  loss: 2.302564573287964\n",
      "  test  acc : 0.1\n",
      "EPOCH: 7\n",
      "  train loss: 2.3022937393188476\n",
      "  train acc : 0.1\n",
      "  test  loss: 2.3022671431303023\n",
      "  test  acc : 0.1\n",
      "EPOCH: 8\n",
      "  train loss: 2.3019883823394776\n",
      "  train acc : 0.1\n",
      "  test  loss: 2.301960164308548\n",
      "  test  acc : 0.1\n",
      "EPOCH: 9\n",
      "  train loss: 2.301634430885315\n",
      "  train acc : 0.0998\n",
      "  test  loss: 2.3016249984502792\n",
      "  test  acc : 0.100125\n",
      "EPOCH: 10\n",
      "  train loss: 2.3012462759017946\n",
      "  train acc : 0.101\n",
      "  test  loss: 2.3012552350759505\n",
      "  test  acc : 0.101125\n",
      "EPOCH: 11\n",
      "  train loss: 2.300802779197693\n",
      "  train acc : 0.1052\n",
      "  test  loss: 2.3008293509483337\n",
      "  test  acc : 0.1055\n",
      "EPOCH: 12\n",
      "  train loss: 2.300444731712341\n",
      "  train acc : 0.1152\n",
      "  test  loss: 2.3003278762102126\n",
      "  test  acc : 0.1175\n",
      "EPOCH: 13\n",
      "  train loss: 2.2997306299209597\n",
      "  train acc : 0.1288\n",
      "  test  loss: 2.2997277706861494\n",
      "  test  acc : 0.130125\n",
      "EPOCH: 14\n",
      "  train loss: 2.298980813026428\n",
      "  train acc : 0.1396\n",
      "  test  loss: 2.2989758282899855\n",
      "  test  acc : 0.143625\n",
      "EPOCH: 15\n",
      "  train loss: 2.298067026138306\n",
      "  train acc : 0.1566\n",
      "  test  loss: 2.298001927137375\n",
      "  test  acc : 0.15525\n",
      "EPOCH: 16\n",
      "  train loss: 2.2965773868560793\n",
      "  train acc : 0.1794\n",
      "  test  loss: 2.2965397477149962\n",
      "  test  acc : 0.16625\n",
      "EPOCH: 17\n",
      "  train loss: 2.2941552066802977\n",
      "  train acc : 0.1872\n",
      "  test  loss: 2.294166749715805\n",
      "  test  acc : 0.179\n",
      "EPOCH: 18\n",
      "  train loss: 2.290212621688843\n",
      "  train acc : 0.1894\n",
      "  test  loss: 2.2898628145456312\n",
      "  test  acc : 0.188\n",
      "EPOCH: 19\n",
      "  train loss: 2.2813818645477295\n",
      "  train acc : 0.1808\n",
      "  test  loss: 2.2813825100660323\n",
      "  test  acc : 0.184375\n",
      "EPOCH: 20\n",
      "  train loss: 2.262829909324646\n",
      "  train acc : 0.1802\n",
      "  test  loss: 2.262180891633034\n",
      "  test  acc : 0.178125\n",
      "EPOCH: 21\n",
      "  train loss: 2.2232572078704833\n",
      "  train acc : 0.1718\n",
      "  test  loss: 2.2217932015657427\n",
      "  test  acc : 0.17525\n",
      "EPOCH: 22\n",
      "  train loss: 2.1785424518585206\n",
      "  train acc : 0.1988\n",
      "  test  loss: 2.173441046476364\n",
      "  test  acc : 0.188875\n",
      "EPOCH: 23\n",
      "  train loss: 2.151254334449768\n",
      "  train acc : 0.2032\n",
      "  test  loss: 2.146834120154381\n",
      "  test  acc : 0.19825\n",
      "EPOCH: 24\n",
      "  train loss: 2.11911883354187\n",
      "  train acc : 0.2386\n",
      "  test  loss: 2.1118617460131643\n",
      "  test  acc : 0.243375\n",
      "EPOCH: 25\n",
      "  train loss: 2.0847550988197328\n",
      "  train acc : 0.25\n",
      "  test  loss: 2.079339000582695\n",
      "  test  acc : 0.253625\n",
      "EPOCH: 26\n",
      "  train loss: 2.0540478157997133\n",
      "  train acc : 0.2686\n",
      "  test  loss: 2.049632027745247\n",
      "  test  acc : 0.26675\n",
      "EPOCH: 27\n",
      "  train loss: 2.02251704454422\n",
      "  train acc : 0.2762\n",
      "  test  loss: 2.019436825811863\n",
      "  test  acc : 0.278\n",
      "EPOCH: 28\n",
      "  train loss: 2.0011636543273927\n",
      "  train acc : 0.2868\n",
      "  test  loss: 2.0021270886063576\n",
      "  test  acc : 0.28425\n",
      "EPOCH: 29\n",
      "  train loss: 1.9567531108856202\n",
      "  train acc : 0.2998\n",
      "  test  loss: 1.9617099925875663\n",
      "  test  acc : 0.296375\n",
      "EPOCH: 30\n",
      "  train loss: 1.9232992339134216\n",
      "  train acc : 0.3208\n",
      "  test  loss: 1.9249493062496186\n",
      "  test  acc : 0.313375\n",
      "EPOCH: 31\n",
      "  train loss: 1.9001727080345154\n",
      "  train acc : 0.3214\n",
      "  test  loss: 1.9059668824076652\n",
      "  test  acc : 0.318625\n",
      "EPOCH: 32\n",
      "  train loss: 1.867229609489441\n",
      "  train acc : 0.329\n",
      "  test  loss: 1.87546626329422\n",
      "  test  acc : 0.317375\n",
      "EPOCH: 33\n",
      "  train loss: 1.896747817993164\n",
      "  train acc : 0.3146\n",
      "  test  loss: 1.8960726767778397\n",
      "  test  acc : 0.30925\n",
      "EPOCH: 34\n",
      "  train loss: 1.8414747357368468\n",
      "  train acc : 0.3336\n",
      "  test  loss: 1.8493051424622535\n",
      "  test  acc : 0.331\n",
      "EPOCH: 35\n",
      "  train loss: 1.8037760591506957\n",
      "  train acc : 0.3402\n",
      "  test  loss: 1.8167933031916619\n",
      "  test  acc : 0.3315\n",
      "EPOCH: 36\n",
      "  train loss: 1.771695215702057\n",
      "  train acc : 0.3674\n",
      "  test  loss: 1.777315054833889\n",
      "  test  acc : 0.36925\n",
      "EPOCH: 37\n",
      "  train loss: 1.73248393535614\n",
      "  train acc : 0.3802\n",
      "  test  loss: 1.7431133911013603\n",
      "  test  acc : 0.37725\n",
      "EPOCH: 38\n",
      "  train loss: 1.723625144958496\n",
      "  train acc : 0.3918\n",
      "  test  loss: 1.7453392773866654\n",
      "  test  acc : 0.370125\n",
      "EPOCH: 39\n",
      "  train loss: 1.6973834443092346\n",
      "  train acc : 0.393\n",
      "  test  loss: 1.7182822942733764\n",
      "  test  acc : 0.3785\n",
      "EPOCH: 40\n",
      "  train loss: 1.6818483543395997\n",
      "  train acc : 0.392\n",
      "  test  loss: 1.7025642052292824\n",
      "  test  acc : 0.385875\n",
      "EPOCH: 41\n",
      "  train loss: 1.7505517268180848\n",
      "  train acc : 0.3842\n",
      "  test  loss: 1.7688125163316726\n",
      "  test  acc : 0.3705\n",
      "EPOCH: 42\n",
      "  train loss: 1.6683432745933533\n",
      "  train acc : 0.4124\n",
      "  test  loss: 1.7072260528802872\n",
      "  test  acc : 0.386\n",
      "EPOCH: 43\n",
      "  train loss: 1.6332542967796326\n",
      "  train acc : 0.4108\n",
      "  test  loss: 1.6666932344436645\n",
      "  test  acc : 0.39925\n",
      "EPOCH: 44\n",
      "  train loss: 1.6209197783470153\n",
      "  train acc : 0.4184\n",
      "  test  loss: 1.6716444224119187\n",
      "  test  acc : 0.396875\n",
      "EPOCH: 45\n",
      "  train loss: 1.695256359577179\n",
      "  train acc : 0.3848\n",
      "  test  loss: 1.7402064248919487\n",
      "  test  acc : 0.368625\n",
      "EPOCH: 46\n",
      "  train loss: 1.6041656041145325\n",
      "  train acc : 0.4346\n",
      "  test  loss: 1.6570472717285156\n",
      "  test  acc : 0.41075\n",
      "EPOCH: 47\n",
      "  train loss: 1.643220500946045\n",
      "  train acc : 0.4226\n",
      "  test  loss: 1.6912907406687736\n",
      "  test  acc : 0.399125\n",
      "EPOCH: 48\n",
      "  train loss: 1.5589916849136352\n",
      "  train acc : 0.44\n",
      "  test  loss: 1.6164931118488313\n",
      "  test  acc : 0.409875\n",
      "EPOCH: 49\n",
      "  train loss: 1.6248582792282105\n",
      "  train acc : 0.4344\n",
      "  test  loss: 1.691534584760666\n",
      "  test  acc : 0.4005\n",
      "EPOCH: 50\n",
      "  train loss: 1.532684144973755\n",
      "  train acc : 0.4514\n",
      "  test  loss: 1.5904882237315179\n",
      "  test  acc : 0.427625\n",
      "EPOCH: 51\n",
      "  train loss: 1.5043910455703735\n",
      "  train acc : 0.4608\n",
      "  test  loss: 1.5785049363970756\n",
      "  test  acc : 0.430625\n",
      "EPOCH: 52\n",
      "  train loss: 1.524941475391388\n",
      "  train acc : 0.4494\n",
      "  test  loss: 1.6045220777392388\n",
      "  test  acc : 0.416875\n",
      "EPOCH: 53\n",
      "  train loss: 1.5120044231414795\n",
      "  train acc : 0.4634\n",
      "  test  loss: 1.582095703482628\n",
      "  test  acc : 0.420875\n",
      "EPOCH: 54\n",
      "  train loss: 1.477079792022705\n",
      "  train acc : 0.4708\n",
      "  test  loss: 1.5697962865233421\n",
      "  test  acc : 0.42825\n",
      "EPOCH: 55\n",
      "  train loss: 1.4599960851669311\n",
      "  train acc : 0.4728\n",
      "  test  loss: 1.5441246137022973\n",
      "  test  acc : 0.44525\n",
      "EPOCH: 56\n",
      "  train loss: 1.4663677072525025\n",
      "  train acc : 0.4802\n",
      "  test  loss: 1.5388819068670272\n",
      "  test  acc : 0.44775\n",
      "EPOCH: 57\n",
      "  train loss: 1.4585085225105285\n",
      "  train acc : 0.4654\n",
      "  test  loss: 1.568229828774929\n",
      "  test  acc : 0.424625\n",
      "EPOCH: 58\n",
      "  train loss: 1.4133051466941833\n",
      "  train acc : 0.4892\n",
      "  test  loss: 1.5100848361849786\n",
      "  test  acc : 0.462625\n",
      "EPOCH: 59\n",
      "  train loss: 1.418608603477478\n",
      "  train acc : 0.484\n",
      "  test  loss: 1.538029220700264\n",
      "  test  acc : 0.446625\n",
      "EPOCH: 60\n",
      "  train loss: 1.401701376438141\n",
      "  train acc : 0.4962\n",
      "  test  loss: 1.5079874768853188\n",
      "  test  acc : 0.451875\n",
      "EPOCH: 61\n",
      "  train loss: 1.3850414276123046\n",
      "  train acc : 0.4966\n",
      "  test  loss: 1.4951866671442986\n",
      "  test  acc : 0.4555\n",
      "EPOCH: 62\n",
      "  train loss: 1.3908980560302735\n",
      "  train acc : 0.492\n",
      "  test  loss: 1.5177925288677216\n",
      "  test  acc : 0.446875\n",
      "EPOCH: 63\n",
      "  train loss: 1.4089157271385193\n",
      "  train acc : 0.4814\n",
      "  test  loss: 1.5273990735411644\n",
      "  test  acc : 0.45275\n",
      "EPOCH: 64\n",
      "  train loss: 1.349276351928711\n",
      "  train acc : 0.5084\n",
      "  test  loss: 1.4813881620764733\n",
      "  test  acc : 0.4605\n",
      "EPOCH: 65\n",
      "  train loss: 1.3199273085594176\n",
      "  train acc : 0.5222\n",
      "  test  loss: 1.4639917016029358\n",
      "  test  acc : 0.4735\n",
      "EPOCH: 66\n",
      "  train loss: 1.4016581273078919\n",
      "  train acc : 0.503\n",
      "  test  loss: 1.5726765841245651\n",
      "  test  acc : 0.449\n",
      "EPOCH: 67\n",
      "  train loss: 1.322500648498535\n",
      "  train acc : 0.5244\n",
      "  test  loss: 1.4660729676485063\n",
      "  test  acc : 0.470625\n",
      "EPOCH: 68\n",
      "  train loss: 1.3381277704238892\n",
      "  train acc : 0.5116\n",
      "  test  loss: 1.5055424466729164\n",
      "  test  acc : 0.450125\n",
      "EPOCH: 69\n",
      "  train loss: 1.2728085279464723\n",
      "  train acc : 0.5384\n",
      "  test  loss: 1.447079311311245\n",
      "  test  acc : 0.47675\n",
      "EPOCH: 70\n",
      "  train loss: 1.273694953918457\n",
      "  train acc : 0.5418\n",
      "  test  loss: 1.4638152539730072\n",
      "  test  acc : 0.475875\n",
      "EPOCH: 71\n",
      "  train loss: 1.2450712776184083\n",
      "  train acc : 0.546\n",
      "  test  loss: 1.4387422904372216\n",
      "  test  acc : 0.4815\n",
      "EPOCH: 72\n",
      "  train loss: 1.3068233346939087\n",
      "  train acc : 0.5324\n",
      "  test  loss: 1.4839292094111443\n",
      "  test  acc : 0.47275\n",
      "EPOCH: 73\n",
      "  train loss: 1.2175402736663818\n",
      "  train acc : 0.5676\n",
      "  test  loss: 1.439523532986641\n",
      "  test  acc : 0.48425\n",
      "EPOCH: 74\n",
      "  train loss: 1.2144973850250245\n",
      "  train acc : 0.5682\n",
      "  test  loss: 1.4354526236653329\n",
      "  test  acc : 0.488625\n",
      "EPOCH: 75\n",
      "  train loss: 1.2595883774757386\n",
      "  train acc : 0.5486\n",
      "  test  loss: 1.4731767311692239\n",
      "  test  acc : 0.47975\n",
      "EPOCH: 76\n",
      "  train loss: 1.2072730803489684\n",
      "  train acc : 0.558\n",
      "  test  loss: 1.454018934071064\n",
      "  test  acc : 0.48225\n",
      "EPOCH: 77\n",
      "  train loss: 1.2486424160003662\n",
      "  train acc : 0.5508\n",
      "  test  loss: 1.5079314857721329\n",
      "  test  acc : 0.47675\n",
      "EPOCH: 78\n",
      "  train loss: 1.2014198637008666\n",
      "  train acc : 0.5598\n",
      "  test  loss: 1.4723089441657067\n",
      "  test  acc : 0.47875\n",
      "EPOCH: 79\n",
      "  train loss: 1.1563355720043182\n",
      "  train acc : 0.5844\n",
      "  test  loss: 1.4559991180896759\n",
      "  test  acc : 0.485375\n",
      "EPOCH: 80\n",
      "  train loss: 1.1432216775417328\n",
      "  train acc : 0.585\n",
      "  test  loss: 1.4440137416124343\n",
      "  test  acc : 0.4885\n",
      "EPOCH: 81\n",
      "  train loss: 1.2010798168182373\n",
      "  train acc : 0.5568\n",
      "  test  loss: 1.482152245938778\n",
      "  test  acc : 0.473375\n",
      "EPOCH: 82\n",
      "  train loss: 1.1369214272499084\n",
      "  train acc : 0.5928\n",
      "  test  loss: 1.4509360015392303\n",
      "  test  acc : 0.482625\n",
      "EPOCH: 83\n",
      "  train loss: 1.13087993144989\n",
      "  train acc : 0.5886\n",
      "  test  loss: 1.4750590950250626\n",
      "  test  acc : 0.479625\n",
      "EPOCH: 84\n",
      "  train loss: 1.1872584450244903\n",
      "  train acc : 0.5758\n",
      "  test  loss: 1.5550452426075936\n",
      "  test  acc : 0.472875\n",
      "EPOCH: 85\n",
      "  train loss: 1.0970149791240693\n",
      "  train acc : 0.6002\n",
      "  test  loss: 1.4481282368302346\n",
      "  test  acc : 0.482125\n",
      "EPOCH: 86\n",
      "  train loss: 1.122448205947876\n",
      "  train acc : 0.5856\n",
      "  test  loss: 1.523398108780384\n",
      "  test  acc : 0.466125\n",
      "EPOCH: 87\n",
      "  train loss: 1.0841711270809173\n",
      "  train acc : 0.6114\n",
      "  test  loss: 1.460981610417366\n",
      "  test  acc : 0.491125\n",
      "EPOCH: 88\n",
      "  train loss: 1.040128357410431\n",
      "  train acc : 0.6346\n",
      "  test  loss: 1.4506045654416084\n",
      "  test  acc : 0.493625\n",
      "EPOCH: 89\n",
      "  train loss: 1.0274166285991668\n",
      "  train acc : 0.6294\n",
      "  test  loss: 1.4456537671387195\n",
      "  test  acc : 0.501625\n",
      "EPOCH: 90\n",
      "  train loss: 0.9824078667163849\n",
      "  train acc : 0.6528\n",
      "  test  loss: 1.4430991038680077\n",
      "  test  acc : 0.49825\n",
      "EPOCH: 91\n",
      "  train loss: 0.9913969719409943\n",
      "  train acc : 0.6438\n",
      "  test  loss: 1.4550983726978302\n",
      "  test  acc : 0.498\n",
      "EPOCH: 92\n",
      "  train loss: 0.9852084743976594\n",
      "  train acc : 0.6562\n",
      "  test  loss: 1.4601720228791237\n",
      "  test  acc : 0.50275\n",
      "EPOCH: 93\n",
      "  train loss: 1.0355600929260254\n",
      "  train acc : 0.6298\n",
      "  test  loss: 1.5040006265044212\n",
      "  test  acc : 0.476125\n",
      "EPOCH: 94\n",
      "  train loss: 1.0685030937194824\n",
      "  train acc : 0.6046\n",
      "  test  loss: 1.5819489821791648\n",
      "  test  acc : 0.457125\n",
      "EPOCH: 95\n",
      "  train loss: 0.9638446879386902\n",
      "  train acc : 0.6458\n",
      "  test  loss: 1.4790130257606506\n",
      "  test  acc : 0.489875\n",
      "EPOCH: 96\n",
      "  train loss: 0.9861490917205811\n",
      "  train acc : 0.656\n",
      "  test  loss: 1.5202976629137992\n",
      "  test  acc : 0.47625\n",
      "EPOCH: 97\n",
      "  train loss: 0.9939584147930145\n",
      "  train acc : 0.6502\n",
      "  test  loss: 1.5323529422283173\n",
      "  test  acc : 0.480375\n",
      "EPOCH: 98\n",
      "  train loss: 0.9312825846672058\n",
      "  train acc : 0.6648\n",
      "  test  loss: 1.5141996383666991\n",
      "  test  acc : 0.487125\n",
      "EPOCH: 99\n",
      "  train loss: 0.9517478883266449\n",
      "  train acc : 0.6584\n",
      "  test  loss: 1.5315074026584625\n",
      "  test  acc : 0.492125\n",
      "EPOCH: 100\n",
      "  train loss: 0.9714844763278961\n",
      "  train acc : 0.6434\n",
      "  test  loss: 1.5690835192799568\n",
      "  test  acc : 0.480625\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "print(model)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.00001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "nepoch = 100\n",
    "\n",
    "for i in range(nepoch):\n",
    "  print(f\"EPOCH: {i+1}\")\n",
    "\n",
    "  ### Train ###\n",
    "  model.train()\n",
    "  for x, t in dataloader_train:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    model.zero_grad()\n",
    "    y = model(x)\n",
    "    loss = criterion(y, t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  sum_loss = 0.0\n",
    "  sum_correct = 0\n",
    "  sum_iter = 0\n",
    "  for x, t in dataloader_train:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    y = model(x)\n",
    "    loss = criterion(y, t)\n",
    "    _, predicted = y.max(1)\n",
    "    sum_loss += loss.cpu().detach().numpy()\n",
    "    sum_correct += (predicted == t).sum().item()\n",
    "    sum_iter += 1\n",
    "  print(f\"  train loss: {sum_loss/sum_iter}\")\n",
    "  print(f\"  train acc : {sum_correct/(sum_iter*batch_size)}\")\n",
    "\n",
    "  ### Test ###\n",
    "  model.eval()\n",
    "  sum_loss = 0.0\n",
    "  sum_correct = 0\n",
    "  sum_iter = 0\n",
    "  for x, t in dataloader_test:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    y = model(x)\n",
    "    loss = criterion(y, t)\n",
    "    _, predicted = y.max(1)\n",
    "    sum_loss += loss.cpu().detach().numpy()\n",
    "    sum_correct += (predicted == t).sum().item()\n",
    "    sum_iter += 1\n",
    "  print(f\"  test  loss: {sum_loss/sum_iter}\")\n",
    "  print(f\"  test  acc : {sum_correct/(sum_iter*batch_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7FlMACHuH5q"
   },
   "source": [
    "参考<br>\n",
    "https://qiita.com/shoheihoh/items/137141864394c3e4866b"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO2kXKY3X+ekxI+zOd50qas",
   "name": "augumentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
