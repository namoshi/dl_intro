{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/namoshi/dl_intro/blob/master/lab_tutorial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inLMQZKU0rUs"
   },
   "source": [
    "3. Siamese Network\n",
    "\n",
    "The Siamese Neural Network consists of two identical networks joined at their outputs.\n",
    "\n",
    "The two networks extract feature vectors from two different samples.\n",
    "\n",
    "Usually, the weights of the two networks are shared, the objective function of the optimization for training the parameters of the networks is defined by using these extracted feature vectors.\n",
    "\n",
    "The parameters of the Siamese Network are trained to distinguish between similar and dissimilar pairs of the training samples. \n",
    "\n",
    "This network architecture is usually used for metric learning, and a contrastive loss over the metric defined on the trained representation is used as the objective function for the optimization. The objective function is defined as\n",
    "\n",
    "\\begin{align}\n",
    "E&=\\frac{1}{2N}\\sum_i^N \\sum_j^N l_{ij}(D_{ij})^2 + (1-l_{ij})max(m-D_{ij}, ~0)^2 \\\\\n",
    "D_{ij}&=||{\\boldsymbol y}_{i}-{\\boldsymbol y}_{j}||^2_2\n",
    "\\end{align}\n",
    "\n",
    "where $m$ is a parameter indicating the distance between clusters and $D_{ij}$ represents the distance between the pair of the outputs $\\boldsymbol{y}_i$ and $\\boldsymbol{y}_j$ of each network for the sample pair $\\boldsymbol{x}_i$ and$\\boldsymbol{x}_j$.\n",
    "\n",
    "A label $l$ is assigned for each sample pair such that label is $l_{ij}=1$ when the pair $i$ and $j$ is similar and label is $l_{ij}=0$ when the pair $i$ and $j$ is dissimilar.\n",
    "\n",
    "After the training of Siamese Network, the outputs for dissimilar pair will be dissimilar while the outputs for similar pair become similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27jSOzDfDQ17"
   },
   "source": [
    "â€»If you want to perform learning, create a source file.\n",
    "\n",
    "Import of required library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEegKhQ00m4r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer.functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlcQrfhJD14m"
   },
   "source": [
    "Fixed seed value for random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sB_m2S6yD45I"
   },
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed_all(100100)\n",
    "torch.manual_seed(100100)\n",
    "np.random.seed(100100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KMehWf5rD89M"
   },
   "source": [
    "Reading dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4A6hskyjECro"
   },
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(\"./data\", download=True, train=True)\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "mnist_test = dsets.MNIST(\"./data\", download=True, train=False)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(mnist_train)):\n",
    "\tx_train.append(np.array(mnist_train[i][0]))\n",
    "\ty_train.append(mnist_train[i][1])\n",
    "\t\n",
    "for i in range(len(mnist_test)):\n",
    "\tx_test.append(np.array(mnist_test[i][0]))\n",
    "\ty_test.append(mnist_test[i][1])\n",
    "\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_train = x_train.reshape(len(mnist_train), 1, 28, 28)\n",
    "x_train = x_train / 255.0\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "x_test = x_test.reshape(len(mnist_test), 1, 28, 28)\n",
    "x_test = x_test / 255.0\n",
    "datasize = len(y_train)\n",
    "datasize_test = len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d7o65WdSENN-"
   },
   "source": [
    "Designation of mini-batch size and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hK1McKQiENdZ"
   },
   "outputs": [],
   "source": [
    "epoch=100\n",
    "batchsize=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EektVQdcEaNZ"
   },
   "source": [
    "Define the model structure.\n",
    "\n",
    "By alternately inputting two samples to the model and obtaining the respective outputs, the two models sharing weights is designed.\n",
    "\n",
    "In this example, a two-dimensional feature vector is obtained as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "wE1Hjx9kEaWE",
    "outputId": "3daaf1fa-c814-4ead-b344-ea57cfc8775f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=1568, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Network, self).__init__()\n",
    "\t\tself.conv1 = nn.Conv2d(1,32,3, padding=1)\n",
    "\t\tself.conv2 = nn.Conv2d(32,32,3, padding=1)\n",
    "\t\tself.fc1 = nn.Linear(32*7*7, 128)\n",
    "\t\tself.fc2 = nn.Linear(128, 2)\n",
    "\t\tself.pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "\tdef forward(self, x1, x2):\n",
    "\t\tx1 = self.pool(F.relu(self.conv1(x1)))\n",
    "\t\tx1 = self.pool(F.relu(self.conv2(x1)))\n",
    "\t\tx1 = x1.view(-1, 32 * 7 * 7)\n",
    "\t\tx1 = F.relu(self.fc1(x1))\n",
    "\t\tx1 = self.fc2(x1)\n",
    "\n",
    "\t\tx2 = self.pool(F.relu(self.conv1(x2)))\n",
    "\t\tx2 = self.pool(F.relu(self.conv2(x2)))\n",
    "\t\tx2 = x2.view(-1, 32 * 7 * 7)\n",
    "\t\tx2 = F.relu(self.fc1(x2))\n",
    "\t\tx2 = self.fc2(x2)\n",
    "\t\treturn x1, x2\n",
    "\n",
    "\tdef forward_test(self, x):\n",
    "\t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "\t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "\t\tx = x.view(-1, 32 * 7 * 7)\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "net = Network()\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "to_yNhBoGNg9"
   },
   "source": [
    "Define loss and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m4juAaB3GiCz"
   },
   "outputs": [],
   "source": [
    "contrastive_loss = ContrastiveLoss(margin=2.0)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tvoCrFq4GrWp"
   },
   "source": [
    "Input data into the model for each mini-batch and perform parameter update.\n",
    "\n",
    "The number of similar pairs and similar pairs is adjusted to be the same in the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "colab_type": "code",
    "id": "iRplKNZ4GskZ",
    "outputId": "5838b76b-45ce-4710-96aa-12fcbafbaf20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "train  mean loss=0.1723920859148105\n",
      "test  mean loss=0.16755626477301122\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e27d1ddc394e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my1_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mx2_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0my2_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss=[]\n",
    "train_acc=[]\n",
    "test_loss=[]\n",
    "test_acc=[]\n",
    "\n",
    "n_class=10\n",
    "\n",
    "result = np.empty((y_train.shape[0], 2))\n",
    "result_test = np.empty((y_test.shape[0], 2))\n",
    "halfsize = int(datasize / 2)\n",
    "\n",
    "for epoch in range(1, epoch+1):\n",
    "  #Create input pair so that the number of pairs of the same class and the number of pairs of different classes are equal.\n",
    "\tx1_train = np.zeros((datasize, 1, 28, 28)).astype(np.float32)\n",
    "\tx2_train = np.zeros((datasize, 1, 28, 28)).astype(np.float32)\n",
    "\ty1_train = np.zeros((datasize, )).astype(np.int32)\n",
    "\ty2_train = np.zeros((datasize, )).astype(np.int32)\n",
    "\tperm = np.random.permutation(datasize)\n",
    " \n",
    "\tfor i in range(halfsize):\n",
    "\t\tx1_train[i] = x_train[perm[i]].copy()\n",
    "\t\ty1_train[i] = y_train[perm[i]].copy()\n",
    "\n",
    "\t\ttemp = np.where(y_train == y1_train[i])[0]\n",
    "\t\tsize = len(temp)\n",
    "\t\tnum = np.random.randint(0, size)\n",
    "\t\twhile(temp[num] == perm[i]):\n",
    "\t\t\tnum = np.random.randint(0, size)\n",
    "\t\tx2_train[i] = x_train[temp[num]].copy()\n",
    "\t\ty2_train[i] = y_train[temp[num]].copy()\n",
    "\n",
    "\tfor i in range(halfsize, datasize):\n",
    "\t\tx1_train[i] = x_train[perm[i]].copy()\n",
    "\t\ty1_train[i] = y_train[perm[i]].copy()\n",
    "\n",
    "\t\ttemp = np.where(y_train != y1_train[i])[0]\n",
    "\t\tsize = len(temp)\n",
    "\t\tnum = np.random.randint(0, size)\n",
    "\t\tx2_train[i] = x_train[temp[num]].copy()\n",
    "\t\ty2_train[i] = y_train[temp[num]].copy()\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "\tprint('epoch', epoch)\n",
    "\tperm = np.random.permutation(datasize)\n",
    "\tfor i in range(0, datasize, batchsize):\n",
    "\t\tx1_batch = x1_train[perm[i:i+batchsize]]\n",
    "\t\ty1_batch = y1_train[perm[i:i+batchsize]]\n",
    "\t\tx2_batch = x2_train[perm[i:i+batchsize]]\n",
    "\t\ty2_batch = y2_train[perm[i:i+batchsize]]\n",
    "\t\tlabel = np.array(y1_batch == y2_batch, dtype=np.int32)\n",
    "\t\tx1_batch = torch.from_numpy(x1_batch.copy()).float().cuda()\n",
    "\t\tx2_batch = torch.from_numpy(x2_batch.copy()).float().cuda()\n",
    "\t\tlabel = torch.from_numpy(label).float().cuda()\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\ty1, y2 = net(x1_batch, x2_batch)\n",
    "\t\tloss = contrastive_loss(y1, y2, label)           \n",
    "\t\tloss.backward()                           \n",
    "\t\toptimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\tsum_score = 0\n",
    "\tsum_loss = 0\n",
    "\n",
    "\tfor i in range(0, datasize, batchsize):\n",
    "\t\tx1_batch = x_train[perm[i:i+batchsize]]\n",
    "\t\ty1_batch = y_train[perm[i:i+batchsize]]\n",
    "\t\tx2_batch = x_train[perm[i:i+batchsize]][::-1]\n",
    "\t\ty2_batch = y_train[perm[i:i+batchsize]][::-1]\n",
    "\t\tlabel = np.array(y1_batch == y2_batch, dtype=np.int32)\n",
    "\t\tx1_batch = torch.from_numpy(x1_batch.copy()).float().cuda()\n",
    "\t\tx2_batch = torch.from_numpy(x2_batch.copy()).float().cuda()\n",
    "\t\tlabel = torch.from_numpy(label).float().cuda()\n",
    "\t\ty1, y2 = net(x1_batch, x2_batch)\n",
    "\t\tloss = contrastive_loss(y1, y2, label)           \n",
    "\t\tsum_loss += float(loss.cpu().data.item()) * batchsize\n",
    "\tprint(\"train  mean loss={}\".format(sum_loss / datasize))\n",
    "\ttrain_loss.append(sum_loss / datasize)\n",
    "\n",
    "\n",
    "\tsum_score = 0\n",
    "\tsum_loss = 0\n",
    "\tperm = np.random.permutation(datasize_test)\n",
    "\tfor i in range(0, datasize_test, batchsize):\n",
    "\t\tx1_batch = x_test[perm[i:i+batchsize]]\n",
    "\t\ty1_batch = y_test[perm[i:i+batchsize]]\n",
    "\t\tx2_batch = x_test[perm[i:i+batchsize]][::-1]\n",
    "\t\ty2_batch = y_test[perm[i:i+batchsize]][::-1]\n",
    "\t\tlabel = np.array(y1_batch == y2_batch, dtype=np.int32)\n",
    "\t\tx1_batch = torch.from_numpy(x1_batch.copy()).float().cuda()\n",
    "\t\tx2_batch = torch.from_numpy(x2_batch.copy()).float().cuda()\n",
    "\t\tlabel = torch.from_numpy(label).float().cuda()\n",
    "\t\ty1, y2 = net(x1_batch, x2_batch)\n",
    "\t\tloss = contrastive_loss(y1, y2, label)           \n",
    "\t\tsum_loss += float(loss.cpu().data.item()) * batchsize\n",
    "\tprint(\"test  mean loss={}\".format(sum_loss / datasize_test))\n",
    "\ttest_loss.append(sum_loss / datasize_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HndLkGJdU34n"
   },
   "source": [
    "Plot the feature space obtained by learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zqnIhD6GO68j"
   },
   "outputs": [],
   "source": [
    "#Get the model output from train data.\n",
    "for i in range(0, datasize, batchsize):\n",
    "\tx1_batch = x_train[i:i+batchsize]\n",
    "\tx1_batch = torch.from_numpy(x1_batch.copy()).float().cuda()\n",
    "\ttest_result = net.forward_test(x1_batch)\n",
    "\ttest_result = test_result.cpu().data.numpy()\n",
    "\tresult[i:i + batchsize] = test_result\n",
    "\n",
    "\n",
    "#Get the model output from test data.\n",
    "for i in range(0, datasize_test, batchsize):\n",
    "\tx1_batch = x_test[i:i+batchsize]\n",
    "\tx1_batch = torch.from_numpy(x1_batch.copy()).float().cuda()\n",
    "\ttest_result = net.forward_test(x1_batch)\n",
    "\ttest_result = test_result.cpu().data.numpy()\n",
    "\tresult_test[i:i + batchsize] = test_result\n",
    "\n",
    "\n",
    "#plot the obtained output.\n",
    "c = ['#ff0000', '#ffff00', '#00ff00', '#00ffff', '#0000ff','#ff00ff', '#990000', '#999900', '#009900', '#009999']\n",
    "plt.figure()\n",
    "for i in range(10):\n",
    "\tfeat = result[np.where(y_train == i)]\n",
    "\tplt.plot(feat[:,0], feat[:,1], '.', c=c[i])\n",
    "plt.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for i in range(10):\n",
    "\tfeat = result_test[np.where(y_test == i)]\n",
    "\tplt.plot(feat[:,0], feat[:,1], '.', c=c[i])\n",
    "plt.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(range(epoch), train_loss)\n",
    "plt.plot(range(epoch), test_loss, c='#00ff00')\n",
    "plt.legend(['train loss', 'test loss'])\n",
    "plt.title('loss')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1OS1Y3mVV9E"
   },
   "source": [
    "â€»Pytorch does not implement contrastive loss, which is a standard loss of the siamese network.\n",
    "\n",
    "So, if you want to use siamese network with contrastive loss, you need to import the following source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ZykIwy0Wl72"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\tdef __init__(self, margin=1.0):\n",
    "\t\tsuper(ContrastiveLoss, self).__init__()\n",
    "\t\tself.margin = margin\n",
    "\n",
    "\tdef check_type_forward(self, in_types):\n",
    "\t\tassert len(in_types) == 3\n",
    "\n",
    "\n",
    "\t\tx0_type, x1_type, y_type = in_types\n",
    "\t\tassert x0_type.size() == x1_type.shape\n",
    "\t\tassert x1_type.size()[0] == y_type.shape[0]\n",
    "\t\tassert x1_type.size()[0] > 0\n",
    "\t\tassert x0_type.dim() == 2\n",
    "\t\tassert x1_type.dim() == 2\n",
    "\t\tassert y_type.dim() == 1\n",
    "\n",
    "\tdef forward(self, x0, x1, y):\n",
    "\t\tself.check_type_forward((x0, x1, y))\n",
    "\n",
    "\t\tdiff = x0 - x1\n",
    "\t\tdist = diff.norm(dim=1)\n",
    "\t\tdist_sq = torch.pow(dist, 2)\n",
    "\n",
    "\t\tmdist = self.margin - dist\n",
    "\t\tdist = torch.clamp(mdist, min=0.0)\n",
    "\t\tloss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
    "\t\tloss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "\t\treturn loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "lab_tutorial3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
