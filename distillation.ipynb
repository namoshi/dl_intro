{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQZBcWkr8rXJ"
   },
   "source": [
    "<b>Knowledge Distillation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2792,
     "status": "ok",
     "timestamp": 1598977643501,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "16eSkWuS8lqc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3596,
     "status": "ok",
     "timestamp": 1598977644314,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "NKrOzUQ8_V3b",
    "outputId": "e35f4e97-525f-4785-8774-ec48ce5cc258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils as utils\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset_train = datasets.CIFAR10(\n",
    "    './data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform)\n",
    "dataset_test  = datasets.CIFAR10(\n",
    "    './data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform)\n",
    "\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_test))\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "dataloader_train = utils.data.DataLoader(dataset_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4)\n",
    "dataloader_test = utils.data.DataLoader(dataset_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1806,
     "status": "ok",
     "timestamp": 1598977649327,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "wuDarEP-_V5-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TeacherNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(4, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Network : Teacher\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class TeacherNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TeacherNet, self).__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(3, 4, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(4, 8, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(8, 16, kernel_size=5, padding=2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "    )\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(16 * 4 * 4, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 10),\n",
    "    )\n",
    "\n",
    "  def forward(self, x1):\n",
    "    x2 = self.conv(x1)\n",
    "    x3 = x2.view(x2.size()[0], -1)\n",
    "    x4 = self.fc(x3)\n",
    "    return x4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_t = TeacherNet().to(device)\n",
    "print(model_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 463009,
     "status": "ok",
     "timestamp": 1598978116690,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "xRiThVGC_V8Q",
    "outputId": "c55b81cf-5b5b-4c0a-fd9f-9e538f14c0a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "  train loss: 2.2931676821708677\n",
      "  train acc : 0.11062\n",
      "  test  loss: 2.293211588859558\n",
      "  test  acc : 0.1096\n",
      "EPOCH: 2\n",
      "  train loss: 2.10754123711586\n",
      "  train acc : 0.23526\n",
      "  test  loss: 2.1074262726306916\n",
      "  test  acc : 0.2402\n",
      "EPOCH: 3\n",
      "  train loss: 1.925419793844223\n",
      "  train acc : 0.3194\n",
      "  test  loss: 1.9195134139060974\n",
      "  test  acc : 0.3231\n",
      "EPOCH: 4\n",
      "  train loss: 1.7611073186397552\n",
      "  train acc : 0.3714\n",
      "  test  loss: 1.7557154047489165\n",
      "  test  acc : 0.3706\n",
      "EPOCH: 5\n",
      "  train loss: 1.685886886358261\n",
      "  train acc : 0.39968\n",
      "  test  loss: 1.680221792459488\n",
      "  test  acc : 0.3964\n",
      "EPOCH: 6\n",
      "  train loss: 1.5804206295013428\n",
      "  train acc : 0.43392\n",
      "  test  loss: 1.5767684173583985\n",
      "  test  acc : 0.4283\n",
      "EPOCH: 7\n",
      "  train loss: 1.604973829984665\n",
      "  train acc : 0.42878\n",
      "  test  loss: 1.6015375423431397\n",
      "  test  acc : 0.4257\n",
      "EPOCH: 8\n",
      "  train loss: 1.5612058963775635\n",
      "  train acc : 0.44518\n",
      "  test  loss: 1.5616987907886506\n",
      "  test  acc : 0.4465\n",
      "EPOCH: 9\n",
      "  train loss: 1.4764125971794129\n",
      "  train acc : 0.47216\n",
      "  test  loss: 1.4824352288246154\n",
      "  test  acc : 0.4663\n",
      "EPOCH: 10\n",
      "  train loss: 1.4367219944000245\n",
      "  train acc : 0.4799\n",
      "  test  loss: 1.4484267258644103\n",
      "  test  acc : 0.4727\n",
      "EPOCH: 11\n",
      "  train loss: 1.393360925912857\n",
      "  train acc : 0.50128\n",
      "  test  loss: 1.4085634422302247\n",
      "  test  acc : 0.4925\n",
      "EPOCH: 12\n",
      "  train loss: 1.354861156463623\n",
      "  train acc : 0.5149\n",
      "  test  loss: 1.3767248713970184\n",
      "  test  acc : 0.5007\n",
      "EPOCH: 13\n",
      "  train loss: 1.333468222141266\n",
      "  train acc : 0.52098\n",
      "  test  loss: 1.360563247203827\n",
      "  test  acc : 0.5068\n",
      "EPOCH: 14\n",
      "  train loss: 1.3308502702713012\n",
      "  train acc : 0.52598\n",
      "  test  loss: 1.3547914397716523\n",
      "  test  acc : 0.5172\n",
      "EPOCH: 15\n",
      "  train loss: 1.2881249653100968\n",
      "  train acc : 0.5399\n",
      "  test  loss: 1.3192717957496642\n",
      "  test  acc : 0.5235\n",
      "EPOCH: 16\n",
      "  train loss: 1.2716653699874878\n",
      "  train acc : 0.54434\n",
      "  test  loss: 1.3062572360038758\n",
      "  test  acc : 0.5344\n",
      "EPOCH: 17\n",
      "  train loss: 1.2432474570274352\n",
      "  train acc : 0.55816\n",
      "  test  loss: 1.2848091006278992\n",
      "  test  acc : 0.542\n",
      "EPOCH: 18\n",
      "  train loss: 1.20905331325531\n",
      "  train acc : 0.57208\n",
      "  test  loss: 1.2518192434310913\n",
      "  test  acc : 0.5519\n",
      "EPOCH: 19\n",
      "  train loss: 1.2286988149881364\n",
      "  train acc : 0.56226\n",
      "  test  loss: 1.2752495408058167\n",
      "  test  acc : 0.5482\n",
      "EPOCH: 20\n",
      "  train loss: 1.191545263528824\n",
      "  train acc : 0.57898\n",
      "  test  loss: 1.243042163848877\n",
      "  test  acc : 0.5632\n",
      "EPOCH: 21\n",
      "  train loss: 1.1833353275060654\n",
      "  train acc : 0.57914\n",
      "  test  loss: 1.2402513527870178\n",
      "  test  acc : 0.5629\n",
      "EPOCH: 22\n",
      "  train loss: 1.1421701239347457\n",
      "  train acc : 0.59426\n",
      "  test  loss: 1.2065152007341384\n",
      "  test  acc : 0.573\n",
      "EPOCH: 23\n",
      "  train loss: 1.1668721935749053\n",
      "  train acc : 0.5856\n",
      "  test  loss: 1.232238631248474\n",
      "  test  acc : 0.56\n",
      "EPOCH: 24\n",
      "  train loss: 1.1044601125717164\n",
      "  train acc : 0.61286\n",
      "  test  loss: 1.1777316236495972\n",
      "  test  acc : 0.5882\n",
      "EPOCH: 25\n",
      "  train loss: 1.0873707406520843\n",
      "  train acc : 0.61714\n",
      "  test  loss: 1.1651495963335037\n",
      "  test  acc : 0.5904\n",
      "EPOCH: 26\n",
      "  train loss: 1.092980627298355\n",
      "  train acc : 0.61688\n",
      "  test  loss: 1.1739390355348587\n",
      "  test  acc : 0.593\n",
      "EPOCH: 27\n",
      "  train loss: 1.0612091299295425\n",
      "  train acc : 0.62908\n",
      "  test  loss: 1.1463108998537064\n",
      "  test  acc : 0.6005\n",
      "EPOCH: 28\n",
      "  train loss: 1.054932771205902\n",
      "  train acc : 0.6295\n",
      "  test  loss: 1.144388666152954\n",
      "  test  acc : 0.5999\n",
      "EPOCH: 29\n",
      "  train loss: 1.0723755754232407\n",
      "  train acc : 0.61916\n",
      "  test  loss: 1.1673989760875703\n",
      "  test  acc : 0.5872\n",
      "EPOCH: 30\n",
      "  train loss: 1.0206712939739226\n",
      "  train acc : 0.6412\n",
      "  test  loss: 1.1228770756721496\n",
      "  test  acc : 0.6114\n",
      "EPOCH: 31\n",
      "  train loss: 1.0417168934345244\n",
      "  train acc : 0.63044\n",
      "  test  loss: 1.1400727486610414\n",
      "  test  acc : 0.5959\n",
      "EPOCH: 32\n",
      "  train loss: 1.0248672713041305\n",
      "  train acc : 0.63748\n",
      "  test  loss: 1.1304330545663834\n",
      "  test  acc : 0.6068\n",
      "EPOCH: 33\n",
      "  train loss: 0.9871212218999863\n",
      "  train acc : 0.6563\n",
      "  test  loss: 1.0964487010240556\n",
      "  test  acc : 0.6174\n",
      "EPOCH: 34\n",
      "  train loss: 0.983528244137764\n",
      "  train acc : 0.65552\n",
      "  test  loss: 1.0989647006988525\n",
      "  test  acc : 0.6164\n",
      "EPOCH: 35\n",
      "  train loss: 0.957406467795372\n",
      "  train acc : 0.6665\n",
      "  test  loss: 1.0793594461679459\n",
      "  test  acc : 0.6264\n",
      "EPOCH: 36\n",
      "  train loss: 0.989738899230957\n",
      "  train acc : 0.65006\n",
      "  test  loss: 1.105552258491516\n",
      "  test  acc : 0.6128\n",
      "EPOCH: 37\n",
      "  train loss: 0.9539655418395996\n",
      "  train acc : 0.66844\n",
      "  test  loss: 1.0766202330589294\n",
      "  test  acc : 0.6295\n",
      "EPOCH: 38\n",
      "  train loss: 0.9310913389921188\n",
      "  train acc : 0.67668\n",
      "  test  loss: 1.0708602619171144\n",
      "  test  acc : 0.6313\n",
      "EPOCH: 39\n",
      "  train loss: 0.9534614320993423\n",
      "  train acc : 0.66816\n",
      "  test  loss: 1.0897552090883256\n",
      "  test  acc : 0.6241\n",
      "EPOCH: 40\n",
      "  train loss: 0.9231463154554367\n",
      "  train acc : 0.67834\n",
      "  test  loss: 1.064574461579323\n",
      "  test  acc : 0.6361\n",
      "EPOCH: 41\n",
      "  train loss: 0.8996288465261459\n",
      "  train acc : 0.68722\n",
      "  test  loss: 1.046831458210945\n",
      "  test  acc : 0.6417\n",
      "EPOCH: 42\n",
      "  train loss: 0.9162131832838059\n",
      "  train acc : 0.67994\n",
      "  test  loss: 1.0732064366340637\n",
      "  test  acc : 0.6347\n",
      "EPOCH: 43\n",
      "  train loss: 0.8895532728433609\n",
      "  train acc : 0.6882\n",
      "  test  loss: 1.0515688395500182\n",
      "  test  acc : 0.6366\n",
      "EPOCH: 44\n",
      "  train loss: 0.939040941119194\n",
      "  train acc : 0.67148\n",
      "  test  loss: 1.1048619383573532\n",
      "  test  acc : 0.6275\n",
      "EPOCH: 45\n",
      "  train loss: 0.8567150775194168\n",
      "  train acc : 0.70054\n",
      "  test  loss: 1.0284973788261413\n",
      "  test  acc : 0.6452\n",
      "EPOCH: 46\n",
      "  train loss: 0.8619879423379898\n",
      "  train acc : 0.69864\n",
      "  test  loss: 1.0426904267072679\n",
      "  test  acc : 0.6445\n",
      "EPOCH: 47\n",
      "  train loss: 0.8641595706939698\n",
      "  train acc : 0.6996\n",
      "  test  loss: 1.0473250329494477\n",
      "  test  acc : 0.6442\n",
      "EPOCH: 48\n",
      "  train loss: 0.8616532620191574\n",
      "  train acc : 0.69596\n",
      "  test  loss: 1.0499435424804688\n",
      "  test  acc : 0.6354\n",
      "EPOCH: 49\n",
      "  train loss: 0.8310989500284195\n",
      "  train acc : 0.70878\n",
      "  test  loss: 1.0245838797092437\n",
      "  test  acc : 0.6511\n",
      "EPOCH: 50\n",
      "  train loss: 0.8672023833990097\n",
      "  train acc : 0.69868\n",
      "  test  loss: 1.056706472635269\n",
      "  test  acc : 0.6413\n",
      "EPOCH: 51\n",
      "  train loss: 0.8133565069437027\n",
      "  train acc : 0.71598\n",
      "  test  loss: 1.0234035086631774\n",
      "  test  acc : 0.6537\n",
      "EPOCH: 52\n",
      "  train loss: 0.8154144430160523\n",
      "  train acc : 0.71486\n",
      "  test  loss: 1.0373025238513947\n",
      "  test  acc : 0.6491\n",
      "EPOCH: 53\n",
      "  train loss: 0.8248048961162567\n",
      "  train acc : 0.71308\n",
      "  test  loss: 1.0439594030380248\n",
      "  test  acc : 0.6473\n",
      "EPOCH: 54\n",
      "  train loss: 0.7923887349367141\n",
      "  train acc : 0.72452\n",
      "  test  loss: 1.0166434866189957\n",
      "  test  acc : 0.6545\n",
      "EPOCH: 55\n",
      "  train loss: 0.8232555602788925\n",
      "  train acc : 0.7109\n",
      "  test  loss: 1.0490756559371948\n",
      "  test  acc : 0.6463\n",
      "EPOCH: 56\n",
      "  train loss: 0.7997243288755417\n",
      "  train acc : 0.7217\n",
      "  test  loss: 1.02545840382576\n",
      "  test  acc : 0.6522\n",
      "EPOCH: 57\n",
      "  train loss: 0.7886505725383759\n",
      "  train acc : 0.72634\n",
      "  test  loss: 1.0304640555381774\n",
      "  test  acc : 0.6479\n",
      "EPOCH: 58\n",
      "  train loss: 0.8003325835466385\n",
      "  train acc : 0.71962\n",
      "  test  loss: 1.0468264430761338\n",
      "  test  acc : 0.648\n",
      "EPOCH: 59\n",
      "  train loss: 0.7549879624843597\n",
      "  train acc : 0.73718\n",
      "  test  loss: 1.0099241995811463\n",
      "  test  acc : 0.6569\n",
      "EPOCH: 60\n",
      "  train loss: 0.7669782508611679\n",
      "  train acc : 0.73268\n",
      "  test  loss: 1.0281341397762298\n",
      "  test  acc : 0.6531\n",
      "EPOCH: 61\n",
      "  train loss: 0.7539842387437821\n",
      "  train acc : 0.73664\n",
      "  test  loss: 1.0208402514457702\n",
      "  test  acc : 0.6555\n",
      "EPOCH: 62\n",
      "  train loss: 0.7939032967686653\n",
      "  train acc : 0.72524\n",
      "  test  loss: 1.0628523844480515\n",
      "  test  acc : 0.6475\n",
      "EPOCH: 63\n",
      "  train loss: 0.7568915036320686\n",
      "  train acc : 0.7341\n",
      "  test  loss: 1.0436737596988679\n",
      "  test  acc : 0.648\n",
      "EPOCH: 64\n",
      "  train loss: 0.7499907692074775\n",
      "  train acc : 0.73656\n",
      "  test  loss: 1.0324336910247802\n",
      "  test  acc : 0.6533\n",
      "EPOCH: 65\n",
      "  train loss: 0.7584998930692672\n",
      "  train acc : 0.73342\n",
      "  test  loss: 1.054541413784027\n",
      "  test  acc : 0.6449\n",
      "EPOCH: 66\n",
      "  train loss: 0.7439504600763321\n",
      "  train acc : 0.73782\n",
      "  test  loss: 1.044155743122101\n",
      "  test  acc : 0.6539\n",
      "EPOCH: 67\n",
      "  train loss: 0.7184480192065239\n",
      "  train acc : 0.74854\n",
      "  test  loss: 1.0214773261547088\n",
      "  test  acc : 0.6512\n",
      "EPOCH: 68\n",
      "  train loss: 0.7331847151517868\n",
      "  train acc : 0.74288\n",
      "  test  loss: 1.0456777507066726\n",
      "  test  acc : 0.6515\n",
      "EPOCH: 69\n",
      "  train loss: 0.7445951217412948\n",
      "  train acc : 0.7352\n",
      "  test  loss: 1.0625248348712921\n",
      "  test  acc : 0.6483\n",
      "EPOCH: 70\n",
      "  train loss: 0.7091200512051582\n",
      "  train acc : 0.75176\n",
      "  test  loss: 1.0254102373123168\n",
      "  test  acc : 0.6555\n",
      "EPOCH: 71\n",
      "  train loss: 0.7402544029951096\n",
      "  train acc : 0.74164\n",
      "  test  loss: 1.0514750510454178\n",
      "  test  acc : 0.6491\n",
      "EPOCH: 72\n",
      "  train loss: 0.6841656969189643\n",
      "  train acc : 0.76218\n",
      "  test  loss: 1.024554278254509\n",
      "  test  acc : 0.6625\n",
      "EPOCH: 73\n",
      "  train loss: 0.6786216523647308\n",
      "  train acc : 0.76486\n",
      "  test  loss: 1.0207340401411056\n",
      "  test  acc : 0.6626\n",
      "EPOCH: 74\n",
      "  train loss: 0.6794124015569687\n",
      "  train acc : 0.76054\n",
      "  test  loss: 1.026132595539093\n",
      "  test  acc : 0.6596\n",
      "EPOCH: 75\n",
      "  train loss: 0.7163638837337494\n",
      "  train acc : 0.74978\n",
      "  test  loss: 1.0709543669223784\n",
      "  test  acc : 0.6472\n",
      "EPOCH: 76\n",
      "  train loss: 0.6943567074537277\n",
      "  train acc : 0.75364\n",
      "  test  loss: 1.0619943130016327\n",
      "  test  acc : 0.6464\n",
      "EPOCH: 77\n",
      "  train loss: 0.6851101868748665\n",
      "  train acc : 0.7586\n",
      "  test  loss: 1.0502609688043594\n",
      "  test  acc : 0.6581\n",
      "EPOCH: 78\n",
      "  train loss: 0.6796899732947349\n",
      "  train acc : 0.7612\n",
      "  test  loss: 1.053891960978508\n",
      "  test  acc : 0.6519\n",
      "EPOCH: 79\n",
      "  train loss: 0.6656258987188339\n",
      "  train acc : 0.76738\n",
      "  test  loss: 1.0409998714923858\n",
      "  test  acc : 0.6561\n",
      "EPOCH: 80\n",
      "  train loss: 0.6584852693676948\n",
      "  train acc : 0.7692\n",
      "  test  loss: 1.0541885590553284\n",
      "  test  acc : 0.6573\n",
      "EPOCH: 81\n",
      "  train loss: 0.6624513892531395\n",
      "  train acc : 0.76696\n",
      "  test  loss: 1.0484238529205323\n",
      "  test  acc : 0.6511\n",
      "EPOCH: 82\n",
      "  train loss: 0.6334951769709587\n",
      "  train acc : 0.78052\n",
      "  test  loss: 1.0407379651069641\n",
      "  test  acc : 0.66\n",
      "EPOCH: 83\n",
      "  train loss: 0.6439997943639755\n",
      "  train acc : 0.7743\n",
      "  test  loss: 1.0534690880775452\n",
      "  test  acc : 0.6556\n",
      "EPOCH: 84\n",
      "  train loss: 0.6261625703573227\n",
      "  train acc : 0.78206\n",
      "  test  loss: 1.0473940670490265\n",
      "  test  acc : 0.6587\n",
      "EPOCH: 85\n",
      "  train loss: 0.6882208639979362\n",
      "  train acc : 0.75664\n",
      "  test  loss: 1.1077802050113679\n",
      "  test  acc : 0.6462\n",
      "EPOCH: 86\n",
      "  train loss: 0.6493717764019966\n",
      "  train acc : 0.7712\n",
      "  test  loss: 1.0778089743852615\n",
      "  test  acc : 0.6507\n",
      "EPOCH: 87\n",
      "  train loss: 0.620711403310299\n",
      "  train acc : 0.78356\n",
      "  test  loss: 1.0664126992225647\n",
      "  test  acc : 0.6587\n",
      "EPOCH: 88\n",
      "  train loss: 0.6401920301914215\n",
      "  train acc : 0.77444\n",
      "  test  loss: 1.0972152906656265\n",
      "  test  acc : 0.6487\n",
      "EPOCH: 89\n",
      "  train loss: 0.6136849268078804\n",
      "  train acc : 0.78616\n",
      "  test  loss: 1.0784109234809875\n",
      "  test  acc : 0.6584\n",
      "EPOCH: 90\n",
      "  train loss: 0.6135172398090363\n",
      "  train acc : 0.7844\n",
      "  test  loss: 1.0779643541574477\n",
      "  test  acc : 0.6549\n",
      "EPOCH: 91\n",
      "  train loss: 0.6213541243076325\n",
      "  train acc : 0.78\n",
      "  test  loss: 1.0855924528837204\n",
      "  test  acc : 0.6531\n",
      "EPOCH: 92\n",
      "  train loss: 0.5980403795838356\n",
      "  train acc : 0.79296\n",
      "  test  loss: 1.0755024778842925\n",
      "  test  acc : 0.6504\n",
      "EPOCH: 93\n",
      "  train loss: 0.6056848204135895\n",
      "  train acc : 0.78586\n",
      "  test  loss: 1.0848833125829698\n",
      "  test  acc : 0.6525\n",
      "EPOCH: 94\n",
      "  train loss: 0.624405912399292\n",
      "  train acc : 0.78096\n",
      "  test  loss: 1.1037953490018844\n",
      "  test  acc : 0.6507\n",
      "EPOCH: 95\n",
      "  train loss: 0.6092976436018944\n",
      "  train acc : 0.78428\n",
      "  test  loss: 1.1037353348731995\n",
      "  test  acc : 0.6521\n",
      "EPOCH: 96\n",
      "  train loss: 0.6009091561436654\n",
      "  train acc : 0.78986\n",
      "  test  loss: 1.0959677588939667\n",
      "  test  acc : 0.6517\n",
      "EPOCH: 97\n",
      "  train loss: 0.592756465792656\n",
      "  train acc : 0.79266\n",
      "  test  loss: 1.1124474948644638\n",
      "  test  acc : 0.6548\n",
      "EPOCH: 98\n",
      "  train loss: 0.6437264221906662\n",
      "  train acc : 0.77188\n",
      "  test  loss: 1.1408336180448533\n",
      "  test  acc : 0.6448\n",
      "EPOCH: 99\n",
      "  train loss: 0.6275874025821686\n",
      "  train acc : 0.77794\n",
      "  test  loss: 1.1511693918704986\n",
      "  test  acc : 0.6471\n",
      "EPOCH: 100\n",
      "  train loss: 0.5787756912112236\n",
      "  train acc : 0.79804\n",
      "  test  loss: 1.1137356758117676\n",
      "  test  acc : 0.6498\n"
     ]
    }
   ],
   "source": [
    "# Training : Teacher Model\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model_t.parameters(), lr=0.01, weight_decay=0.00001)\n",
    "#optimizer = optim.Adam(model_t.parameters(), lr=0.01, weight_decay=0.001)\n",
    "criterion_t = nn.CrossEntropyLoss()\n",
    "\n",
    "nepoch=100\n",
    "\n",
    "for i in range(nepoch):\n",
    "  print(f\"EPOCH: {i+1}\")\n",
    "\n",
    "  ### Train ###\n",
    "  model_t.train()\n",
    "  for x, t in dataloader_train:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    model_t.zero_grad()\n",
    "    y = model_t(x)\n",
    "    loss = criterion_t(y, t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  model_t.eval()\n",
    "  sum_loss = 0.0\n",
    "  sum_correct = 0\n",
    "  sum_iter = 0\n",
    "  for x, t in dataloader_train:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    y = model_t(x)\n",
    "    loss = criterion_t(y, t)\n",
    "    _, predicted = y.max(1)\n",
    "    sum_loss += loss.cpu().detach().numpy()\n",
    "    sum_correct += (predicted == t).sum().item()\n",
    "    sum_iter += 1\n",
    "  print(f\"  train loss: {sum_loss/sum_iter}\")\n",
    "  print(f\"  train acc : {sum_correct/(sum_iter*batch_size)}\")\n",
    "\n",
    "  ### Test ###\n",
    "  model_t.eval()\n",
    "  sum_loss = 0.0\n",
    "  sum_correct = 0\n",
    "  sum_iter = 0\n",
    "  for x, t in dataloader_test:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    y = model_t(x)\n",
    "    loss = criterion_t(y, t)\n",
    "    _, predicted = y.max(1)\n",
    "    sum_loss += loss.cpu().detach().numpy()\n",
    "    sum_correct += (predicted == t).sum().item()\n",
    "    sum_iter += 1\n",
    "  print(f\"  test  loss: {sum_loss/sum_iter}\")\n",
    "  print(f\"  test  acc : {sum_correct/(sum_iter*batch_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1598978123633,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "5EuquyETe9as"
   },
   "outputs": [],
   "source": [
    "# Freeze Teacher Model Parameter\n",
    "\n",
    "for param in model_t.parameters():\n",
    "  param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 902,
     "status": "ok",
     "timestamp": 1598978126488,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "SRSkh3SR_L7X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StudentNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Network : Student\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(StudentNet, self).__init__()\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(3, 4, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(4, 8, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "    )\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Linear(8 * 8 * 8, 100),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(100, 10),\n",
    "    )\n",
    "\n",
    "  def forward(self, x1):\n",
    "    x2 = self.conv(x1)\n",
    "    x3 = x2.view(x2.size()[0], -1)\n",
    "    x4 = self.fc(x3)\n",
    "    return x4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = StudentNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 463009,
     "status": "ok",
     "timestamp": 1598978116690,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "xRiThVGC_V8Q",
    "outputId": "c55b81cf-5b5b-4c0a-fd9f-9e538f14c0a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "  train loss: 2.1246299471855163\n",
      "  train acc : 0.24438\n",
      "  test  loss: 2.1187277603149415\n",
      "  test  acc : 0.2507\n",
      "EPOCH: 2\n",
      "  train loss: 1.9269846847057344\n",
      "  train acc : 0.31846\n",
      "  test  loss: 1.9150080978870392\n",
      "  test  acc : 0.3256\n",
      "EPOCH: 3\n",
      "  train loss: 1.7830498971939086\n",
      "  train acc : 0.36746\n",
      "  test  loss: 1.775259416103363\n",
      "  test  acc : 0.372\n",
      "EPOCH: 4\n",
      "  train loss: 1.6747831597328187\n",
      "  train acc : 0.40774\n",
      "  test  loss: 1.6701966845989227\n",
      "  test  acc : 0.4117\n",
      "EPOCH: 5\n",
      "  train loss: 1.60275314950943\n",
      "  train acc : 0.43146\n",
      "  test  loss: 1.6032016015052795\n",
      "  test  acc : 0.4368\n",
      "EPOCH: 6\n",
      "  train loss: 1.5313728458881377\n",
      "  train acc : 0.45858\n",
      "  test  loss: 1.5387340426445006\n",
      "  test  acc : 0.4577\n",
      "EPOCH: 7\n",
      "  train loss: 1.4773691787719727\n",
      "  train acc : 0.47872\n",
      "  test  loss: 1.4908929932117463\n",
      "  test  acc : 0.4767\n",
      "EPOCH: 8\n",
      "  train loss: 1.432268519639969\n",
      "  train acc : 0.49384\n",
      "  test  loss: 1.4552681756019592\n",
      "  test  acc : 0.4841\n",
      "EPOCH: 9\n",
      "  train loss: 1.4079386060237884\n",
      "  train acc : 0.49862\n",
      "  test  loss: 1.4343883728981017\n",
      "  test  acc : 0.4934\n",
      "EPOCH: 10\n",
      "  train loss: 1.3798417139053345\n",
      "  train acc : 0.51186\n",
      "  test  loss: 1.4126205778121947\n",
      "  test  acc : 0.5004\n",
      "EPOCH: 11\n",
      "  train loss: 1.3497069919109344\n",
      "  train acc : 0.52616\n",
      "  test  loss: 1.3850413990020751\n",
      "  test  acc : 0.5091\n",
      "EPOCH: 12\n",
      "  train loss: 1.3088609054088594\n",
      "  train acc : 0.53934\n",
      "  test  loss: 1.351311732530594\n",
      "  test  acc : 0.519\n",
      "EPOCH: 13\n",
      "  train loss: 1.296475275993347\n",
      "  train acc : 0.54184\n",
      "  test  loss: 1.3416362917423248\n",
      "  test  acc : 0.5241\n",
      "EPOCH: 14\n",
      "  train loss: 1.266507249712944\n",
      "  train acc : 0.54968\n",
      "  test  loss: 1.3177088820934295\n",
      "  test  acc : 0.5275\n",
      "EPOCH: 15\n",
      "  train loss: 1.2437688112258911\n",
      "  train acc : 0.56104\n",
      "  test  loss: 1.30235098361969\n",
      "  test  acc : 0.5366\n",
      "EPOCH: 16\n",
      "  train loss: 1.2295240243673324\n",
      "  train acc : 0.56686\n",
      "  test  loss: 1.2901571536064147\n",
      "  test  acc : 0.538\n",
      "EPOCH: 17\n",
      "  train loss: 1.2167290730476379\n",
      "  train acc : 0.56798\n",
      "  test  loss: 1.2866279709339141\n",
      "  test  acc : 0.5361\n",
      "EPOCH: 18\n",
      "  train loss: 1.1941839371919631\n",
      "  train acc : 0.57476\n",
      "  test  loss: 1.2662526655197144\n",
      "  test  acc : 0.5456\n",
      "EPOCH: 19\n",
      "  train loss: 1.1734328907728195\n",
      "  train acc : 0.58652\n",
      "  test  loss: 1.2531026303768158\n",
      "  test  acc : 0.5553\n",
      "EPOCH: 20\n",
      "  train loss: 1.1549194239377976\n",
      "  train acc : 0.59434\n",
      "  test  loss: 1.240182363986969\n",
      "  test  acc : 0.5659\n",
      "EPOCH: 21\n",
      "  train loss: 1.1530747003555297\n",
      "  train acc : 0.58988\n",
      "  test  loss: 1.2490495443344116\n",
      "  test  acc : 0.5517\n",
      "EPOCH: 22\n",
      "  train loss: 1.1379278875589371\n",
      "  train acc : 0.6018\n",
      "  test  loss: 1.2342757672071456\n",
      "  test  acc : 0.5615\n",
      "EPOCH: 23\n",
      "  train loss: 1.1210754104852676\n",
      "  train acc : 0.60446\n",
      "  test  loss: 1.2319194960594178\n",
      "  test  acc : 0.5599\n",
      "EPOCH: 24\n",
      "  train loss: 1.0931032375097274\n",
      "  train acc : 0.61782\n",
      "  test  loss: 1.2130182945728303\n",
      "  test  acc : 0.5717\n",
      "EPOCH: 25\n",
      "  train loss: 1.08272858273983\n",
      "  train acc : 0.6183\n",
      "  test  loss: 1.209891293644905\n",
      "  test  acc : 0.5684\n",
      "EPOCH: 26\n",
      "  train loss: 1.085325490117073\n",
      "  train acc : 0.61514\n",
      "  test  loss: 1.2205345821380615\n",
      "  test  acc : 0.5655\n",
      "EPOCH: 27\n",
      "  train loss: 1.06361902487278\n",
      "  train acc : 0.62598\n",
      "  test  loss: 1.201164526939392\n",
      "  test  acc : 0.5766\n",
      "EPOCH: 28\n",
      "  train loss: 1.0577040418386459\n",
      "  train acc : 0.62564\n",
      "  test  loss: 1.203771812915802\n",
      "  test  acc : 0.5704\n",
      "EPOCH: 29\n",
      "  train loss: 1.0424891269207002\n",
      "  train acc : 0.63064\n",
      "  test  loss: 1.1998216491937637\n",
      "  test  acc : 0.5765\n",
      "EPOCH: 30\n",
      "  train loss: 1.0179998021125793\n",
      "  train acc : 0.64268\n",
      "  test  loss: 1.1850621545314788\n",
      "  test  acc : 0.5835\n",
      "EPOCH: 31\n",
      "  train loss: 1.0071504230499269\n",
      "  train acc : 0.64724\n",
      "  test  loss: 1.18076644718647\n",
      "  test  acc : 0.5811\n",
      "EPOCH: 32\n",
      "  train loss: 0.9851322759389878\n",
      "  train acc : 0.65362\n",
      "  test  loss: 1.1695764195919036\n",
      "  test  acc : 0.5911\n",
      "EPOCH: 33\n",
      "  train loss: 1.0037490985393525\n",
      "  train acc : 0.64428\n",
      "  test  loss: 1.191177824139595\n",
      "  test  acc : 0.5798\n",
      "EPOCH: 34\n",
      "  train loss: 0.9688402255773544\n",
      "  train acc : 0.66278\n",
      "  test  loss: 1.1683432632684707\n",
      "  test  acc : 0.5901\n",
      "EPOCH: 35\n",
      "  train loss: 0.9694183464050293\n",
      "  train acc : 0.65978\n",
      "  test  loss: 1.1753260731697082\n",
      "  test  acc : 0.5875\n",
      "EPOCH: 36\n",
      "  train loss: 0.9428391414880752\n",
      "  train acc : 0.67032\n",
      "  test  loss: 1.1552886617183686\n",
      "  test  acc : 0.5941\n",
      "EPOCH: 37\n",
      "  train loss: 0.9320949358940125\n",
      "  train acc : 0.67596\n",
      "  test  loss: 1.1623356664180755\n",
      "  test  acc : 0.5955\n",
      "EPOCH: 38\n",
      "  train loss: 0.957096479177475\n",
      "  train acc : 0.6633\n",
      "  test  loss: 1.1854956036806106\n",
      "  test  acc : 0.5869\n",
      "EPOCH: 39\n",
      "  train loss: 0.9205815529823304\n",
      "  train acc : 0.6797\n",
      "  test  loss: 1.1675935012102128\n",
      "  test  acc : 0.5942\n",
      "EPOCH: 40\n",
      "  train loss: 0.9272129755020142\n",
      "  train acc : 0.67632\n",
      "  test  loss: 1.1872005420923233\n",
      "  test  acc : 0.5902\n",
      "EPOCH: 41\n",
      "  train loss: 0.9148974913358688\n",
      "  train acc : 0.67872\n",
      "  test  loss: 1.1761179304122924\n",
      "  test  acc : 0.5937\n",
      "EPOCH: 42\n",
      "  train loss: 0.8978222165107727\n",
      "  train acc : 0.68682\n",
      "  test  loss: 1.170196390748024\n",
      "  test  acc : 0.5973\n",
      "EPOCH: 43\n",
      "  train loss: 0.9094530327320098\n",
      "  train acc : 0.67914\n",
      "  test  loss: 1.1948466074466706\n",
      "  test  acc : 0.5944\n",
      "EPOCH: 44\n",
      "  train loss: 0.8877041136026382\n",
      "  train acc : 0.6874\n",
      "  test  loss: 1.1698928189277649\n",
      "  test  acc : 0.5963\n",
      "EPOCH: 45\n",
      "  train loss: 0.8637412203550339\n",
      "  train acc : 0.6993\n",
      "  test  loss: 1.170570631623268\n",
      "  test  acc : 0.5951\n",
      "EPOCH: 46\n",
      "  train loss: 0.8966116025447846\n",
      "  train acc : 0.68706\n",
      "  test  loss: 1.2094820493459701\n",
      "  test  acc : 0.5908\n",
      "EPOCH: 47\n",
      "  train loss: 0.845871022105217\n",
      "  train acc : 0.7056\n",
      "  test  loss: 1.167477985024452\n",
      "  test  acc : 0.5956\n",
      "EPOCH: 48\n",
      "  train loss: 0.8438324414491654\n",
      "  train acc : 0.70424\n",
      "  test  loss: 1.1698634082078934\n",
      "  test  acc : 0.5994\n",
      "EPOCH: 49\n",
      "  train loss: 0.8461132589578628\n",
      "  train acc : 0.70454\n",
      "  test  loss: 1.1862213742733\n",
      "  test  acc : 0.5942\n",
      "EPOCH: 50\n",
      "  train loss: 0.8666111257076263\n",
      "  train acc : 0.69784\n",
      "  test  loss: 1.2212369042634963\n",
      "  test  acc : 0.5929\n",
      "EPOCH: 51\n",
      "  train loss: 0.8323022474050522\n",
      "  train acc : 0.70876\n",
      "  test  loss: 1.191751669049263\n",
      "  test  acc : 0.5976\n",
      "EPOCH: 52\n",
      "  train loss: 0.8147671823501587\n",
      "  train acc : 0.71612\n",
      "  test  loss: 1.1865654653310775\n",
      "  test  acc : 0.5975\n",
      "EPOCH: 53\n",
      "  train loss: 0.8399827089309693\n",
      "  train acc : 0.70284\n",
      "  test  loss: 1.2124903827905655\n",
      "  test  acc : 0.5922\n",
      "EPOCH: 54\n",
      "  train loss: 0.7842756088376045\n",
      "  train acc : 0.7278\n",
      "  test  loss: 1.177612288594246\n",
      "  test  acc : 0.6042\n",
      "EPOCH: 55\n",
      "  train loss: 0.8458259934186936\n",
      "  train acc : 0.70324\n",
      "  test  loss: 1.235586782693863\n",
      "  test  acc : 0.5882\n",
      "EPOCH: 56\n",
      "  train loss: 0.7773750475049019\n",
      "  train acc : 0.73008\n",
      "  test  loss: 1.1918983829021454\n",
      "  test  acc : 0.6019\n",
      "EPOCH: 57\n",
      "  train loss: 0.7882230704426766\n",
      "  train acc : 0.72438\n",
      "  test  loss: 1.2114723652601243\n",
      "  test  acc : 0.5997\n",
      "EPOCH: 58\n",
      "  train loss: 0.7754246660470963\n",
      "  train acc : 0.72916\n",
      "  test  loss: 1.2124521052837371\n",
      "  test  acc : 0.6016\n",
      "EPOCH: 59\n",
      "  train loss: 0.7485984690785408\n",
      "  train acc : 0.73984\n",
      "  test  loss: 1.1918293726444245\n",
      "  test  acc : 0.6031\n",
      "EPOCH: 60\n",
      "  train loss: 0.7507588617801666\n",
      "  train acc : 0.73906\n",
      "  test  loss: 1.2018207323551178\n",
      "  test  acc : 0.6022\n",
      "EPOCH: 61\n",
      "  train loss: 0.766211524605751\n",
      "  train acc : 0.73198\n",
      "  test  loss: 1.2223206400871276\n",
      "  test  acc : 0.6006\n",
      "EPOCH: 62\n",
      "  train loss: 0.7339437254071236\n",
      "  train acc : 0.74622\n",
      "  test  loss: 1.206990705728531\n",
      "  test  acc : 0.601\n",
      "EPOCH: 63\n",
      "  train loss: 0.7222217171192169\n",
      "  train acc : 0.75066\n",
      "  test  loss: 1.2087776494026183\n",
      "  test  acc : 0.6015\n",
      "EPOCH: 64\n",
      "  train loss: 0.729342612862587\n",
      "  train acc : 0.74498\n",
      "  test  loss: 1.2283662396669388\n",
      "  test  acc : 0.5973\n",
      "EPOCH: 65\n",
      "  train loss: 0.7045317116975784\n",
      "  train acc : 0.75622\n",
      "  test  loss: 1.2128317993879318\n",
      "  test  acc : 0.6035\n",
      "EPOCH: 66\n",
      "  train loss: 0.7127890995740891\n",
      "  train acc : 0.75328\n",
      "  test  loss: 1.2295037114620209\n",
      "  test  acc : 0.6032\n",
      "EPOCH: 67\n",
      "  train loss: 0.7131255626678467\n",
      "  train acc : 0.75002\n",
      "  test  loss: 1.245510030388832\n",
      "  test  acc : 0.5958\n",
      "EPOCH: 68\n",
      "  train loss: 0.7207553988695145\n",
      "  train acc : 0.7496\n",
      "  test  loss: 1.2663944989442826\n",
      "  test  acc : 0.5962\n",
      "EPOCH: 69\n",
      "  train loss: 0.6963889153003693\n",
      "  train acc : 0.75998\n",
      "  test  loss: 1.2407805562019347\n",
      "  test  acc : 0.5972\n",
      "EPOCH: 70\n",
      "  train loss: 0.7057370135188102\n",
      "  train acc : 0.75412\n",
      "  test  loss: 1.268317221403122\n",
      "  test  acc : 0.5934\n",
      "EPOCH: 71\n",
      "  train loss: 0.6887156537175179\n",
      "  train acc : 0.75908\n",
      "  test  loss: 1.2650495499372483\n",
      "  test  acc : 0.5932\n",
      "EPOCH: 72\n",
      "  train loss: 0.6771768934726715\n",
      "  train acc : 0.76404\n",
      "  test  loss: 1.263713783621788\n",
      "  test  acc : 0.601\n",
      "EPOCH: 73\n",
      "  train loss: 0.6834230549335479\n",
      "  train acc : 0.76272\n",
      "  test  loss: 1.282669216990471\n",
      "  test  acc : 0.5973\n",
      "EPOCH: 74\n",
      "  train loss: 0.674599534869194\n",
      "  train acc : 0.76622\n",
      "  test  loss: 1.27216148853302\n",
      "  test  acc : 0.6014\n",
      "EPOCH: 75\n",
      "  train loss: 0.6596145471930503\n",
      "  train acc : 0.77272\n",
      "  test  loss: 1.2796417146921157\n",
      "  test  acc : 0.5999\n",
      "EPOCH: 76\n",
      "  train loss: 0.6984572142362595\n",
      "  train acc : 0.75442\n",
      "  test  loss: 1.3326860278844834\n",
      "  test  acc : 0.5901\n",
      "EPOCH: 77\n",
      "  train loss: 0.6625498691797257\n",
      "  train acc : 0.7691\n",
      "  test  loss: 1.3043081897497177\n",
      "  test  acc : 0.5921\n",
      "EPOCH: 78\n",
      "  train loss: 0.6648144979476929\n",
      "  train acc : 0.76768\n",
      "  test  loss: 1.3185611218214035\n",
      "  test  acc : 0.5942\n",
      "EPOCH: 79\n",
      "  train loss: 0.620783387362957\n",
      "  train acc : 0.78588\n",
      "  test  loss: 1.2925414276123046\n",
      "  test  acc : 0.6032\n",
      "EPOCH: 80\n",
      "  train loss: 0.6263652504682541\n",
      "  train acc : 0.78236\n",
      "  test  loss: 1.2953209841251374\n",
      "  test  acc : 0.5981\n",
      "EPOCH: 81\n",
      "  train loss: 0.606491258919239\n",
      "  train acc : 0.79226\n",
      "  test  loss: 1.295055313706398\n",
      "  test  acc : 0.601\n",
      "EPOCH: 82\n",
      "  train loss: 0.6136607382893562\n",
      "  train acc : 0.78954\n",
      "  test  loss: 1.309392176270485\n",
      "  test  acc : 0.6022\n",
      "EPOCH: 83\n",
      "  train loss: 0.6095954893231392\n",
      "  train acc : 0.79114\n",
      "  test  loss: 1.3163117706775664\n",
      "  test  acc : 0.6016\n",
      "EPOCH: 84\n",
      "  train loss: 0.6321846396923065\n",
      "  train acc : 0.77682\n",
      "  test  loss: 1.3597541624307632\n",
      "  test  acc : 0.592\n",
      "EPOCH: 85\n",
      "  train loss: 0.6033167070746421\n",
      "  train acc : 0.79148\n",
      "  test  loss: 1.3448013013601303\n",
      "  test  acc : 0.5949\n",
      "EPOCH: 86\n",
      "  train loss: 0.6063240886926651\n",
      "  train acc : 0.79028\n",
      "  test  loss: 1.36897221326828\n",
      "  test  acc : 0.5938\n",
      "EPOCH: 87\n",
      "  train loss: 0.5746600512266159\n",
      "  train acc : 0.80422\n",
      "  test  loss: 1.3425250113010407\n",
      "  test  acc : 0.5957\n",
      "EPOCH: 88\n",
      "  train loss: 0.5722239198684692\n",
      "  train acc : 0.80562\n",
      "  test  loss: 1.3559902590513229\n",
      "  test  acc : 0.5939\n",
      "EPOCH: 89\n",
      "  train loss: 0.622414845764637\n",
      "  train acc : 0.77966\n",
      "  test  loss: 1.4099988341331482\n",
      "  test  acc : 0.5902\n",
      "EPOCH: 90\n",
      "  train loss: 0.5675627080202102\n",
      "  train acc : 0.8075\n",
      "  test  loss: 1.3733859395980834\n",
      "  test  acc : 0.5917\n",
      "EPOCH: 91\n",
      "  train loss: 0.5817767609357833\n",
      "  train acc : 0.79666\n",
      "  test  loss: 1.4000344258546829\n",
      "  test  acc : 0.589\n",
      "EPOCH: 92\n",
      "  train loss: 0.5786737666726113\n",
      "  train acc : 0.7994\n",
      "  test  loss: 1.4093919080495834\n",
      "  test  acc : 0.5928\n",
      "EPOCH: 93\n",
      "  train loss: 0.5750547458529472\n",
      "  train acc : 0.80054\n",
      "  test  loss: 1.4092397427558898\n",
      "  test  acc : 0.5933\n",
      "EPOCH: 94\n",
      "  train loss: 0.568186249256134\n",
      "  train acc : 0.80102\n",
      "  test  loss: 1.427605825662613\n",
      "  test  acc : 0.5882\n",
      "EPOCH: 95\n",
      "  train loss: 0.5635159851312638\n",
      "  train acc : 0.80498\n",
      "  test  loss: 1.432697976231575\n",
      "  test  acc : 0.5889\n",
      "EPOCH: 96\n",
      "  train loss: 0.547095441877842\n",
      "  train acc : 0.80966\n",
      "  test  loss: 1.4194944703578949\n",
      "  test  acc : 0.5934\n",
      "EPOCH: 97\n",
      "  train loss: 0.6075620957612992\n",
      "  train acc : 0.78446\n",
      "  test  loss: 1.4962681365013122\n",
      "  test  acc : 0.5822\n",
      "EPOCH: 98\n",
      "  train loss: 0.5298584372997284\n",
      "  train acc : 0.8186\n",
      "  test  loss: 1.4313937360048294\n",
      "  test  acc : 0.5909\n",
      "EPOCH: 99\n",
      "  train loss: 0.5370268893837928\n",
      "  train acc : 0.81648\n",
      "  test  loss: 1.455860785841942\n",
      "  test  acc : 0.5908\n",
      "EPOCH: 100\n",
      "  train loss: 0.5586095324158669\n",
      "  train acc : 0.80344\n",
      "  test  loss: 1.5012961804866791\n",
      "  test  acc : 0.5877\n"
     ]
    }
   ],
   "source": [
    "# Training : Student Model by the standard CrossEntropyLoss\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.00001)\n",
    "#optimizer = optim.Adam(model_t.parameters(), lr=0.01, weight_decay=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "nepoch=100\n",
    "\n",
    "for i in range(nepoch):\n",
    "  print(f\"EPOCH: {i+1}\")\n",
    "\n",
    "  ### Train ###\n",
    "  model_t.train()\n",
    "  for x, t in dataloader_train:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    model.zero_grad()\n",
    "    y = model(x)\n",
    "    loss = criterion(y, t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  sum_loss = 0.0\n",
    "  sum_correct = 0\n",
    "  sum_iter = 0\n",
    "  for x, t in dataloader_train:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    y = model(x)\n",
    "    loss = criterion(y, t)\n",
    "    _, predicted = y.max(1)\n",
    "    sum_loss += loss.cpu().detach().numpy()\n",
    "    sum_correct += (predicted == t).sum().item()\n",
    "    sum_iter += 1\n",
    "  print(f\"  train loss: {sum_loss/sum_iter}\")\n",
    "  print(f\"  train acc : {sum_correct/(sum_iter*batch_size)}\")\n",
    "\n",
    "  ### Test ###\n",
    "  model.eval()\n",
    "  sum_loss = 0.0\n",
    "  sum_correct = 0\n",
    "  sum_iter = 0\n",
    "  for x, t in dataloader_test:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    y = model(x)\n",
    "    loss = criterion(y, t)\n",
    "    _, predicted = y.max(1)\n",
    "    sum_loss += loss.cpu().detach().numpy()\n",
    "    sum_correct += (predicted == t).sum().item()\n",
    "    sum_iter += 1\n",
    "  print(f\"  test  loss: {sum_loss/sum_iter}\")\n",
    "  print(f\"  test  acc : {sum_correct/(sum_iter*batch_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 902,
     "status": "ok",
     "timestamp": 1598978126488,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "SRSkh3SR_L7X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StudentNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Network : Student for Distilation\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_s = StudentNet().to(device)\n",
    "print(model_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 680277,
     "status": "ok",
     "timestamp": 1598980744954,
     "user": {
      "displayName": "神田圭次郎",
      "photoUrl": "",
      "userId": "06325901568183725812"
     },
     "user_tz": -540
    },
    "id": "5DRk4Q4CbgOm",
    "outputId": "8ea4ba2e-2f00-4651-ec56-b4e09d85c155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "  train loss: 1.635397979259491\n",
      "  train acc : 0.14438\n",
      "  test  loss: 1.6134192073345184\n",
      "  test  acc : 0.1457\n",
      "EPOCH: 2\n",
      "  train loss: 1.3672885711193086\n",
      "  train acc : 0.29066\n",
      "  test  loss: 1.342571724653244\n",
      "  test  acc : 0.2938\n",
      "EPOCH: 3\n",
      "  train loss: 1.2431135723590852\n",
      "  train acc : 0.33892\n",
      "  test  loss: 1.2191564106941224\n",
      "  test  acc : 0.3463\n",
      "EPOCH: 4\n",
      "  train loss: 1.1555605547428132\n",
      "  train acc : 0.37314\n",
      "  test  loss: 1.1344581580162048\n",
      "  test  acc : 0.377\n",
      "EPOCH: 5\n",
      "  train loss: 1.0533302186727524\n",
      "  train acc : 0.40612\n",
      "  test  loss: 1.037122082710266\n",
      "  test  acc : 0.4104\n",
      "EPOCH: 6\n",
      "  train loss: 0.9701670808792114\n",
      "  train acc : 0.4291\n",
      "  test  loss: 0.9553786396980286\n",
      "  test  acc : 0.4349\n",
      "EPOCH: 7\n",
      "  train loss: 0.9479205652475357\n",
      "  train acc : 0.44286\n",
      "  test  loss: 0.9323837625980377\n",
      "  test  acc : 0.4479\n",
      "EPOCH: 8\n",
      "  train loss: 0.8690963395833969\n",
      "  train acc : 0.47078\n",
      "  test  loss: 0.8580413711071014\n",
      "  test  acc : 0.4691\n",
      "EPOCH: 9\n",
      "  train loss: 0.825521358370781\n",
      "  train acc : 0.4863\n",
      "  test  loss: 0.8194616848230362\n",
      "  test  acc : 0.485\n",
      "EPOCH: 10\n",
      "  train loss: 0.7801236555576324\n",
      "  train acc : 0.50288\n",
      "  test  loss: 0.7762293285131454\n",
      "  test  acc : 0.4973\n",
      "EPOCH: 11\n",
      "  train loss: 0.7868168088197708\n",
      "  train acc : 0.49626\n",
      "  test  loss: 0.7875164937973023\n",
      "  test  acc : 0.489\n",
      "EPOCH: 12\n",
      "  train loss: 0.7265209196805954\n",
      "  train acc : 0.5184\n",
      "  test  loss: 0.7253838473558426\n",
      "  test  acc : 0.5128\n",
      "EPOCH: 13\n",
      "  train loss: 0.7070360960960388\n",
      "  train acc : 0.52486\n",
      "  test  loss: 0.709896120429039\n",
      "  test  acc : 0.5204\n",
      "EPOCH: 14\n",
      "  train loss: 0.6974778747558594\n",
      "  train acc : 0.53636\n",
      "  test  loss: 0.7014661091566086\n",
      "  test  acc : 0.5262\n",
      "EPOCH: 15\n",
      "  train loss: 0.6793342276811599\n",
      "  train acc : 0.53214\n",
      "  test  loss: 0.68905517578125\n",
      "  test  acc : 0.5238\n",
      "EPOCH: 16\n",
      "  train loss: 0.6811515607237816\n",
      "  train acc : 0.5353\n",
      "  test  loss: 0.6950050848722458\n",
      "  test  acc : 0.5258\n",
      "EPOCH: 17\n",
      "  train loss: 0.6460527926683426\n",
      "  train acc : 0.56112\n",
      "  test  loss: 0.6616048842668534\n",
      "  test  acc : 0.5487\n",
      "EPOCH: 18\n",
      "  train loss: 0.6276180871129036\n",
      "  train acc : 0.5562\n",
      "  test  loss: 0.646742793917656\n",
      "  test  acc : 0.5394\n",
      "EPOCH: 19\n",
      "  train loss: 0.603572551369667\n",
      "  train acc : 0.57\n",
      "  test  loss: 0.6287346574664116\n",
      "  test  acc : 0.5534\n",
      "EPOCH: 20\n",
      "  train loss: 0.6014398269057274\n",
      "  train acc : 0.57472\n",
      "  test  loss: 0.6285326832532883\n",
      "  test  acc : 0.5545\n",
      "EPOCH: 21\n",
      "  train loss: 0.5913909592628479\n",
      "  train acc : 0.57474\n",
      "  test  loss: 0.6246695569157601\n",
      "  test  acc : 0.5524\n",
      "EPOCH: 22\n",
      "  train loss: 0.5617615169286728\n",
      "  train acc : 0.59154\n",
      "  test  loss: 0.5939277443289757\n",
      "  test  acc : 0.5656\n",
      "EPOCH: 23\n",
      "  train loss: 0.5609067870378495\n",
      "  train acc : 0.59058\n",
      "  test  loss: 0.5944142651557922\n",
      "  test  acc : 0.5622\n",
      "EPOCH: 24\n",
      "  train loss: 0.5494897213578224\n",
      "  train acc : 0.5998\n",
      "  test  loss: 0.5906267964839935\n",
      "  test  acc : 0.5746\n",
      "EPOCH: 25\n",
      "  train loss: 0.5450876787304878\n",
      "  train acc : 0.59296\n",
      "  test  loss: 0.5850331515073777\n",
      "  test  acc : 0.5626\n",
      "EPOCH: 26\n",
      "  train loss: 0.5298347648978233\n",
      "  train acc : 0.60512\n",
      "  test  loss: 0.5760803484916687\n",
      "  test  acc : 0.5747\n",
      "EPOCH: 27\n",
      "  train loss: 0.5326495611071587\n",
      "  train acc : 0.6053\n",
      "  test  loss: 0.5782647117972374\n",
      "  test  acc : 0.5741\n",
      "EPOCH: 28\n",
      "  train loss: 0.533089400947094\n",
      "  train acc : 0.59768\n",
      "  test  loss: 0.5874357300996781\n",
      "  test  acc : 0.5687\n",
      "EPOCH: 29\n",
      "  train loss: 0.5110102959275246\n",
      "  train acc : 0.612\n",
      "  test  loss: 0.5668485489487648\n",
      "  test  acc : 0.575\n",
      "EPOCH: 30\n",
      "  train loss: 0.5039888479709626\n",
      "  train acc : 0.6207\n",
      "  test  loss: 0.5610115146636963\n",
      "  test  acc : 0.5885\n",
      "EPOCH: 31\n",
      "  train loss: 0.5016029242873192\n",
      "  train acc : 0.61284\n",
      "  test  loss: 0.5649448937177658\n",
      "  test  acc : 0.5803\n",
      "EPOCH: 32\n",
      "  train loss: 0.4964300976395607\n",
      "  train acc : 0.62376\n",
      "  test  loss: 0.5562933152914047\n",
      "  test  acc : 0.5887\n",
      "EPOCH: 33\n",
      "  train loss: 0.47634461069107054\n",
      "  train acc : 0.62958\n",
      "  test  loss: 0.5418746730685234\n",
      "  test  acc : 0.5906\n",
      "EPOCH: 34\n",
      "  train loss: 0.46575290673971176\n",
      "  train acc : 0.63226\n",
      "  test  loss: 0.5332254439592361\n",
      "  test  acc : 0.5921\n",
      "EPOCH: 35\n",
      "  train loss: 0.47113415849208834\n",
      "  train acc : 0.63064\n",
      "  test  loss: 0.5424777209758759\n",
      "  test  acc : 0.5888\n",
      "EPOCH: 36\n",
      "  train loss: 0.466832340836525\n",
      "  train acc : 0.63586\n",
      "  test  loss: 0.5386977151036263\n",
      "  test  acc : 0.5925\n",
      "EPOCH: 37\n",
      "  train loss: 0.46316039556264876\n",
      "  train acc : 0.62932\n",
      "  test  loss: 0.5408130857348442\n",
      "  test  acc : 0.587\n",
      "EPOCH: 38\n",
      "  train loss: 0.4565061460733414\n",
      "  train acc : 0.63534\n",
      "  test  loss: 0.5362494087219238\n",
      "  test  acc : 0.5948\n",
      "EPOCH: 39\n",
      "  train loss: 0.44173698353767393\n",
      "  train acc : 0.64744\n",
      "  test  loss: 0.522849323451519\n",
      "  test  acc : 0.6024\n",
      "EPOCH: 40\n",
      "  train loss: 0.442472533583641\n",
      "  train acc : 0.64262\n",
      "  test  loss: 0.5268658757209778\n",
      "  test  acc : 0.5959\n",
      "EPOCH: 41\n",
      "  train loss: 0.43383934289216997\n",
      "  train acc : 0.64748\n",
      "  test  loss: 0.5221953615546227\n",
      "  test  acc : 0.6026\n",
      "EPOCH: 42\n",
      "  train loss: 0.4314471061825752\n",
      "  train acc : 0.65614\n",
      "  test  loss: 0.5209176114201546\n",
      "  test  acc : 0.604\n",
      "EPOCH: 43\n",
      "  train loss: 0.4269361424446106\n",
      "  train acc : 0.65442\n",
      "  test  loss: 0.5202083417773247\n",
      "  test  acc : 0.6022\n",
      "EPOCH: 44\n",
      "  train loss: 0.4238156914710999\n",
      "  train acc : 0.64882\n",
      "  test  loss: 0.5175628635287285\n",
      "  test  acc : 0.6005\n",
      "EPOCH: 45\n",
      "  train loss: 0.4157353894710541\n",
      "  train acc : 0.65902\n",
      "  test  loss: 0.5150505021214485\n",
      "  test  acc : 0.6075\n",
      "EPOCH: 46\n",
      "  train loss: 0.4134510585069656\n",
      "  train acc : 0.65298\n",
      "  test  loss: 0.513810555934906\n",
      "  test  acc : 0.6006\n",
      "EPOCH: 47\n",
      "  train loss: 0.4131735525727272\n",
      "  train acc : 0.65582\n",
      "  test  loss: 0.5162054911255837\n",
      "  test  acc : 0.6051\n",
      "EPOCH: 48\n",
      "  train loss: 0.4108913477063179\n",
      "  train acc : 0.66176\n",
      "  test  loss: 0.5160722151398659\n",
      "  test  acc : 0.6086\n",
      "EPOCH: 49\n",
      "  train loss: 0.4047807297706604\n",
      "  train acc : 0.65768\n",
      "  test  loss: 0.5123375734686851\n",
      "  test  acc : 0.6045\n",
      "EPOCH: 50\n",
      "  train loss: 0.39610887569189074\n",
      "  train acc : 0.66934\n",
      "  test  loss: 0.5049890857934952\n",
      "  test  acc : 0.6137\n",
      "EPOCH: 51\n",
      "  train loss: 0.4140436055064201\n",
      "  train acc : 0.66548\n",
      "  test  loss: 0.5221156868338585\n",
      "  test  acc : 0.6102\n",
      "EPOCH: 52\n",
      "  train loss: 0.39282028877735137\n",
      "  train acc : 0.66536\n",
      "  test  loss: 0.5083333903551102\n",
      "  test  acc : 0.6136\n",
      "EPOCH: 53\n",
      "  train loss: 0.38549939799308774\n",
      "  train acc : 0.67264\n",
      "  test  loss: 0.5008654826879502\n",
      "  test  acc : 0.6155\n",
      "EPOCH: 54\n",
      "  train loss: 0.3819950578212738\n",
      "  train acc : 0.67488\n",
      "  test  loss: 0.5020698928833007\n",
      "  test  acc : 0.6159\n",
      "EPOCH: 55\n",
      "  train loss: 0.38314830568432806\n",
      "  train acc : 0.67462\n",
      "  test  loss: 0.50383892506361\n",
      "  test  acc : 0.6164\n",
      "EPOCH: 56\n",
      "  train loss: 0.38555446529388426\n",
      "  train acc : 0.66884\n",
      "  test  loss: 0.5069993984699249\n",
      "  test  acc : 0.6108\n",
      "EPOCH: 57\n",
      "  train loss: 0.3855761111974716\n",
      "  train acc : 0.6726\n",
      "  test  loss: 0.5078484863042831\n",
      "  test  acc : 0.6123\n",
      "EPOCH: 58\n",
      "  train loss: 0.3745074387192726\n",
      "  train acc : 0.67782\n",
      "  test  loss: 0.5002322474122047\n",
      "  test  acc : 0.6174\n",
      "EPOCH: 59\n",
      "  train loss: 0.3633003870844841\n",
      "  train acc : 0.67898\n",
      "  test  loss: 0.4945986816287041\n",
      "  test  acc : 0.6156\n",
      "EPOCH: 60\n",
      "  train loss: 0.368790662586689\n",
      "  train acc : 0.6801\n",
      "  test  loss: 0.5000259584188461\n",
      "  test  acc : 0.6151\n",
      "EPOCH: 61\n",
      "  train loss: 0.360432723402977\n",
      "  train acc : 0.6888\n",
      "  test  loss: 0.4927518972754478\n",
      "  test  acc : 0.6246\n",
      "EPOCH: 62\n",
      "  train loss: 0.3630594964027405\n",
      "  train acc : 0.68494\n",
      "  test  loss: 0.4987943956255913\n",
      "  test  acc : 0.6179\n",
      "EPOCH: 63\n",
      "  train loss: 0.3525717990100384\n",
      "  train acc : 0.68762\n",
      "  test  loss: 0.4901594564318657\n",
      "  test  acc : 0.622\n",
      "EPOCH: 64\n",
      "  train loss: 0.366496225476265\n",
      "  train acc : 0.68142\n",
      "  test  loss: 0.5063297972083092\n",
      "  test  acc : 0.6159\n",
      "EPOCH: 65\n",
      "  train loss: 0.36338875940442084\n",
      "  train acc : 0.685\n",
      "  test  loss: 0.5038451448082923\n",
      "  test  acc : 0.6225\n",
      "EPOCH: 66\n",
      "  train loss: 0.3539818362593651\n",
      "  train acc : 0.6896\n",
      "  test  loss: 0.4965354749560356\n",
      "  test  acc : 0.6219\n",
      "EPOCH: 67\n",
      "  train loss: 0.34466679680347445\n",
      "  train acc : 0.69354\n",
      "  test  loss: 0.49046169012784957\n",
      "  test  acc : 0.6253\n",
      "EPOCH: 68\n",
      "  train loss: 0.35082990473508835\n",
      "  train acc : 0.69228\n",
      "  test  loss: 0.4986556115746498\n",
      "  test  acc : 0.6257\n",
      "EPOCH: 69\n",
      "  train loss: 0.34218740022182464\n",
      "  train acc : 0.68904\n",
      "  test  loss: 0.4908953911066055\n",
      "  test  acc : 0.6165\n",
      "EPOCH: 70\n",
      "  train loss: 0.34386345383524897\n",
      "  train acc : 0.69472\n",
      "  test  loss: 0.4946659106016159\n",
      "  test  acc : 0.625\n",
      "EPOCH: 71\n",
      "  train loss: 0.34063295504450797\n",
      "  train acc : 0.69258\n",
      "  test  loss: 0.49456439703702926\n",
      "  test  acc : 0.6211\n",
      "EPOCH: 72\n",
      "  train loss: 0.3329945873916149\n",
      "  train acc : 0.69684\n",
      "  test  loss: 0.4898287084698677\n",
      "  test  acc : 0.6246\n",
      "EPOCH: 73\n",
      "  train loss: 0.33205760127305983\n",
      "  train acc : 0.69894\n",
      "  test  loss: 0.48996809601783753\n",
      "  test  acc : 0.6273\n",
      "EPOCH: 74\n",
      "  train loss: 0.332251777946949\n",
      "  train acc : 0.69732\n",
      "  test  loss: 0.49398984789848327\n",
      "  test  acc : 0.6242\n",
      "EPOCH: 75\n",
      "  train loss: 0.32627863079309466\n",
      "  train acc : 0.69852\n",
      "  test  loss: 0.4903364816308022\n",
      "  test  acc : 0.6259\n",
      "EPOCH: 76\n",
      "  train loss: 0.32781803703308104\n",
      "  train acc : 0.6971\n",
      "  test  loss: 0.493479046523571\n",
      "  test  acc : 0.622\n",
      "EPOCH: 77\n",
      "  train loss: 0.3337836429178715\n",
      "  train acc : 0.69474\n",
      "  test  loss: 0.49910047978162764\n",
      "  test  acc : 0.6181\n",
      "EPOCH: 78\n",
      "  train loss: 0.3278947679400444\n",
      "  train acc : 0.70018\n",
      "  test  loss: 0.49344177573919296\n",
      "  test  acc : 0.6238\n",
      "EPOCH: 79\n",
      "  train loss: 0.32678349083662034\n",
      "  train acc : 0.70006\n",
      "  test  loss: 0.49490636616945266\n",
      "  test  acc : 0.622\n",
      "EPOCH: 80\n",
      "  train loss: 0.3214621199667454\n",
      "  train acc : 0.70676\n",
      "  test  loss: 0.49176990538835524\n",
      "  test  acc : 0.6294\n",
      "EPOCH: 81\n",
      "  train loss: 0.3221797331571579\n",
      "  train acc : 0.7045\n",
      "  test  loss: 0.4943986949324608\n",
      "  test  acc : 0.6264\n",
      "EPOCH: 82\n",
      "  train loss: 0.31866891679167747\n",
      "  train acc : 0.70612\n",
      "  test  loss: 0.4926764616370201\n",
      "  test  acc : 0.6258\n",
      "EPOCH: 83\n",
      "  train loss: 0.3174409052431583\n",
      "  train acc : 0.70894\n",
      "  test  loss: 0.49300739496946333\n",
      "  test  acc : 0.6291\n",
      "EPOCH: 84\n",
      "  train loss: 0.3078691381812096\n",
      "  train acc : 0.70928\n",
      "  test  loss: 0.4883409434556961\n",
      "  test  acc : 0.6288\n",
      "EPOCH: 85\n",
      "  train loss: 0.3119466551542282\n",
      "  train acc : 0.71228\n",
      "  test  loss: 0.4919335439801216\n",
      "  test  acc : 0.6309\n",
      "EPOCH: 86\n",
      "  train loss: 0.31444696068763733\n",
      "  train acc : 0.70674\n",
      "  test  loss: 0.49404006481170654\n",
      "  test  acc : 0.6274\n",
      "EPOCH: 87\n",
      "  train loss: 0.3160746256113052\n",
      "  train acc : 0.70412\n",
      "  test  loss: 0.5022726607322693\n",
      "  test  acc : 0.6203\n",
      "EPOCH: 88\n",
      "  train loss: 0.31801364886760713\n",
      "  train acc : 0.70372\n",
      "  test  loss: 0.5030172088742256\n",
      "  test  acc : 0.6256\n",
      "EPOCH: 89\n",
      "  train loss: 0.3038408015072346\n",
      "  train acc : 0.71256\n",
      "  test  loss: 0.4930433678627014\n",
      "  test  acc : 0.6273\n",
      "EPOCH: 90\n",
      "  train loss: 0.30125170412659646\n",
      "  train acc : 0.71016\n",
      "  test  loss: 0.4893046632409096\n",
      "  test  acc : 0.6246\n",
      "EPOCH: 91\n",
      "  train loss: 0.3088650536239147\n",
      "  train acc : 0.70868\n",
      "  test  loss: 0.5021326035261154\n",
      "  test  acc : 0.6281\n",
      "EPOCH: 92\n",
      "  train loss: 0.30905279007554054\n",
      "  train acc : 0.70598\n",
      "  test  loss: 0.501077201962471\n",
      "  test  acc : 0.6225\n",
      "EPOCH: 93\n",
      "  train loss: 0.29964864504337313\n",
      "  train acc : 0.70812\n",
      "  test  loss: 0.49554823100566864\n",
      "  test  acc : 0.6243\n",
      "EPOCH: 94\n",
      "  train loss: 0.2955433352589607\n",
      "  train acc : 0.7155\n",
      "  test  loss: 0.4916798719763756\n",
      "  test  acc : 0.6317\n",
      "EPOCH: 95\n",
      "  train loss: 0.29358070662617686\n",
      "  train acc : 0.71672\n",
      "  test  loss: 0.49066237270832064\n",
      "  test  acc : 0.6305\n",
      "EPOCH: 96\n",
      "  train loss: 0.292851174145937\n",
      "  train acc : 0.71868\n",
      "  test  loss: 0.4937993589043617\n",
      "  test  acc : 0.629\n",
      "EPOCH: 97\n",
      "  train loss: 0.2952266758978367\n",
      "  train acc : 0.71578\n",
      "  test  loss: 0.49716996639966965\n",
      "  test  acc : 0.63\n",
      "EPOCH: 98\n",
      "  train loss: 0.2899096935689449\n",
      "  train acc : 0.71426\n",
      "  test  loss: 0.49330561846494675\n",
      "  test  acc : 0.6295\n",
      "EPOCH: 99\n",
      "  train loss: 0.2874779809117317\n",
      "  train acc : 0.71838\n",
      "  test  loss: 0.49179038524627683\n",
      "  test  acc : 0.6257\n",
      "EPOCH: 100\n",
      "  train loss: 0.29218278646469115\n",
      "  train acc : 0.71846\n",
      "  test  loss: 0.4989086800813675\n",
      "  test  acc : 0.6325\n"
     ]
    }
   ],
   "source": [
    "# Training : Student Model\n",
    "\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = optim.SGD(model_s.parameters(), lr=0.01, weight_decay=0.00001)\n",
    "#optimizer = optim.Adam(model_s.parameters(), lr=0.01, weight_decay=0.001)\n",
    "criterion_s = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "nepoch = 100\n",
    "\n",
    "model_t.eval()\n",
    "\n",
    "for i in range(nepoch):\n",
    "  print(f\"EPOCH: {i+1}\")\n",
    "\n",
    "  ### Train ###\n",
    "  model_s.train()\n",
    "  for x, _ in dataloader_train:\n",
    "    x = x.to(device)\n",
    "    model_s.zero_grad()\n",
    "    y_s = model_s(x)\n",
    "    y_t = model_t(x)\n",
    "    loss = criterion_s(F.log_softmax(y_s, dim=1), F.softmax(y_t, dim=1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  model_s.eval()\n",
    "  sum_loss = 0.0\n",
    "  sum_correct = 0\n",
    "  sum_iter = 0\n",
    "  for x, t in dataloader_train:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    y_s = model_s(x)\n",
    "    y_t = model_t(x)\n",
    "    loss = criterion_s(F.log_softmax(y_s, dim=1), F.softmax(y_t, dim=1))\n",
    "    _, predicted = y_s.max(1)\n",
    "    sum_loss += loss.cpu().detach().numpy()\n",
    "    sum_correct += (predicted == t).sum().item()\n",
    "    sum_iter += 1\n",
    "  print(f\"  train loss: {sum_loss/sum_iter}\")\n",
    "  print(f\"  train acc : {sum_correct/(sum_iter*batch_size)}\")\n",
    "\n",
    "  ### Test ###\n",
    "  model_s.eval()\n",
    "  sum_loss = 0.0\n",
    "  sum_correct = 0\n",
    "  sum_iter = 0\n",
    "  for x, t in dataloader_test:\n",
    "    x = x.to(device)\n",
    "    t = t.to(device)\n",
    "    y_s = model_s(x)\n",
    "    y_t = model_t(x)\n",
    "    loss = criterion_s(F.log_softmax(y_s, dim=1), F.softmax(y_t, dim=1))\n",
    "    _, predicted = y_s.max(1)\n",
    "    sum_loss += loss.cpu().detach().numpy()\n",
    "    sum_correct += (predicted == t).sum().item()\n",
    "    sum_iter += 1\n",
    "  print(f\"  test  loss: {sum_loss/sum_iter}\")\n",
    "  print(f\"  test  acc : {sum_correct/(sum_iter*batch_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4ZqvpB8sVCW"
   },
   "source": [
    "参考<br>\n",
    "https://github.com/peterliht/knowledge-distillation-pytorch<br>\n",
    "http://codecrafthouse.jp/p/2018/01/knowledge-distillation/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPy/6yCmvldj9+Km02AcKdI",
   "name": "distillation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
